{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to segger","text":"<p>segger is a cutting-edge tool for cell segmentation in single-molecule spatial omics datasets. By leveraging graph neural networks (GNNs) and heterogeneous graphs, segger offers unmatched accuracy and scalability.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p> Installation Guide   Get started with installing segger on your machine.</p> </li> <li> <p> User Guide   Learn how to use segger for cell segmentation tasks.</p> </li> <li> <p> Command-Line Interface (CLI)   Explore the CLI options for working with segger.</p> </li> <li> <p> API Reference   Dive into the detailed API documentation for advanced usage.</p> </li> </ul>"},{"location":"#why-segger","title":"Why segger?","text":"<ul> <li> Highly parallelizable  \u2013 Optimized for multi-GPU environments</li> <li> Fast and efficient \u2013 Trains in a fraction of the time compared to alternatives</li> <li> Transfer learning \u2013 Easily adaptable to new datasets and technologies</li> </ul>"},{"location":"#challenges-in-segmentation","title":"Challenges in Segmentation","text":"<p>Spatial omics segmentation faces issues like:</p> <ul> <li>Over/Under-segmentation</li> <li>Transcript contamination</li> <li>Scalability limitations</li> </ul> <p>segger tackles these with a graph-based approach, achieving superior segmentation accuracy.</p>"},{"location":"#how-segger-works","title":"How segger Works","text":""},{"location":"#_1","title":"Home","text":""},{"location":"#powered-by","title":"Powered by","text":"<ul> <li> PyTorch Lightning &amp; PyTorch Geometric: Enables fast, efficient graph neural network (GNN) implementation for heterogeneous graphs.</li> <li> Dask: Scalable parallel processing and distributed task scheduling, ideal for handling large transcriptomic datasets.</li> <li> Shapely &amp; Geopandas: Utilized for spatial operations such as polygon creation, scaling, and spatial relationship computations.</li> <li> RAPIDS: Provides GPU-accelerated computation for tasks like k-nearest neighbors (KNN) graph construction.</li> <li> AnnData &amp; Scanpy: Efficient processing for single-cell datasets.</li> <li> SciPy: Facilitates spatial graph construction, including distance metrics and convex hull calculations for transcript clustering.</li> </ul>"},{"location":"#contributions","title":"Contributions","text":"<p>segger is open-source and welcomes contributions. Join us in advancing spatial omics segmentation!</p> <ul> <li> <p> Source Code GitHub</p> </li> <li> <p> Bug Tracker Report Issues</p> </li> <li> <p> Full Documentation API Reference</p> </li> </ul>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#segger-command-line-interface","title":"Segger Command Line Interface","text":""},{"location":"cli/#1-creating-a-dataset","title":"1. Creating a Dataset","text":"<p>The <code>create_dataset</code> command helps you to build a dataset for spatial transcriptomics. Here\u2019s a breakdown of the options available:</p> // Example: Creating a dataset for spatial transcriptomicspython3 src/segger/cli/create_dataset_fast.py \\    --base_dir /path/to/raw_data \\    --data_dir /path/to/save/processed_data \\    --sample_type xenium \\    --scrnaseq_file /path/to/scrnaseq_file \\    --celltype_column celltype_column_name \\    --k_bd 3 \\    --dist_bd 15.0 \\    --k_tx 3 \\    --dist_tx 5.0 \\    --tile_width 200 \\    --tile_height 200 \\    --neg_sampling_ratio 5.0 \\    --frac 1.0 \\    --val_prob 0.1 \\    --test_prob 0.2 \\    --n_workers 16"},{"location":"cli/#parameters","title":"Parameters","text":"Parameter Description Default Value <code>base_dir</code> Directory containing the raw dataset (e.g., transcripts, boundaries). - <code>data_dir</code> Directory to save the processed Segger dataset (in PyTorch Geometric format). - <code>sample_type</code> The sample type of the raw data, e.g., \"xenium\" or \"merscope\". None <code>scrnaseq_file</code> Path to the scRNAseq file. None <code>celltype_column</code> Column name for cell type annotations in the scRNAseq file. None <code>k_bd</code> Number of nearest neighbors for boundary nodes. <code>3</code> <code>dist_bd</code> Maximum distance for boundary neighbors. <code>15.0</code> <code>k_tx</code> Number of nearest neighbors for transcript nodes. <code>3</code> <code>dist_tx</code> Maximum distance for transcript neighbors. <code>5.0</code> <code>tile_width</code> Width of the tiles in pixels (ignored if <code>tile_size</code> is provided). None <code>tile_height</code> Height of the tiles in pixels (ignored if <code>tile_size</code> is provided). None <code>neg_sampling_ratio</code> Ratio of negative samples. <code>5.0</code> <code>frac</code> Fraction of the dataset to process. Useful for subsampling large datasets. <code>1.0</code> <code>val_prob</code> Proportion of the dataset used for validation split. <code>0.1</code> <code>test_prob</code> Proportion of the dataset used for testing split. <code>0.2</code> <code>n_workers</code> Number of workers for parallel processing. <code>1</code>"},{"location":"cli/#key-updates","title":"Key Updates","text":"<ul> <li>Faster Dataset Creation This method is way faster due to the use of ND-tree-based partitioning and parallel processing.</li> </ul> <p>Customizing Your Dataset</p> <ul> <li>dataset_type: Defines the type of spatial transcriptomics data. Currently, xenium and merscope are supported and have been tested.</li> <li>val_prob, test_prob: Control the dataset portions for validation and testing. Adjust based on your dataset size and evaluation needs.</li> <li>frac: Specifies the fraction of the dataset to process. Reducing <code>frac</code> can be useful when working with very large datasets, allowing for faster dataset creation by only processing a subset of the data.</li> </ul> <p>Faster Dataset Creation</p> <p>Increasing the number of workers (<code>n_workers</code>) can significantly accelerate the dataset creation process, especially for large datasets, by taking advantage of parallel processing across multiple CPU cores.</p> <p>Enhancing Segmentation Accuracy with scRNA-seq</p> <p>Incorporating single cell RNA sequencing (scRNA-seq) data as features can provide additional biological context, improving the accuracy of the segger model.</p>"},{"location":"cli/#2-training-a-model","title":"2. Training a Model","text":"<p>The <code>train</code> command initializes and trains a model using the dataset created. Here are the key parameters:</p> // Example: Training a segger modelpython3 src/segger/cli/train_model.py \\     --dataset_dir /path/to/saved/processed_data \\     --models_dir /path/to/save/model/checkpoints \\     --sample_tag first_training \\     --init_emb 8 \\     --hidden_channels 32 \\     --num_tx_tokens 500 \\     --out_channels 8 \\     --heads 2 \\     --num_mid_layers 2 \\     --batch_size 4 \\     --num_workers 2 \\     --accelerator cuda \\     --max_epochs 200 \\     --devices 4 \\     --strategy auto \\     --precision 16-mixed"},{"location":"cli/#parameters_1","title":"Parameters","text":"Parameter Description Default Value <code>dataset_dir</code> Directory containing the processed Segger dataset (in PyTorch Geometric format). - <code>models_dir</code> Directory to save the trained model and training logs. - <code>sample_tag</code> Tag used to identify the dataset during training. - <code>init_emb</code> Size of the embedding layer for input data. <code>8</code> <code>hidden_channels</code> Number of hidden units in each layer of the neural network. <code>32</code> <code>num_tx_tokens</code> Number of transcript tokens used during training. <code>500</code> <code>out_channels</code> Number of output channels from the model. <code>8</code> <code>heads</code> Number of attention heads used in graph attention layers. <code>2</code> <code>num_mid_layers</code> Number of mid layers in the model. <code>2</code> <code>batch_size</code> Number of samples to process per training batch. <code>4</code> <code>num_workers</code> Number of workers to use for parallel data loading. <code>2</code> <code>accelerator</code> Device used for training (e.g., <code>cuda</code> for GPU or <code>cpu</code>). <code>cuda</code> <code>max_epochs</code> Number of training epochs. <code>200</code> <code>devices</code> Number of devices (GPUs) to use during training. <code>4</code> <code>strategy</code> Strategy used for training (e.g., <code>ddp</code> for distributed training or <code>auto</code>). <code>auto</code> <code>precision</code> Precision used for training (e.g., <code>16-mixed</code> for mixed precision training). <code>16-mixed</code> <p>Optimizing training time</p> <ul> <li>devices: Use multiple GPUs by increasing the <code>devices</code> parameter to further accelerate training.</li> <li>batch_size: A larger batch size can speed up training, but requires more memory. Adjust based on your hardware capabilities.</li> <li>epochs: Increasing the number of epochs can improve model performance by allowing more learning cycles, but it will also extend the overall training time. Balance this based on your time constraints and hardware capacity.</li> </ul> <p>Ensure Correct CUDA and PyTorch Setup</p> <p>Before using the <code>--accelerator cuda</code> flag, ensure your system has CUDA installed and configured correctly. Also, check that the installed CUDA version is compatible with your PyTorch and PyTorch Geometric versions.</p>"},{"location":"cli/#3-making-predictions","title":"3. Making Predictions","text":"<p>After training the model, use the <code>predict</code> command to make predictions on new data:</p> // Example: Make predictions using a trained modelpython3 src/segger/cli/predict_fast.py \\     --segger_data_dir /path/to/saved/processed_data \\     --models_dir /path/to/saved/model/checkpoints \\     --benchmarks_dir /path/to/save/segmentation/results \\     --transcripts_file /path/to/raw_data/transcripts.parquet \\     --batch_size 1 \\     --num_workers 1 \\     --model_version 0 \\     --save_tag segger_embedding_1001 \\     --min_transcripts 5 \\     --cell_id_col segger_cell_id \\     --use_cc false \\     --knn_method cuda \\     --file_format anndata \\     --k_bd 4 \\     --dist_bd 12.0 \\     --k_tx 5 \\     --dist_tx 5.0"},{"location":"cli/#parameters_2","title":"Parameters","text":"Parameter Description Default Value <code>segger_data_dir</code> Directory containing the processed Segger dataset (in PyTorch Geometric format). - <code>models_dir</code> Directory containing the trained models. - <code>benchmarks_dir</code> Directory to save the segmentation results, including cell boundaries and associations. - <code>transcripts_file</code> Path to the transcripts.parquet file. - <code>batch_size</code> Number of samples to process per batch during prediction. <code>1</code> <code>num_workers</code> Number of workers for parallel data loading. <code>1</code> <code>model_version</code> Model version number to load for predictions, corresponding to the version from training logs. <code>0</code> <code>save_tag</code> Tag used to name and organize the segmentation results. <code>segger_embedding_1001</code> <code>min_transcripts</code> Minimum number of transcripts required for segmentation. <code>5</code> <code>cell_id_col</code> Column name for cell IDs in the output data. <code>segger_cell_id</code> <code>use_cc</code> Whether to use connected components for grouping transcripts without direct nucleus association. <code>False</code> <code>knn_method</code> Method for KNN computation (e.g., <code>cuda</code> for GPU-based computation). <code>cuda</code> <code>file_format</code> Format for the output segmentation data (e.g., <code>anndata</code>). <code>anndata</code> <code>k_bd</code> Number of nearest neighbors for boundary nodes. <code>4</code> <code>dist_bd</code> Maximum distance for boundary nodes. <code>12.0</code> <code>k_tx</code> Number of nearest neighbors for transcript nodes. <code>5</code> <code>dist_tx</code> Maximum distance for transcript nodes. <code>5.0</code> <p>Improving Prediction Pipeline</p> <ul> <li>batch_size: A larger batch size can speed up training, but requires more memory. Adjust based on your hardware capabilities.</li> <li>use_cc: Enabling connected component analysis can improve the accuracy of transcript assignments.</li> </ul> <p>Ensure Correct CUDA, cuVS, and PyTorch Setup</p> <p>Before using the <code>knn_method cuda</code> flag, ensure your system has CUDA installed and configured properly. Also, verify that the installed CUDA version is compatible with your cuPy, cuVS, PyTorch, and PyTorch Geometric versions.</p>"},{"location":"cli/#4-running-the-entire-pipeline","title":"4. Running the Entire Pipeline","text":"<p>The <code>submit_job.py</code> script allows you to run the complete Segger pipeline or specific stages like dataset creation, training, or prediction. The pipeline execution is determined by the configuration provided in a YAML file, supporting various environments like Docker, Singularity, and HPC systems (with LSF, Slurm support is planned).</p>"},{"location":"cli/#selecting-pipelines","title":"Selecting Pipelines","text":"<p>You can run the three stages\u2014dataset creation, training, and prediction\u2014sequentially or independently by specifying the pipelines in the YAML configuration file:</p> <pre><code>- `1` for dataset creation\n- `2` for model training\n- `3` for prediction\n</code></pre> <p>This allows you to run the full pipeline or just specific steps. Set the desired stages under the pipelines field in your YAML file.</p>"},{"location":"cli/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Use the following command to run the pipeline:</p> <pre><code>python3 submit_job.py --config_file=filename.yaml\n</code></pre> <ul> <li>If no <code>--config_file</code> is provided, the default <code>config.yaml</code> file will be used.</li> </ul>"},{"location":"cli/#5-containerization","title":"5. Containerization","text":"<p>For users who want a portable, containerized environment, segger supports both Docker and Singularity containers. These containers provide a consistent runtime environment with all dependencies pre-installed.</p>"},{"location":"cli/#using-docker","title":"Using Docker","text":"<p>You can pull the segger Docker image from Docker Hub with this command:</p> <pre><code>docker pull danielunyi42/segger_dev:cuda121\n</code></pre> <p>To run the pipeline in Docker, make sure your YAML configuration includes the following settings:</p> <ul> <li><code>use_singularity</code>: false</li> <li><code>use_lsf</code>: false</li> </ul> <p>Afterwards, run the pipeline inside the Docker container with the same <code>submit_job.py</code> command.</p>"},{"location":"cli/#using-singularity","title":"Using Singularity","text":"<p>For a Singularity environment, pull the image with:</p> <pre><code>singularity pull docker://danielunyi42/segger_dev:cuda121\n</code></pre> <p>Ensure <code>use_singularity: true</code> in the YAML file and specify the Singularity image file (e.g., <code>segger_dev_latest.sif</code>) in the <code>singularity_image</code> field.</p> <p>Containerization</p> <ul> <li>The segger Docker image currently supports CUDA 11.8 and CUDA 12.1.</li> </ul>"},{"location":"cli/#6-hpc-environments","title":"6. HPC Environments","text":"<p>Segger also supports HPC environments with LSF job scheduling. To run the pipeline on an HPC cluster using LSF, set <code>use_lsf: true</code> in your YAML configuration.</p> <p>If your HPC system supports Slurm, a similar setup is planned and will be introduced soon.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#segger-installation-guide","title":"segger Installation Guide","text":"<p>segger provides multiple installation options to suit your requirements. You can install it using:</p> <ul> <li>Virtual environments (recommended for most users)</li> <li>Containerized environments (Docker or Singularity)</li> <li>Editable mode from GitHub (for developers or users who want to modify the source code)</li> </ul> <p>Recommendation</p> <p>To avoid dependency conflicts, we recommend installing segger in a virtual environment or a container environment.</p> <p>segger requires CUDA 11 or CUDA 12 for GPU acceleration.</p>"},{"location":"installation/#installation-in-virtual-environment","title":"Installation in Virtual Environment","text":""},{"location":"installation/#using-venv","title":"Using <code>venv</code>","text":"<pre><code># Step 1: Create and activate the virtual environment.\npython3.10 -m venv segger-venv\nsource segger-venv/bin/activate\n\n# Step 2: Install segger with CUDA support.\npip install --upgrade pip\npip install .[cuda12]\n\n# Step 3: Verify the installation.\npython --version\npip show segger\n\n# step 4 [Optional]: If your system doesn't have a universally installed CUDA toolkit, you can link CuPy to PyTorch's CUDA runtime library.\nexport LD_LIBRARY_PATH=$(pwd)/segger-venv/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"installation/#using-conda","title":"Using <code>conda</code>","text":"<pre><code># Step 1: Create and activate the conda environment.\nconda create -n segger-env python=3.10\nconda activate segger-env\n\n# Step 2: Install segger with CUDA support.\npip install --upgrade pip\npip install .[cuda12]\n\n# Step 3: Verify the installation.\npython --version\npip show segger\n\n# Step 4 [Optional]: If your system doesn't have a universally installed CUDA toolkit, you can link CuPy to PyTorch's CUDA runtime library.\nexport LD_LIBRARY_PATH=$(conda info --base)/envs/segger-env/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"installation/#how-to-choose-between-cuda11-and-cuda12","title":"How to Choose Between <code>[cuda11]</code> and <code>[cuda12]</code>","text":"<ol> <li>Check Your NVIDIA Driver Version: Run <code>nvidia-smi</code>. Use <code>[cuda11]</code> for driver version \u2265 450.80.02 or <code>[cuda12]</code> for version \u2265 525.60.13.</li> <li>Check for a CUDA Toolkit: Run <code>nvcc --version</code>. If it outputs a CUDA version (11.x or 12.x), choose the corresponding <code>[cuda11]</code> or <code>[cuda12]</code>.</li> <li>Default to PyTorch CUDA Runtime: If CUDA toolkit is not installed, segger can use PyTorch's bundled CUDA runtime. You can link CuPy as shown in Step 4 of the venv/conda installation.</li> </ol>"},{"location":"installation/#installation-in-container-environment","title":"Installation in Container Environment","text":""},{"location":"installation/#using-docker","title":"Using <code>docker</code>","text":"<pre><code># Step 1: Pull the official Docker image.\ndocker pull danielunyi42/segger_dev:cuda121\n\n# Step 2: Run the Docker container with GPU support.\ndocker run --gpus all -it danielunyi42/segger_dev:cuda121\n</code></pre> <p>The official Docker image comes with all dependencies pre-installed, including the CUDA toolkit, PyTorch, and CuPy. The current images support CUDA 11.8 and CUDA 12.1, which can be specified in the image tag.</p>"},{"location":"installation/#using-singularity","title":"Using <code>singularity</code>","text":"<pre><code># Step 1: Pull the official Docker image.\nsingularity pull docker://danielunyi42/segger_dev:cuda121\n\n# Step 2: Run the Singularity container with GPU support.\nsingularity exec --nv segger_dev_cuda121.sif\n</code></pre> <p>The Singularity image is derived from the official Docker image and includes all pre-installed dependencies.</p>"},{"location":"installation/#directory-mapping-for-input-and-output-data","title":"Directory Mapping for Input and Output Data","text":"<p>Directory mapping allows:</p> <ul> <li>Access to input data (spatial transcriptomics datasets) from your local machine inside the container.</li> <li>Saving output data (segmentation results and logs) generated by segger to your local machine.</li> </ul> <p>Setting up directory mapping is really easy:</p> <ul> <li> <p>For Docker: <pre><code>docker run --gpus all -it -v /path/to/local/data:/workspace/data danielunyi42/segger_dev:cuda121\n</code></pre></p> </li> <li> <p>For Singularity: <pre><code>singularity exec --nv -B /path/to/local/data:/workspace/data segger_dev_cuda121.sif\n</code></pre></p> </li> <li>Place your input datasets in <code>/path/to/local/data</code> on your host machine.</li> <li>Inside the container, access these datasets from <code>/workspace/data</code>.</li> <li>Save results to <code>/workspace/data</code>, which will be available in <code>/path/to/local/data</code> on the host machine.</li> </ul>"},{"location":"installation/#editable-github-installation","title":"Editable GitHub installation","text":"<p>For developers or users who want to modify the source code:</p> <pre><code>git clone https://github.com/EliHei2/segger_dev.git\ncd segger_dev\npip install -e \".[cuda12]\"\n</code></pre> <p>Common Installation Issues</p> <ul> <li> <p>Python Version: Ensure you are using Python &gt;= 3.10. Check your Python version by running:   <pre><code>python --version\n</code></pre>   If your version is lower than 3.10, please upgrade Python.</p> </li> <li> <p>CUDA Compatibility (GPU): For GPU installations, verify that your system has the correct NVIDIA drivers installed. Run:   <pre><code>nvidia-smi\n</code></pre>   Ensure that the displayed CUDA version is compatible with your selected <code>[cuda11]</code> or <code>[cuda12]</code> extra.</p> <ul> <li>Minimum driver version for CUDA 11.x: <code>450.80.02</code></li> <li>Minimum driver version for CUDA 12.x: <code>525.60.13</code></li> </ul> </li> <li> <p>Permissions: If you encounter permission errors during installation, use the --user flag to install the package without requiring administrative privileges:   <pre><code>pip install --user .[cuda12]\n</code></pre>   Alternatively, consider using a virtual environment (venv or conda) to isolate the installation.</p> </li> <li> <p>Environment Configuration: Ensure that all required dependencies are installed in your environment.</p> </li> </ul>"},{"location":"_build/html/_sources/cli/","title":"Command-Line Interface (CLI)","text":"<p>Documentation for the Segger CLI.</p> <p>```python import click</p> <p>@click.group() def cli():     pass</p> <p>@cli.command() def create_dataset():     \"\"\"Create a new dataset.\"\"\"     pass</p> <p>if name == 'main':     cli()</p>"},{"location":"_build/html/_sources/installation/","title":"Installation Guide","text":"<p>This guide provides detailed instructions for installing the <code>segger</code> package. Whether you are installing for CPU or GPU, the instructions below will guide you through the process.</p> <pre><code>Ensure you are using `Python &gt;= 3.10` before starting the installation process.\n</code></pre>"},{"location":"_build/html/_sources/installation/#install-the-package","title":"Install the Package","text":""},{"location":"_build/html/_sources/installation/#from-source","title":"From Source","text":"<p>To install <code>segger</code> from the source code:</p> <ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/EliHei2/segger_dev.git\n</code></pre></p> </li> <li> <p>Navigate to the project directory:     <pre><code>cd segger_dev\n</code></pre></p> </li> <li> <p>Install the package:     <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"_build/html/_sources/installation/#cpu-and-gpu-installation-from-pypi","title":"CPU and GPU Installation from PyPI","text":"<p><pre><code>```{tab-item} CPU Installation\nIf you only need CPU support, use the following command:\n\n```bash\npip install segger\n</code></pre> This will install the package without any GPU-related dependencies.</p> <pre><code>This is ideal for environments where GPU support is not required or available.\n</code></pre> <p>```{tab-item} GPU Installation For installations with GPU support, use the following command:</p> <pre><code>pip install segger[gpu]\n</code></pre> <p>This includes the necessary dependencies for CUDA-enabled GPUs.</p> <pre><code>Ensure your machine has the appropriate CUDA drivers and NVIDIA libraries installed.\n</code></pre> <pre><code>\n</code></pre> <pre><code>## Optional Dependencies\n\nThe following sections describe optional dependencies you can install for specific features.\n\n```{tab-set}\n```{tab-item} Torch Geometric\n\nTo install `torch-geometric` related dependencies, run:\n\n```bash\npip install segger[torch-geometric]\n</code></pre> <p>Follow the additional steps on the PyTorch Geometric installation page to ensure proper setup.</p> <pre><code>Ensure you install `torch-geometric` with the correct CUDA version for GPU support.\n</code></pre> <p>```{tab-item} Multiprocessing</p> <p>To install <code>segger</code> with multiprocessing support, use the following command:</p> <pre><code>pip install segger[multiprocessing]\n</code></pre> <p>This will enable multi-core parallel processing features.</p> <pre><code>## Platform-Specific Installations\n\nBelow are instructions for installing `segger` on different operating systems.\n\n```{tab-set}\n```{tab-item} Linux\n\nOn Linux, use the following command to install the package:\n\n```bash\npip install segger\n</code></pre> <p>For GPU support on Linux, ensure you have the necessary CUDA drivers installed:</p> <pre><code>pip install segger[gpu]\n</code></pre> <p>```{tab-item} macOS</p> <p>To install on macOS, use the following command:</p> <pre><code>pip install segger\n</code></pre> <p>Note that macOS does not natively support CUDA, so GPU support is not available.</p> <pre><code>If you require GPU support, we recommend using Linux or Windows.\n</code></pre> <p>```{tab-item} Windows</p> <p>To install on Windows, use the following command:</p> <pre><code>pip install segger\n</code></pre> <p>For GPU support on Windows:</p> <pre><code>pip install segger[gpu]\n</code></pre> <pre><code>Ensure your CUDA drivers are installed on Windows by using `nvidia-smi`.\n</code></pre> <pre><code>## Installation for Developers\n\nFor developers looking to contribute or work with `segger` in a development environment, you can install the package with development dependencies.\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/EliHei2/segger_dev.git\n    ```\n\n2. Navigate to the project directory:\n    ```bash\n    cd segger_dev\n    ```\n\n3. Install the package with development dependencies:\n    ```bash\n    pip install -e .[dev]\n    ```\n\nThis will install additional dependencies such as `pytest`, `black`, `flake8`, and more.\n\n## Common Installation Issues\n\n```{tip}\nIf you encounter installation issues, ensure that you are using the correct Python version (`&gt;= 3.10`) and that you have the necessary permissions to install packages on your system.\n</code></pre> <p>Some common errors include:</p> <ul> <li>Missing Python version: Ensure you are using <code>Python &gt;= 3.10</code>.</li> <li>Insufficient permissions: Use <code>pip install --user</code> if you do not have admin permissions.</li> <li>Conflicting CUDA drivers: Ensure you have compatible CUDA versions installed if using GPU support.</li> </ul> <p>For further troubleshooting, please refer to the official documentation.</p> <p>For more information, visit the official GitHub repository.</p> <p>```</p>"},{"location":"api/","title":"API Reference","text":"<p>This page contains auto-generated API reference documentation [^1].</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>CLI</li> <li>Data</li> <li>Validation</li> <li>Predict</li> <li>Train Model</li> </ul>"},{"location":"api/data/","title":"segger.data","text":"<p>The <code>segger.data</code> module in Segger is a comprehensive data processing and management system designed specifically for spatial transcriptomics datasets. It provides a unified, scalable interface for handling large-scale spatial transcriptomics data from technologies such as Xenium and Merscope, with a focus on preparing data for graph-based deep learning models.</p> <ul> <li>Sample: Core sample handling, tiling, and data management classes</li> <li>PyG Dataset: PyTorch Geometric dataset integration and utilities</li> <li>Utils: Core utility functions for data processing, filtering, and analysis</li> <li>Transcript Embedding: Transcript feature encoding and embedding utilities</li> <li>NDTree: Spatial partitioning and load balancing utilities</li> </ul>"},{"location":"api/data/#module-overview","title":"Module Overview","text":"<p>The <code>segger.data</code> package is organized into several key modules, each serving a specific purpose in the spatial transcriptomics data processing pipeline.</p>"},{"location":"api/data/#sample-module","title":"Sample Module","text":"<p>The main data processing module containing the core classes for handling spatial transcriptomics data.</p> <p>Main Functions:</p> <ul> <li>Data loading and validation</li> <li>Spatial tiling and partitioning</li> <li>Graph construction and feature engineering</li> <li>Parallel processing coordination</li> </ul> <p>Key Classes:</p> <ul> <li><code>STSampleParquet</code>: Main orchestrator for data loading and processing</li> <li><code>STInMemoryDataset</code>: In-memory dataset with spatial indexing</li> <li><code>STTile</code>: Individual spatial tile processing</li> </ul>"},{"location":"api/data/#utils-module","title":"Utils Module","text":"<p>Core utility functions for data processing, filtering, and analysis.</p> <p>Key Functions:</p> <ul> <li><code>get_xy_extents()</code>: Extract spatial extents from parquet files</li> <li><code>read_parquet_region()</code>: Read specific spatial regions from parquet files</li> <li><code>filter_transcripts()</code>: Filter transcripts by quality and gene type</li> <li><code>filter_boundaries()</code>: Filter boundaries by spatial criteria</li> <li><code>load_settings()</code>: Load technology-specific configurations</li> <li><code>find_markers()</code>: Identify marker genes for cell types</li> </ul>"},{"location":"api/data/#pyg-dataset-module","title":"PyG Dataset Module","text":"<p>PyTorch Geometric dataset integration for machine learning workflows.</p> <ul> <li>Automatic tile discovery and loading</li> <li>PyTorch Lightning integration</li> <li>Built-in data validation</li> <li>Efficient memory management</li> </ul> <p>Key Classes:</p> <ul> <li><code>STPyGDataset</code>: PyTorch Geometric dataset wrapper</li> </ul>"},{"location":"api/data/#transcript-embedding-module","title":"Transcript Embedding Module","text":"<p>Utilities for encoding transcript features into numerical representations.</p>"},{"location":"api/data/#ndtree-module","title":"NDTree Module","text":"<p>Spatial partitioning and load balancing utilities.</p> <p>Key Classes:</p> <ul> <li><code>NDTree</code>: N-dimensional spatial partitioning</li> <li><code>innernode</code>: Internal tree node structure</li> </ul> <p>Features:</p> <ul> <li>Efficient spatial data partitioning</li> <li>Load balancing for parallel processing</li> <li>Memory-optimized data structures</li> <li>Configurable region sizing</li> </ul>"},{"location":"api/data/#configuration-and-settings","title":"Configuration and Settings","text":""},{"location":"api/data/#settings-directory","title":"Settings Directory","text":"<p>Technology-specific configuration files for different spatial transcriptomics platforms.</p> <p>Available Platforms:</p> <ul> <li>Xenium</li> <li>Merscope</li> <li>CosMx</li> <li>Xenium v2 (segmentation kit)</li> </ul> <p>Configuration Options:</p> <ul> <li>Column mappings</li> <li>Quality thresholds</li> <li>Spatial parameters</li> <li>Platform-specific defaults</li> </ul>"},{"location":"api/data/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>Raw Data (Parquet) \u2192 STSampleParquet \u2192 Spatial Tiling \u2192 STInMemoryDataset \u2192 STTile \u2192 PyG Graph\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nMetadata Extraction    Region Division   Data Filtering   Tile Creation   Graph Construction\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nSettings Loading      Load Balancing    Spatial Indexing   Feature Comp.   Edge Generation\n</code></pre>"},{"location":"api/data/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>STSampleParquet (Main Orchestrator)\n\u251c\u2500\u2500 STInMemoryDataset (Region Processing)\n\u2502   \u2514\u2500\u2500 STTile (Individual Tile Processing)\n\u251c\u2500\u2500 TranscriptEmbedding (Feature Encoding)\n\u2514\u2500\u2500 NDTree (Spatial Partitioning)\n\nSTPyGDataset (ML Integration)\n\u2514\u2500\u2500 PyTorch Geometric Integration\n\nBackendHandler (Experimental)\n\u2514\u2500\u2500 Multi-backend Support\n</code></pre>"},{"location":"api/data/#usage","title":"Usage","text":"<pre><code>from segger.data.sample import STSampleParquet\n\n# Load and process data\nsample = STSampleParquet(base_dir=\"path/to/data\", n_workers=4)\nsample.save(data_dir=\"./processed\", tile_size=1000)\n</code></pre>"},{"location":"api/data/_utils/","title":"segger.data._utils","text":"<p>The <code>_utils</code> module provides core utility functions for data processing, filtering, and management in the Segger framework. This module contains essential functions for handling spatial transcriptomics data, including coordinate processing, data filtering, and settings management.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.add_transcript_ids","title":"add_transcript_ids","text":"<pre><code>add_transcript_ids(transcripts_df, x_col, y_col, id_col='transcript_id', precision=1000)\n</code></pre> <p>Add unique transcript IDs to a DataFrame based on x,y coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>DataFrame containing transcript data with x,y coordinates.</p> required <code>x_col</code> <code>str</code> <p>Name of the x-coordinate column.</p> required <code>y_col</code> <code>str</code> <p>Name of the y-coordinate column.</p> required <code>id_col</code> <code>str</code> <p>Name of the column to store the transcript IDs. Defaults to \"transcript_id\".</p> <code>'transcript_id'</code> <code>precision</code> <code>int</code> <p>Precision multiplier for coordinate values to handle floating point precision. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with added transcript_id column.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def add_transcript_ids(\n    transcripts_df: pd.DataFrame,\n    x_col: str,\n    y_col: str,\n    id_col: str = \"transcript_id\",\n    precision: int = 1000,\n) -&gt; pd.DataFrame:\n    \"\"\"Add unique transcript IDs to a DataFrame based on x,y coordinates.\n\n    Args:\n        transcripts_df: DataFrame containing transcript data with x,y coordinates.\n        x_col: Name of the x-coordinate column.\n        y_col: Name of the y-coordinate column.\n        id_col: Name of the column to store the transcript IDs. Defaults to \"transcript_id\".\n        precision: Precision multiplier for coordinate values to handle floating point precision.\n            Defaults to 1000.\n\n    Returns:\n        pd.DataFrame: DataFrame with added transcript_id column.\n    \"\"\"\n    # Create coordinate strings with specified precision\n    x_coords = np.round(transcripts_df[x_col] * precision).astype(int).astype(str)\n    y_coords = np.round(transcripts_df[y_col] * precision).astype(int).astype(str)\n    coords_str = x_coords + \"_\" + y_coords\n\n    # Generate unique IDs using a deterministic hash function\n    def hash_coords(s):\n        \"\"\"Generate a deterministic hash for coordinate strings.\n\n        Args:\n            s: Coordinate string to hash.\n\n        Returns:\n            int: 8-digit integer hash value.\n        \"\"\"\n        # Use a fixed seed for reproducibility\n        seed = 1996\n        # Combine string with seed and take modulo to get an 8-digit integer\n        return abs(hash(s + str(seed))) % 100000000\n\n    tx_ids = np.array([hash_coords(s) for s in coords_str], dtype=np.int32)\n\n    # Add IDs to DataFrame\n    transcripts_df = transcripts_df.copy()\n    transcripts_df[id_col] = tx_ids\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.compute_nuclear_transcripts","title":"compute_nuclear_transcripts","text":"<pre><code>compute_nuclear_transcripts(polygons, transcripts, x_col, y_col, nuclear_column=None, nuclear_value=None)\n</code></pre> <p>Compute which transcripts are nuclear based on their coordinates and the nuclear polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>GeoSeries</code> <p>The nuclear polygons.</p> required <code>transcripts</code> <code>DataFrame</code> <p>The transcripts DataFrame.</p> required <code>x_col</code> <code>str</code> <p>The x-coordinate column name.</p> required <code>y_col</code> <code>str</code> <p>The y-coordinate column name.</p> required <code>nuclear_column</code> <code>str</code> <p>The column name that indicates if a transcript is nuclear. Defaults to None.</p> <code>None</code> <code>nuclear_value</code> <code>str</code> <p>The value in nuclear_column that indicates a nuclear transcript. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A boolean series indicating which transcripts are nuclear.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def compute_nuclear_transcripts(\n    polygons: gpd.GeoSeries,\n    transcripts: pd.DataFrame,\n    x_col: str,\n    y_col: str,\n    nuclear_column: str = None,\n    nuclear_value: str = None,\n) -&gt; pd.Series:\n    \"\"\"Compute which transcripts are nuclear based on their coordinates and the\n    nuclear polygons.\n\n    Args:\n        polygons: The nuclear polygons.\n        transcripts: The transcripts DataFrame.\n        x_col: The x-coordinate column name.\n        y_col: The y-coordinate column name.\n        nuclear_column: The column name that indicates if a transcript is nuclear. Defaults to None.\n        nuclear_value: The value in nuclear_column that indicates a nuclear transcript. Defaults to None.\n\n    Returns:\n        pd.Series: A boolean series indicating which transcripts are nuclear.\n    \"\"\"\n    # If nuclear_column and nuclear_value are provided, use them\n    if nuclear_column is not None and nuclear_value is not None:\n        if nuclear_column in transcripts.columns:\n            return transcripts[nuclear_column].eq(nuclear_value)\n\n    # Otherwise compute based on coordinates\n    points = gpd.GeoSeries(gpd.points_from_xy(transcripts[x_col], transcripts[y_col]))\n    return points.apply(lambda p: any(p.within(poly) for poly in polygons))\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.ensure_transcript_ids","title":"ensure_transcript_ids","text":"<pre><code>ensure_transcript_ids(parquet_path, x_col, y_col, id_col='transcript_id', precision=1000)\n</code></pre> <p>Ensure that a parquet file has transcript IDs by adding them if missing.</p> <p>Parameters:</p> Name Type Description Default <code>parquet_path</code> <code>PathLike</code> <p>Path to the parquet file.</p> required <code>x_col</code> <code>str</code> <p>Name of the x-coordinate column.</p> required <code>y_col</code> <code>str</code> <p>Name of the y-coordinate column.</p> required <code>id_col</code> <code>str</code> <p>Name of the column to store the transcript IDs. Defaults to \"transcript_id\".</p> <code>'transcript_id'</code> <code>precision</code> <code>int</code> <p>Precision multiplier for coordinate values to handle floating point precision. Defaults to 1000.</p> <code>1000</code> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def ensure_transcript_ids(\n    parquet_path: os.PathLike,\n    x_col: str,\n    y_col: str,\n    id_col: str = \"transcript_id\",\n    precision: int = 1000,\n) -&gt; None:\n    \"\"\"Ensure that a parquet file has transcript IDs by adding them if missing.\n\n    Args:\n        parquet_path: Path to the parquet file.\n        x_col: Name of the x-coordinate column.\n        y_col: Name of the y-coordinate column.\n        id_col: Name of the column to store the transcript IDs. Defaults to \"transcript_id\".\n        precision: Precision multiplier for coordinate values to handle floating point precision.\n            Defaults to 1000.\n    \"\"\"\n    # First check metadata to see if column exists\n    metadata = pq.read_metadata(parquet_path)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Only proceed if the column doesn't exist\n    if id_col not in schema_idx:\n        # Read the parquet file\n        df = pd.read_parquet(parquet_path)\n\n        # Add transcript IDs\n        df = add_transcript_ids(df, x_col, y_col, id_col, precision)\n\n        # Convert DataFrame to Arrow table\n        table = pa.Table.from_pandas(df)\n\n        # Write back to parquet\n        pq.write_table(\n            table,\n            parquet_path,\n            version=\"2.6\",  # Use latest stable version\n            write_statistics=True,  # Ensure statistics are written\n            compression=\"snappy\",  # Use snappy compression for better performance\n        )\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.filter_boundaries","title":"filter_boundaries","text":"<pre><code>filter_boundaries(boundaries, inset, outset, x, y, label)\n</code></pre> <p>Filter boundary polygons based on their overlap with specified inset and outset regions.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries</code> <code>DataFrame</code> <p>A DataFrame containing the boundary data with x and y coordinates and identifiers.</p> required <code>inset</code> <code>Polygon</code> <p>A polygon representing the inner region to filter the boundaries.</p> required <code>outset</code> <code>Polygon</code> <p>A polygon representing the outer region to filter the boundaries.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <code>label</code> <code>str</code> <p>The name of the column representing the cell or nucleus label.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the filtered boundary polygons.</p> Note <p>The function determines overlaps of boundary polygons with the specified inset and outset regions. It creates boolean masks for overlaps with the top, left, right, and bottom sides of the outset region, as well as the center region defined by the inset polygon. The filtering logic includes polygons that: - Are completely within the center region. - Overlap with the center and the left side, but not the bottom side. - Overlap with the center and the top side, but not the right side.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def filter_boundaries(\n    boundaries: pd.DataFrame,\n    inset: shapely.Polygon,\n    outset: shapely.Polygon,\n    x: str,\n    y: str,\n    label: str,\n):\n    \"\"\"Filter boundary polygons based on their overlap with specified inset and\n    outset regions.\n\n    Args:\n        boundaries: A DataFrame containing the boundary data with x and y coordinates and\n            identifiers.\n        inset: A polygon representing the inner region to filter the boundaries.\n        outset: A polygon representing the outer region to filter the boundaries.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n        label: The name of the column representing the cell or nucleus label.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered boundary polygons.\n\n    Note:\n        The function determines overlaps of boundary polygons with the specified\n        inset and outset regions. It creates boolean masks for overlaps with the\n        top, left, right, and bottom sides of the outset region, as well as the\n        center region defined by the inset polygon. The filtering logic includes\n        polygons that:\n        - Are completely within the center region.\n        - Overlap with the center and the left side, but not the bottom side.\n        - Overlap with the center and the top side, but not the right side.\n    \"\"\"\n\n    # Determine overlaps of boundary polygons\n    def in_region(region):\n        \"\"\"Check if boundaries are within a specified region.\n\n        Args:\n            region: Shapely polygon defining the region to check.\n\n        Returns:\n            pd.Series: Boolean mask indicating which boundaries are in the region.\n        \"\"\"\n        in_x = boundaries[x].between(region.bounds[0], region.bounds[2])\n        in_y = boundaries[y].between(region.bounds[1], region.bounds[3])\n        return in_x &amp; in_y\n\n    x1, y1, x4, y4 = outset.bounds\n    x2, y2, x3, y3 = inset.bounds\n    boundaries[\"top\"] = in_region(shapely.box(x1, y1, x4, y2))\n    boundaries[\"left\"] = in_region(shapely.box(x1, y1, x2, y4))\n    boundaries[\"right\"] = in_region(shapely.box(x3, y1, x4, y4))\n    boundaries[\"bottom\"] = in_region(shapely.box(x1, y3, x4, y4))\n    boundaries[\"center\"] = in_region(inset)\n\n    # Filter boundary polygons\n    # Include overlaps with top and left, not bottom and right\n    gb = boundaries.groupby(label, sort=False)\n    total = gb[\"center\"].transform(\"size\")\n    in_top = gb[\"top\"].transform(\"sum\")\n    in_left = gb[\"left\"].transform(\"sum\")\n    in_right = gb[\"right\"].transform(\"sum\")\n    in_bottom = gb[\"bottom\"].transform(\"sum\")\n    in_center = gb[\"center\"].transform(\"sum\")\n    keep = in_center == total\n    keep |= (in_center &gt; 0) &amp; (in_left &gt; 0) &amp; (in_bottom == 0)\n    keep |= (in_center &gt; 0) &amp; (in_top &gt; 0) &amp; (in_right == 0)\n    inset_boundaries = boundaries.loc[keep]\n    return inset_boundaries\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, label=None, filter_substrings=None, qv_column=None, min_qv=None)\n</code></pre> <p>Filter transcripts based on quality value and remove unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>label</code> <code>Optional[str]</code> <p>The label of transcript features. Defaults to None.</p> <code>None</code> <code>filter_substrings</code> <code>Optional[List[str]]</code> <p>The list of feature substrings to remove. Defaults to None.</p> <code>None</code> <code>qv_column</code> <code>Optional[str]</code> <p>The name of the column representing the quality value. Defaults to None.</p> <code>None</code> <code>min_qv</code> <code>Optional[float]</code> <p>The minimum quality value threshold for filtering transcripts. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def filter_transcripts(\n    transcripts_df: pd.DataFrame,\n    label: Optional[str] = None,\n    filter_substrings: Optional[List[str]] = None,\n    qv_column: Optional[str] = None,\n    min_qv: Optional[float] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter transcripts based on quality value and remove unwanted transcripts.\n\n    Args:\n        transcripts_df: The dataframe containing transcript data.\n        label: The label of transcript features. Defaults to None.\n        filter_substrings: The list of feature substrings to remove. Defaults to None.\n        qv_column: The name of the column representing the quality value. Defaults to None.\n        min_qv: The minimum quality value threshold for filtering transcripts. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    mask = pd.Series(True, index=transcripts_df.index)\n    if filter_substrings is not None and label is not None:\n        mask &amp;= ~transcripts_df[label].str.startswith(tuple(filter_substrings))\n    if min_qv is not None and qv_column is not None:\n        mask &amp;= transcripts_df[qv_column].ge(min_qv)\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.find_markers","title":"find_markers","text":"<pre><code>find_markers(adata, cell_type_column, pos_percentile=5, neg_percentile=10, percentage=50)\n</code></pre> <p>Identify positive and negative marker genes for each cell type in an AnnData object.</p> <p>Positive markers are top-ranked genes that are expressed in at least <code>percentage</code> percent of cells in the given cell type.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>Annotated data object containing gene expression data and cell type annotations.</p> required <code>cell_type_column</code> <code>str</code> <p>Name of the column in <code>adata.obs</code> specifying cell type identity for each cell.</p> required <code>pos_percentile</code> <code>float</code> <p>Percentile threshold for selecting top highly expressed genes as positive markers. Defaults to 5.</p> <code>5</code> <code>neg_percentile</code> <code>float</code> <p>Percentile threshold for selecting lowest expressed genes as negative markers. Defaults to 10.</p> <code>10</code> <code>percentage</code> <code>float</code> <p>Minimum percent of cells (0-100) in a cell type expressing a gene for it to be a marker. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Dict[str, List[str]]]</code> <p>Dictionary mapping cell type names to: {     'positive': [list of positive marker gene names],     'negative': [list of negative marker gene names] }</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def find_markers(\n    adata: ad.AnnData,\n    cell_type_column: str,\n    pos_percentile: float = 5,\n    neg_percentile: float = 10,\n    percentage: float = 50,\n) -&gt; Dict[str, Dict[str, List[str]]]:\n    \"\"\"Identify positive and negative marker genes for each cell type in an AnnData object.\n\n    Positive markers are top-ranked genes that are expressed in at least\n    `percentage` percent of cells in the given cell type.\n\n    Args:\n        adata: Annotated data object containing gene expression data and cell type annotations.\n        cell_type_column: Name of the column in `adata.obs` specifying cell type identity for each cell.\n        pos_percentile: Percentile threshold for selecting top highly expressed genes as positive markers. Defaults to 5.\n        neg_percentile: Percentile threshold for selecting lowest expressed genes as negative markers. Defaults to 10.\n        percentage: Minimum percent of cells (0-100) in a cell type expressing a gene for it to be a marker. Defaults to 50.\n\n    Returns:\n        dict: Dictionary mapping cell type names to:\n            {\n                'positive': [list of positive marker gene names],\n                'negative': [list of negative marker gene names]\n            }\n    \"\"\"\n    markers = {}\n    sc.tl.rank_genes_groups(adata, groupby=cell_type_column)\n    genes = np.array(adata.var_names)\n    n_genes = adata.shape[1]\n\n    # Work with a dense matrix for expression fraction calculation\n    # (convert sparse to dense if needed)\n    if not isinstance(adata.X, np.ndarray):\n        expr_matrix = adata.X.toarray()\n    else:\n        expr_matrix = adata.X\n\n    for cell_type in adata.obs[cell_type_column].unique():\n        mask = (adata.obs[cell_type_column] == cell_type).values\n        gene_names = np.array(adata.uns['rank_genes_groups']['names'][cell_type])\n\n        n_pos = max(1, int(n_genes * pos_percentile // 100))\n        n_neg = max(1, int(n_genes * neg_percentile // 100))\n\n        # Calculate percent of cells in this cell type expressing each gene\n        expr_frac = (expr_matrix[mask] &gt; 0).mean(axis=0) * 100  # as percent\n\n        # Filter positive markers by expression fraction\n        pos_indices = []\n        for idx in range(n_pos):\n            gene = gene_names[idx]\n            gene_idx = np.where(genes == gene)[0][0]\n            if expr_frac[gene_idx] &gt;= percentage:\n                pos_indices.append(idx)\n        positive_markers = list(gene_names[pos_indices])\n\n        # Negative markers are the lowest-ranked\n        negative_markers = list(gene_names[-n_neg:])\n\n        markers[cell_type] = {\n            \"positive\": positive_markers,\n            \"negative\": negative_markers\n        }\n    return markers\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.find_mutually_exclusive_genes","title":"find_mutually_exclusive_genes","text":"<pre><code>find_mutually_exclusive_genes(adata, markers, cell_type_column)\n</code></pre> <p>Identify mutually exclusive genes based on expression criteria.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>Annotated data object containing gene expression data.</p> required <code>markers</code> <code>Dict[str, Dict[str, List[str]]]</code> <p>Dictionary where keys are cell types and values are dictionaries containing: 'positive': list of top x% highly expressed genes 'negative': list of top x% lowly expressed genes.</p> required <code>cell_type_column</code> <code>str</code> <p>Column name in <code>adata.obs</code> that specifies cell types.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Tuple[str, str]]</code> <p>List of mutually exclusive gene pairs.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def find_mutually_exclusive_genes(\n    adata: ad.AnnData, markers: Dict[str, Dict[str, List[str]]], cell_type_column: str\n) -&gt; List[Tuple[str, str]]:\n    \"\"\"Identify mutually exclusive genes based on expression criteria.\n\n    Args:\n        adata: Annotated data object containing gene expression data.\n        markers: Dictionary where keys are cell types and values are dictionaries containing:\n            'positive': list of top x% highly expressed genes\n            'negative': list of top x% lowly expressed genes.\n        cell_type_column: Column name in `adata.obs` that specifies cell types.\n\n    Returns:\n        list: List of mutually exclusive gene pairs.\n    \"\"\"\n    exclusive_genes = {}\n    all_exclusive = []\n    gene_expression = adata.to_df()\n    for cell_type, marker_sets in markers.items():\n        positive_markers = marker_sets[\"positive\"]\n        exclusive_genes[cell_type] = []\n        for gene in positive_markers:\n            gene_expr = adata[:, gene].X\n            cell_type_mask = adata.obs[cell_type_column] == cell_type\n            non_cell_type_mask = ~cell_type_mask\n            if (gene_expr[cell_type_mask] &gt; 0).mean() &gt; 0.2 and (\n                gene_expr[non_cell_type_mask] &gt; 0\n            ).mean() &lt; 0.05:\n                exclusive_genes[cell_type].append(gene)\n                all_exclusive.append(gene)\n    unique_genes = list(\n        {\n            gene\n            for i in exclusive_genes.keys()\n            for gene in exclusive_genes[i]\n            if gene in all_exclusive\n        }\n    )\n    filtered_exclusive_genes = {\n        i: [gene for gene in exclusive_genes[i] if gene in unique_genes]\n        for i in exclusive_genes.keys()\n    }\n    mutually_exclusive_gene_pairs = [\n        tuple(sorted((gene1, gene2)))\n        for key1, key2 in combinations(filtered_exclusive_genes.keys(), 2)\n        if key1 != key2\n        for gene1 in filtered_exclusive_genes[key1]\n        for gene2 in filtered_exclusive_genes[key2]\n    ]\n    return set(mutually_exclusive_gene_pairs)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.get_polygons_from_xy","title":"get_polygons_from_xy","text":"<pre><code>get_polygons_from_xy(boundaries, x, y, label, scale_factor=1.0)\n</code></pre> <p>Convert boundary coordinates from a DataFrame to a GeoSeries of polygons.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries</code> <code>DataFrame</code> <p>A DataFrame containing the boundary data with x and y coordinates and identifiers.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <code>label</code> <code>str</code> <p>The name of the column representing the cell or nucleus label.</p> required <code>scale_factor</code> <code>float</code> <p>A ratio to scale the polygons. A value of 1.0 means no change, greater than 1.0 expands the polygons, and less than 1.0 shrinks the polygons. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>GeoSeries</code> <p>gpd.GeoSeries: A GeoSeries containing the polygons created from the boundary coordinates.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get_polygons_from_xy(\n    boundaries: pd.DataFrame,\n    x: str,\n    y: str,\n    label: str,\n    scale_factor: float = 1.0,\n) -&gt; gpd.GeoSeries:\n    \"\"\"Convert boundary coordinates from a DataFrame to a GeoSeries of polygons.\n\n    Args:\n        boundaries: A DataFrame containing the boundary data with x and y coordinates\n            and identifiers.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n        label: The name of the column representing the cell or nucleus label.\n        scale_factor: A ratio to scale the polygons. A value of 1.0 means no change,\n            greater than 1.0 expands the polygons, and less than 1.0 shrinks the polygons.\n            Defaults to 1.0.\n\n    Returns:\n        gpd.GeoSeries: A GeoSeries containing the polygons created from the boundary\n            coordinates.\n    \"\"\"\n    # Polygon offsets in coords\n    ids = boundaries[label].values\n    splits = np.where(ids[:-1] != ids[1:])[0] + 1\n    geometry_offset = np.hstack([0, splits, len(ids)])\n    part_offset = np.arange(len(np.unique(ids)) + 1)\n\n    # Convert to GeoSeries of polygons\n    polygons = shapely.from_ragged_array(\n        shapely.GeometryType.POLYGON,\n        coords=boundaries[[x, y]].values.copy(order=\"C\"),\n        offsets=(geometry_offset, part_offset),\n    )\n    gs = gpd.GeoSeries(polygons, index=np.unique(ids))\n\n    # print(gs)\n\n    if scale_factor != 1.0:\n        # Scale polygons around their centroid\n        gs = gpd.GeoSeries(\n            [\n                scale(geom, xfact=scale_factor, yfact=scale_factor, origin='centroid')\n                for geom in gs\n            ],\n            index=gs.index,\n        )\n        # print(gs)\n\n    return gs\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p> <p>If the min/max statistics are not available, compute them efficiently from the data.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <p>The path to the Parquet file.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <p>Returns:</p> Type Description <code>Polygon</code> <p>shapely.Polygon: A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; shapely.Polygon:\n    \"\"\"Get the bounding box of the x and y coordinates from a Parquet file.\n\n    If the min/max statistics are not available, compute them efficiently from the data.\n\n    Args:\n        filepath: The path to the Parquet file.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n\n    Returns:\n        shapely.Polygon: A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Process row groups\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    group = metadata.row_group(0)\n    try:\n        for i in range(metadata.num_row_groups):\n            # print('*1')\n            group = metadata.row_group(i)\n            x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n            x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n            y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n            y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n            bounds = shapely.box(x_min, y_min, x_max, y_max)\n    # If statistics are not available, compute them manually from the data\n    except:\n        import gc\n\n        print(\n            \"metadata lacks the statistics of the tile's bounding box, computing might take longer!\"\n        )\n        parquet_file = pd.read_parquet(filepath)\n        x_col = parquet_file.loc[:, x]\n        y_col = parquet_file.loc[:, y]\n        del parquet_file\n        gc.collect()\n        x_min = x_col.min()\n        y_min = y_col.min()\n        x_max = x_col.max()\n        y_max = y_col.max()\n    bounds = shapely.geometry.box(x_min, y_min, x_max, y_max)\n    return bounds\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.load_settings","title":"load_settings","text":"<pre><code>load_settings(sample_type)\n</code></pre> <p>Load a matching YAML file from the _settings/ directory and convert its contents into a SimpleNamespace.</p> <p>Parameters:</p> Name Type Description Default <code>sample_type</code> <code>str</code> <p>Name of the sample type to load (case-insensitive).</p> required <p>Returns:</p> Name Type Description <code>SimpleNamespace</code> <code>SimpleNamespace</code> <p>The settings loaded from the YAML file as a SimpleNamespace.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>sample_type</code> does not match any filenames.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def load_settings(sample_type: str) -&gt; SimpleNamespace:\n    \"\"\"Load a matching YAML file from the _settings/ directory and convert its\n    contents into a SimpleNamespace.\n\n    Args:\n        sample_type: Name of the sample type to load (case-insensitive).\n\n    Returns:\n        SimpleNamespace: The settings loaded from the YAML file as a SimpleNamespace.\n\n    Raises:\n        FileNotFoundError: If `sample_type` does not match any filenames.\n    \"\"\"\n    settings_dir = Path(__file__).parent.resolve() / \"_settings\"\n    # Get a list of YAML filenames (without extensions) in the _settings dir\n    filenames = [file.stem for file in settings_dir.glob(\"*.yaml\")]\n    # Convert sample_type to lowercase and check if it matches any filename\n    sample_type = sample_type.lower()\n    if sample_type not in filenames:\n        msg = (\n            f\"Sample type '{sample_type}' not found in settings. \"\n            f\"Available options: {', '.join(filenames)}\"\n        )\n        raise FileNotFoundError(msg)\n    # Load the matching YAML file\n    yaml_file_path = settings_dir / f\"{sample_type}.yaml\"\n    with yaml_file_path.open(\"r\") as file:\n        data = yaml.safe_load(file)\n\n    # Convert the YAML data into a SimpleNamespace recursively\n    return _dict_to_namespace(data)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.read_parquet_region","title":"read_parquet_region","text":"<pre><code>read_parquet_region(filepath, x, y, bounds=None, extra_columns=[], extra_filters=[], row_group_chunksize=None)\n</code></pre> <p>Read a region from a Parquet file based on x and y coordinates and optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <p>The path to the Parquet file.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <code>bounds</code> <code>Polygon</code> <p>A polygon representing the bounding box to filter the data. If None, no bounding box filter is applied. Defaults to None.</p> <code>None</code> <code>extra_columns</code> <code>list[str]</code> <p>A list of additional columns to include in the output DataFrame. Defaults to [].</p> <code>[]</code> <code>extra_filters</code> <code>list[str]</code> <p>A list of additional filters to apply to the data. Defaults to [].</p> <code>[]</code> <code>row_group_chunksize</code> <code>Optional[int]</code> <p>Chunk size for row group processing. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the filtered data from the Parquet file.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def read_parquet_region(\n    filepath,\n    x: str,\n    y: str,\n    bounds: shapely.Polygon = None,\n    extra_columns: list[str] = [],\n    extra_filters: list[str] = [],\n    row_group_chunksize: Optional[int] = None,\n):\n    \"\"\"Read a region from a Parquet file based on x and y coordinates and optional\n    filters.\n\n    Args:\n        filepath: The path to the Parquet file.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n        bounds: A polygon representing the bounding box to filter the data. If None,\n            no bounding box filter is applied. Defaults to None.\n        extra_columns: A list of additional columns to include in the output DataFrame. Defaults to [].\n        extra_filters: A list of additional filters to apply to the data. Defaults to [].\n        row_group_chunksize: Chunk size for row group processing. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered data from the Parquet file.\n    \"\"\"\n    # Check backend and load dependencies if not already loaded\n\n    # Find bounds of full file if not supplied\n    if bounds is None:\n        bounds = get_xy_bounds(filepath, x, y)\n\n    # Load pre-filtered data from Parquet file\n    filters = [\n        [\n            (x, \"&gt;\", bounds.bounds[0]),\n            (y, \"&gt;\", bounds.bounds[1]),\n            (x, \"&lt;\", bounds.bounds[2]),\n            (y, \"&lt;\", bounds.bounds[3]),\n        ]\n        + extra_filters\n    ]\n\n    columns = list({x, y} | set(extra_columns))\n\n    # Check if 'Geometry', 'geometry', 'polygon', or 'Polygon' is in the columns\n    if any(col in columns for col in ['Geometry', 'geometry', 'polygon', 'Polygon']):\n        import geopandas as gpd\n        # If geometry columns are present, read with geopandas\n        region = gpd.read_parquet(\n            filepath,\n            filters=filters,\n            columns=columns,\n        )\n    else:\n        # Otherwise, read with pandas\n        region = pd.read_parquet(\n            filepath,\n            filters=filters,\n            columns=columns,\n        )\n    return region\n</code></pre>"},{"location":"api/data/api_reference/","title":"Data Module API Reference","text":"<p>This page provides a comprehensive reference to all the classes, functions, and modules in the <code>segger.data</code> package.</p>"},{"location":"api/data/api_reference/#module-overview","title":"Module Overview","text":"<p>The <code>segger.data</code> package is organized into several key modules, each serving a specific purpose in the spatial transcriptomics data processing pipeline.</p>"},{"location":"api/data/api_reference/#core-modules","title":"Core Modules","text":""},{"location":"api/data/api_reference/#sample-module","title":"Sample Module","text":"<p>The main data processing module containing the core classes for handling spatial transcriptomics data.</p> <p>Key Classes: - <code>STSampleParquet</code>: Main orchestrator for data loading and processing - <code>STInMemoryDataset</code>: In-memory dataset with spatial indexing - <code>STTile</code>: Individual spatial tile processing</p> <p>Main Functions: - Data loading and validation - Spatial tiling and partitioning - Graph construction and feature engineering - Parallel processing coordination</p>"},{"location":"api/data/api_reference/#utils-module","title":"Utils Module","text":"<p>Core utility functions for data processing, filtering, and analysis.</p> <p>Key Functions: - <code>get_xy_extents()</code>: Extract spatial extents from parquet files - <code>read_parquet_region()</code>: Read specific spatial regions from parquet files - <code>filter_transcripts()</code>: Filter transcripts by quality and gene type - <code>filter_boundaries()</code>: Filter boundaries by spatial criteria - <code>load_settings()</code>: Load technology-specific configurations - <code>find_markers()</code>: Identify marker genes for cell types</p>"},{"location":"api/data/api_reference/#pyg-dataset-module","title":"PyG Dataset Module","text":"<p>PyTorch Geometric dataset integration for machine learning workflows.</p> <p>Key Classes: - <code>STPyGDataset</code>: PyTorch Geometric dataset wrapper</p> <p>Features: - Automatic tile discovery and loading - PyTorch Lightning integration - Built-in data validation - Efficient memory management</p>"},{"location":"api/data/api_reference/#transcript-embedding-module","title":"Transcript Embedding Module","text":"<p>Utilities for encoding transcript features into numerical representations.</p> <p>Key Classes: - <code>TranscriptEmbedding</code>: Transcript feature encoding and embedding</p> <p>Features: - Index-based and weighted embeddings - PyTorch module integration - Input validation and error handling - Support for custom embedding strategies</p>"},{"location":"api/data/api_reference/#experimental-module","title":"Experimental Module","text":"<p>Experimental features and multi-backend support.</p> <p>Key Classes: - <code>BackendHandler</code>: Multi-backend DataFrame support</p> <p>Supported Backends: - pandas: Standard data processing - dask: Parallel processing - cudf: GPU acceleration - dask_cudf: Distributed GPU processing</p>"},{"location":"api/data/api_reference/#ndtree-module","title":"NDTree Module","text":"<p>Spatial partitioning and load balancing utilities.</p> <p>Key Classes: - <code>NDTree</code>: N-dimensional spatial partitioning - <code>innernode</code>: Internal tree node structure</p> <p>Features: - Efficient spatial data partitioning - Load balancing for parallel processing - Memory-optimized data structures - Configurable region sizing</p>"},{"location":"api/data/api_reference/#configuration-and-settings","title":"Configuration and Settings","text":""},{"location":"api/data/api_reference/#settings-directory","title":"Settings Directory","text":"<p>Technology-specific configuration files for different spatial transcriptomics platforms.</p> <p>Available Platforms: - Xenium - Merscope - CosMx - Xenium v2</p> <p>Configuration Options: - Column mappings - Quality thresholds - Spatial parameters - Platform-specific defaults</p>"},{"location":"api/data/api_reference/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>Raw Data (Parquet) \u2192 STSampleParquet \u2192 Spatial Tiling \u2192 STInMemoryDataset \u2192 STTile \u2192 PyG Graph\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nMetadata Extraction    Region Division   Data Filtering   Tile Creation   Graph Construction\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nSettings Loading      Load Balancing    Spatial Indexing   Feature Comp.   Edge Generation\n</code></pre>"},{"location":"api/data/api_reference/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>STSampleParquet (Main Orchestrator)\n\u251c\u2500\u2500 STInMemoryDataset (Region Processing)\n\u2502   \u2514\u2500\u2500 STTile (Individual Tile Processing)\n\u251c\u2500\u2500 TranscriptEmbedding (Feature Encoding)\n\u2514\u2500\u2500 NDTree (Spatial Partitioning)\n\nSTPyGDataset (ML Integration)\n\u2514\u2500\u2500 PyTorch Geometric Integration\n\nBackendHandler (Experimental)\n\u2514\u2500\u2500 Multi-backend Support\n</code></pre>"},{"location":"api/data/api_reference/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"api/data/api_reference/#1-basic-data-processing","title":"1. Basic Data Processing","text":"<pre><code>from segger.data.sample import STSampleParquet\n\n# Load and process data\nsample = STSampleParquet(base_dir=\"path/to/data\", n_workers=4)\nsample.save(data_dir=\"./processed\", tile_size=1000)\n</code></pre>"},{"location":"api/data/api_reference/#2-custom-processing","title":"2. Custom Processing","text":"<pre><code>from segger.data.sample import STInMemoryDataset, STTile\n\n# Process specific regions\ndataset = STInMemoryDataset(sample=sample, extents=region)\ntiles = dataset._tile(width=100, height=100)\n\n# Process individual tiles\nfor tile in tiles:\n    pyg_data = tile.to_pyg_dataset()\n    # Continue processing...\n</code></pre>"},{"location":"api/data/api_reference/#3-machine-learning-integration","title":"3. Machine Learning Integration","text":"<pre><code>from segger.data.pyg_dataset import STPyGDataset\n\n# Load processed data\ndataset = STPyGDataset(root=\"./processed_data\")\n\n# Use in training\nfor data in dataset:\n    # Train your model...\n</code></pre>"},{"location":"api/data/api_reference/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/data/api_reference/#memory-management","title":"Memory Management","text":"<ul> <li>Lazy Loading: Data loaded only when needed</li> <li>Spatial Filtering: Only relevant data in memory</li> <li>Intelligent Caching: Frequently accessed data cached</li> <li>Parallel Processing: Multiple workers process different regions</li> </ul>"},{"location":"api/data/api_reference/#optimization-strategies","title":"Optimization Strategies","text":"<ul> <li>Tile Size Selection: Balance memory usage and processing efficiency</li> <li>Worker Configuration: Match worker count to available resources</li> <li>Spatial Indexing: KDTree-based fast spatial queries</li> <li>Quality Filtering: Early filtering to reduce processing overhead</li> </ul>"},{"location":"api/data/api_reference/#error-handling","title":"Error Handling","text":"<p>The package includes comprehensive error handling:</p> <ul> <li>File Validation: Checks for valid data files and formats</li> <li>Data Quality: Validates transcript and boundary data</li> <li>Memory Management: Handles out-of-memory situations gracefully</li> <li>Worker Coordination: Manages worker failures and recovery</li> </ul>"},{"location":"api/data/api_reference/#best-practices","title":"Best Practices","text":""},{"location":"api/data/api_reference/#data-organization","title":"Data Organization","text":"<ul> <li>Use consistent directory structures</li> <li>Include metadata in file names</li> <li>Validate data quality before processing</li> <li>Monitor memory usage during processing</li> </ul>"},{"location":"api/data/api_reference/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Choose appropriate tile sizes for your hardware</li> <li>Balance worker count with available resources</li> <li>Use debug mode for initial setup and troubleshooting</li> <li>Monitor processing times and adjust parameters</li> </ul>"},{"location":"api/data/api_reference/#quality-control","title":"Quality Control","text":"<ul> <li>Set appropriate quality thresholds</li> <li>Filter unwanted genes early in the pipeline</li> <li>Validate spatial relationships</li> <li>Check output data consistency</li> </ul>"},{"location":"api/data/api_reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/data/api_reference/#common-issues","title":"Common Issues","text":"<ol> <li>Memory Errors: Reduce tile size or number of workers</li> <li>Slow Processing: Check spatial indexing and parallelization</li> <li>Data Quality Issues: Verify input data format and quality metrics</li> <li>Worker Failures: Check worker configuration and resource availability</li> </ol>"},{"location":"api/data/api_reference/#debug-tips","title":"Debug Tips","text":"<ul> <li>Use <code>save_debug()</code> for detailed processing information</li> <li>Monitor memory usage during processing</li> <li>Check tile generation parameters</li> <li>Validate spatial relationships in output data</li> </ul>"},{"location":"api/data/api_reference/#future-developments","title":"Future Developments","text":"<p>Planned improvements include:</p> <ul> <li>Additional Platforms: Support for more spatial transcriptomics technologies</li> <li>Enhanced Tiling: More sophisticated spatial partitioning strategies</li> <li>Real-time Processing: Streaming data processing capabilities</li> <li>Cloud Integration: Support for cloud-based data processing</li> <li>Advanced Features: More sophisticated feature engineering and graph construction</li> </ul>"},{"location":"api/data/api_reference/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Areas for improvement include:</p> <ul> <li>New Data Formats: Support for additional spatial transcriptomics platforms</li> <li>Performance Optimization: Better algorithms and data structures</li> <li>Documentation: Examples, tutorials, and best practices</li> <li>Testing: Comprehensive test coverage and validation</li> </ul>"},{"location":"api/data/api_reference/#dependencies","title":"Dependencies","text":""},{"location":"api/data/api_reference/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>pandas: Data manipulation and analysis</li> <li>geopandas: Spatial data handling</li> <li>shapely: Geometric operations</li> <li>pyarrow: Parquet file handling</li> <li>scipy: Spatial data structures</li> </ul>"},{"location":"api/data/api_reference/#ml-dependencies","title":"ML Dependencies","text":"<ul> <li>torch: PyTorch integration</li> <li>torch_geometric: Graph neural network support</li> <li>pytorch_lightning: Training framework integration</li> </ul>"},{"location":"api/data/api_reference/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>dask: Parallel processing support</li> <li>cudf: GPU acceleration</li> <li>scanpy: Single-cell analysis</li> <li>anndata: Annotated data structures</li> </ul>"},{"location":"api/data/ndtree/","title":"segger.data.ndtree","text":"<p>The <code>ndtree</code> module provides a specialized data structure for spatially partitioning multi-dimensional data. This module implements an NDTree (N-dimensional tree) that efficiently splits spatial data into balanced regions for parallel processing.</p>"},{"location":"api/data/ndtree/#src.segger.data._ndtree","title":"_ndtree","text":""},{"location":"api/data/ndtree/#src.segger.data._ndtree.NDTree","title":"NDTree","text":"<p>NDTree is a data structure for recursively splitting multi-dimensional data into smaller regions until each leaf node contains less than or equal to a specified number of points. It stores these regions in a balanced binary tree.</p> <p>Attributes:</p> Name Type Description <code>data</code> <p>The input data to be partitioned.</p> <code>n</code> <p>The maximum number of points allowed in a leaf node.</p> <code>idx</code> <p>The indices of the input data points.</p> <code>boxes</code> <p>A list to store the bounding boxes (as shapely polygons) of each region in the tree.</p> <code>rect</code> <p>The bounding box of the entire input data space.</p> <code>tree</code> <p>The root of the NDTree.</p> Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>class NDTree:\n    \"\"\"NDTree is a data structure for recursively splitting multi-dimensional data\n    into smaller regions until each leaf node contains less than or equal to a\n    specified number of points. It stores these regions in a balanced binary\n    tree.\n\n    Attributes:\n        data: The input data to be partitioned.\n        n: The maximum number of points allowed in a leaf node.\n        idx: The indices of the input data points.\n        boxes: A list to store the bounding boxes (as shapely polygons) of each region\n            in the tree.\n        rect: The bounding box of the entire input data space.\n        tree: The root of the NDTree.\n    \"\"\"\n\n    def __init__(self, data, n):\n        \"\"\"Initialize the NDTree with the given data and maximum points per leaf\n        node.\n\n        Args:\n            data: The input data to be partitioned.\n            n: The maximum number of points allowed in a leaf node.\n        \"\"\"\n        self.data = np.asarray(data)\n        self.n = n\n        self.idx = np.arange(data.shape[0])\n        self.boxes = []\n        self.rect = Rectangle(data.min(0), data.max(0))\n        self.tree = innernode(self.n, self.idx, self.rect, self)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.NDTree.__init__","title":"__init__","text":"<pre><code>__init__(data, n)\n</code></pre> <p>Initialize the NDTree with the given data and maximum points per leaf node.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The input data to be partitioned.</p> required <code>n</code> <p>The maximum number of points allowed in a leaf node.</p> required Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>def __init__(self, data, n):\n    \"\"\"Initialize the NDTree with the given data and maximum points per leaf\n    node.\n\n    Args:\n        data: The input data to be partitioned.\n        n: The maximum number of points allowed in a leaf node.\n    \"\"\"\n    self.data = np.asarray(data)\n    self.n = n\n    self.idx = np.arange(data.shape[0])\n    self.boxes = []\n    self.rect = Rectangle(data.min(0), data.max(0))\n    self.tree = innernode(self.n, self.idx, self.rect, self)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.innernode","title":"innernode","text":"<p>Represent a node in the NDTree. Each node either stores a bounding box for the data it contains (leaf nodes) or splits the data into two child nodes.</p> <p>Attributes:</p> Name Type Description <code>n</code> <p>The maximum number of points allowed in a leaf node for this subtree.</p> <code>idx</code> <p>The indices of the data points in this node.</p> <code>tree</code> <p>The reference to the main NDTree that holds the data and bounding boxes.</p> <code>rect</code> <p>The bounding box of the data points in this node.</p> <code>split_dim</code> <p>The dimension along which the node splits the data.</p> <code>split_point</code> <p>The value along the split dimension used to divide the data.</p> <code>less</code> <p>The child node containing data points less than or equal to the split point.</p> <code>greater</code> <p>The child node containing data points greater than the split point.</p> Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>class innernode:\n    \"\"\"Represent a node in the NDTree. Each node either stores a bounding box for\n    the data it contains (leaf nodes) or splits the data into two child nodes.\n\n    Attributes:\n        n: The maximum number of points allowed in a leaf node for this subtree.\n        idx: The indices of the data points in this node.\n        tree: The reference to the main NDTree that holds the data and bounding boxes.\n        rect: The bounding box of the data points in this node.\n        split_dim: The dimension along which the node splits the data.\n        split_point: The value along the split dimension used to divide the data.\n        less: The child node containing data points less than or equal to the split\n            point.\n        greater: The child node containing data points greater than the split point.\n    \"\"\"\n\n    def __init__(self, n, idx, rect, tree):\n        \"\"\"Initialize the innernode and split the data if necessary.\n\n        Args:\n            n: The maximum number of points allowed in a leaf node.\n            idx: The indices of the data points in this node.\n            rect: The bounding box of the data points in this node.\n            tree: The reference to the main NDTree.\n        \"\"\"\n        self.n = n\n        self.idx = idx\n        self.tree = tree\n        self.rect = rect\n        if not n == 1:\n            self.split()\n        else:\n            box = shapely.box(*self.rect.mins, *self.rect.maxes)\n            self.tree.boxes.append(box)\n\n    def split(self):\n        \"\"\"Recursively split the node's data into two child nodes along the\n        dimension with the largest spread.\n        \"\"\"\n        less = math.floor(self.n // 2)\n        greater = self.n - less\n        data = self.tree.data[self.idx]\n        self.split_dim = np.argmax(self.rect.maxes - self.rect.mins)\n        data = data[:, self.split_dim]\n        self.split_point = np.quantile(data, less / (less + greater))\n        mask = data &lt;= self.split_point\n        less_rect, greater_rect = self.rect.split(self.split_dim, self.split_point)\n        self.less = innernode(less, self.idx[mask], less_rect, self.tree)\n        self.greater = innernode(greater, self.idx[~mask], greater_rect, self.tree)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.innernode.__init__","title":"__init__","text":"<pre><code>__init__(n, idx, rect, tree)\n</code></pre> <p>Initialize the innernode and split the data if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>The maximum number of points allowed in a leaf node.</p> required <code>idx</code> <p>The indices of the data points in this node.</p> required <code>rect</code> <p>The bounding box of the data points in this node.</p> required <code>tree</code> <p>The reference to the main NDTree.</p> required Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>def __init__(self, n, idx, rect, tree):\n    \"\"\"Initialize the innernode and split the data if necessary.\n\n    Args:\n        n: The maximum number of points allowed in a leaf node.\n        idx: The indices of the data points in this node.\n        rect: The bounding box of the data points in this node.\n        tree: The reference to the main NDTree.\n    \"\"\"\n    self.n = n\n    self.idx = idx\n    self.tree = tree\n    self.rect = rect\n    if not n == 1:\n        self.split()\n    else:\n        box = shapely.box(*self.rect.mins, *self.rect.maxes)\n        self.tree.boxes.append(box)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.innernode.split","title":"split","text":"<pre><code>split()\n</code></pre> <p>Recursively split the node's data into two child nodes along the dimension with the largest spread.</p> Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>def split(self):\n    \"\"\"Recursively split the node's data into two child nodes along the\n    dimension with the largest spread.\n    \"\"\"\n    less = math.floor(self.n // 2)\n    greater = self.n - less\n    data = self.tree.data[self.idx]\n    self.split_dim = np.argmax(self.rect.maxes - self.rect.mins)\n    data = data[:, self.split_dim]\n    self.split_point = np.quantile(data, less / (less + greater))\n    mask = data &lt;= self.split_point\n    less_rect, greater_rect = self.rect.split(self.split_dim, self.split_point)\n    self.less = innernode(less, self.idx[mask], less_rect, self.tree)\n    self.greater = innernode(greater, self.idx[~mask], greater_rect, self.tree)\n</code></pre>"},{"location":"api/data/pyg_dataset/","title":"segger.data.pyg_dataset","text":"<p>The <code>pyg_dataset</code> module provides PyTorch Geometric (PyG) dataset integration for spatial transcriptomics data. This module enables seamless integration between Segger's spatial data processing and PyTorch-based machine learning workflows.</p>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset","title":"pyg_dataset","text":""},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset","title":"STPyGDataset","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>An in-memory dataset class for handling training using spatial transcriptomics data.</p> Source code in <code>src/segger/data/pyg_dataset.py</code> <pre><code>class STPyGDataset(InMemoryDataset):\n    \"\"\"An in-memory dataset class for handling training using spatial\n    transcriptomics data.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        transform: Optional[Callable] = None,\n        pre_transform: Optional[Callable] = None,\n        pre_filter: Optional[Callable] = None,\n    ):\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n    @property\n    def raw_file_names(self) -&gt; List[str]:\n        \"\"\"Return a list of raw file names in the raw directory.\n\n        Returns:\n            List[str]: List of raw file names.\n        \"\"\"\n        return os.listdir(self.raw_dir)\n\n    @property\n    def processed_file_names(self) -&gt; List[str]:\n        \"\"\"Return a list of processed file names in the processed directory.\n\n        Returns:\n            List[str]: List of processed file names.\n        \"\"\"\n        paths = glob.glob(f\"{self.processed_dir}/tiles_x*_y*_*_*.pt\")\n        # paths = paths.append(paths = glob.glob(f'{self.processed_dir}/tiles_x*_y*_*_*.pt'))\n        file_names = list(map(os.path.basename, paths))\n        return file_names\n\n    def len(self) -&gt; int:\n        \"\"\"Return the number of processed files.\n\n        Returns:\n            int: Number of processed files.\n        \"\"\"\n        return len(self.processed_file_names)\n\n    def get(self, idx: int) -&gt; Data:\n        \"\"\"Get a processed data object.\n\n        Args:\n            idx: Index of the data object to retrieve.\n\n        Returns:\n            Data: The processed data object.\n        \"\"\"\n        filepath = Path(self.processed_dir) / self.processed_file_names[idx]\n        data = torch.load(filepath)\n        # this is an issue in PyG's RandomLinkSplit, dimensions are not consistent if there is only one edge in the graph\n        if hasattr(data[\"tx\", \"belongs\", \"bd\"], \"edge_label_index\"):\n            if data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 1:\n                data[\"tx\", \"belongs\", \"bd\"].edge_label_index = data[\n                    \"tx\", \"belongs\", \"bd\"\n                ].edge_label_index.unsqueeze(1)\n                data[\"tx\", \"belongs\", \"bd\"].edge_label = data[\n                    \"tx\", \"belongs\", \"bd\"\n                ].edge_label.unsqueeze(0)\n            assert data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 2\n        return data\n</code></pre>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/pyg_dataset.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx: Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    filepath = Path(self.processed_dir) / self.processed_file_names[idx]\n    data = torch.load(filepath)\n    # this is an issue in PyG's RandomLinkSplit, dimensions are not consistent if there is only one edge in the graph\n    if hasattr(data[\"tx\", \"belongs\", \"bd\"], \"edge_label_index\"):\n        if data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 1:\n            data[\"tx\", \"belongs\", \"bd\"].edge_label_index = data[\n                \"tx\", \"belongs\", \"bd\"\n            ].edge_label_index.unsqueeze(1)\n            data[\"tx\", \"belongs\", \"bd\"].edge_label = data[\n                \"tx\", \"belongs\", \"bd\"\n            ].edge_label.unsqueeze(0)\n        assert data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 2\n    return data\n</code></pre>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/pyg_dataset.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/pyg_dataset/#overview","title":"Overview","text":"<p>The <code>STPyGDataset</code> class extends PyTorch Geometric's <code>InMemoryDataset</code> to provide a standardized interface for loading and managing spatial transcriptomics data in machine learning pipelines. It handles the conversion of processed tiles into PyTorch Geometric format and provides utilities for training and validation.</p>"},{"location":"api/data/sample/","title":"segger.data.sample","text":"<p>The <code>sample</code> module is the core of the Segger data processing framework, providing comprehensive classes for handling spatial transcriptomics data. This module contains the main classes that orchestrate the entire data processing pipeline from raw data to machine learning-ready graphs.</p>"},{"location":"api/data/sample/#src.segger.data.sample","title":"sample","text":""},{"location":"api/data/sample/#src.segger.data.sample.STInMemoryDataset","title":"STInMemoryDataset","text":"<p>A class for handling in-memory representations of ST data.</p> <p>This class is used to load and manage ST sample data from parquet files, filter boundaries and transcripts, and provide spatial tiling for further analysis. The class also pre-loads KDTrees for efficient spatial queries.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>STSampleParquet</code> <p>The ST sample containing paths to the data files.</p> required <code>extents</code> <code>Polygon</code> <p>The polygon defining the spatial extents for the dataset.</p> required <code>margin</code> <code>int</code> <p>The margin to buffer around the extents when filtering data. Defaults to 10.</p> <code>10</code> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>STSampleParquet</code> <p>The ST sample from which the data is loaded.</p> required <code>extents</code> <code>Polygon</code> <p>The spatial extents of the dataset.</p> required <code>margin</code> <code>int</code> <p>The buffer margin around the extents for filtering.</p> <code>10</code> <code>transcripts</code> <p>The filtered transcripts within the dataset extents.</p> required <code>boundaries</code> <p>The filtered boundaries within the dataset extents.</p> required <code>kdtree_tx</code> <p>The KDTree for fast spatial queries on the transcripts.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transcripts or boundaries could not be loaded or filtered.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>class STInMemoryDataset:\n    \"\"\"A class for handling in-memory representations of ST data.\n\n    This class is used to load and manage ST sample data from parquet files,\n    filter boundaries and transcripts, and provide spatial tiling for further\n    analysis. The class also pre-loads KDTrees for efficient spatial queries.\n\n    Args:\n        sample: The ST sample containing paths to the data files.\n        extents: The polygon defining the spatial extents for the dataset.\n        margin: The margin to buffer around the extents when filtering data. Defaults to 10.\n\n    Args:\n        sample: The ST sample from which the data is loaded.\n        extents: The spatial extents of the dataset.\n        margin: The buffer margin around the extents for filtering.\n        transcripts: The filtered transcripts within the dataset extents.\n        boundaries: The filtered boundaries within the dataset extents.\n        kdtree_tx: The KDTree for fast spatial queries on the transcripts.\n\n    Raises:\n        ValueError: If the transcripts or boundaries could not be loaded or filtered.\n    \"\"\"\n\n    def __init__(\n        self,\n        sample: STSampleParquet,\n        extents: shapely.Polygon,\n        margin: int = 10,\n    ):\n        \"\"\"Initialize the STInMemoryDataset instance by loading transcripts\n        and boundaries from parquet files and pre-loading a KDTree for fast\n        spatial queries.\n\n        Args:\n            sample: The ST sample containing paths to the data files.\n            extents: The polygon defining the spatial extents for the dataset.\n            margin: The margin to buffer around the extents when filtering data. Defaults to 10.\n        \"\"\"\n        # Set properties\n        self.sample = sample\n        self.extents = extents\n        self.margin = margin\n        self.settings = self.sample.settings\n\n        # Load data from parquet files\n        self._load_transcripts(self.sample._transcripts_filepath)\n        self._load_boundaries(self.sample._boundaries_filepath)\n\n        # Pre-load KDTrees\n        self.kdtree_tx = KDTree(\n            self.transcripts[self.settings.transcripts.xy], leafsize=100\n        )\n\n    def _load_transcripts(self, path: os.PathLike, min_qv: float = 30.0):\n        \"\"\"Load and filter the transcripts dataframe for the dataset.\n\n        Args:\n            path: The file path to the transcripts parquet file.\n            min_qv: The minimum quality value (QV) for filtering transcripts. Defaults to 30.0.\n\n        Raises:\n            ValueError: If the transcripts dataframe cannot be loaded or filtered.\n        \"\"\"\n        # Load and filter transcripts dataframe\n        bounds = self.extents.buffer(self.margin, join_style=\"mitre\")\n        transcripts = utils.read_parquet_region(\n            path,\n            x=self.settings.transcripts.x,\n            y=self.settings.transcripts.y,\n            bounds=bounds,\n            extra_columns=self.settings.transcripts.columns,\n        )\n        transcripts[self.settings.transcripts.label] = transcripts[\n            self.settings.transcripts.label\n        ].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n        qv_column = getattr(self.settings.transcripts, \"qv_column\", None)\n        transcripts = utils.filter_transcripts(\n            transcripts,\n            self.settings.transcripts.label,\n            self.settings.transcripts.filter_substrings,\n            qv_column,\n            min_qv,\n        )\n\n        # Only set object properties once everything finishes successfully\n        self.transcripts = transcripts\n\n    def _load_boundaries(self, path: os.PathLike):\n        \"\"\"Load and filter the boundaries dataframe for the dataset.\n\n        Args:\n            path: The file path to the boundaries parquet file.\n\n        Raises:\n            ValueError: If the boundaries dataframe cannot be loaded or filtered.\n        \"\"\"\n        # Load and filter boundaries dataframe\n        outset = self.extents.buffer(self.margin, join_style=\"mitre\")\n        boundaries = utils.read_parquet_region(\n            path,\n            x=self.settings.boundaries.x,\n            y=self.settings.boundaries.y,\n            bounds=outset,\n            extra_columns=self.settings.boundaries.columns,\n        )\n        boundaries = utils.filter_boundaries(\n            boundaries,\n            inset=self.extents,\n            outset=outset,\n            x=self.settings.boundaries.x,\n            y=self.settings.boundaries.y,\n            label=self.settings.boundaries.label,\n        )\n        self.boundaries = boundaries\n\n    def _get_rectangular_tile_bounds(\n        self,\n        tile_width: float,\n        tile_height: float,\n    ) -&gt; List[shapely.Polygon]:\n        \"\"\"Generate rectangular tiles for the dataset based on the extents.\n\n        Args:\n            tile_width: The width of each tile.\n            tile_height: The height of each tile.\n\n        Returns:\n            List[shapely.Polygon]: A list of polygons representing the rectangular tiles.\n        \"\"\"\n        # Generate the x and y coordinates for the tile boundaries\n        x_min, y_min, x_max, y_max = self.extents.bounds\n        x_coords = np.arange(x_min, x_max, tile_width)\n        x_coords = np.append(x_coords, x_max)\n        y_coords = np.arange(y_min, y_max, tile_height)\n        y_coords = np.append(y_coords, y_max)\n\n        # Generate tiles from grid points\n        tiles = []\n        for x_min, x_max in zip(x_coords[:-1], x_coords[1:]):\n            for y_min, y_max in zip(y_coords[:-1], y_coords[1:]):\n                tiles.append(shapely.box(x_min, y_min, x_max, y_max))\n\n        return tiles\n\n    def _get_balanced_tile_bounds(\n        self,\n        max_size: Optional[int],\n    ) -&gt; List[shapely.Polygon]:\n        \"\"\"Generate spatially balanced tiles based on KDTree partitioning.\n\n        Args:\n            max_size: The maximum number of points in each tile.\n\n        Returns:\n            List[shapely.Polygon]: A list of polygons representing balanced tile bounds.\n\n        Raises:\n            ValueError: If `max_size` is smaller than the KDTree's leaf size.\n        \"\"\"\n        # Can only request up to brute force resolution of KDTree\n        leafsize = self.kdtree_tx.leafsize\n        if max_size &lt; leafsize:\n            msg = f\"Arg 'max_size' less than KDTree 'leafsize', {leafsize}.\"\n            raise ValueError(msg)\n\n        # DFS search to construct tile bounds\n        def recurse(node, bounds):\n            if node.children &lt;= max_size:\n                bounds = shapely.box(*bounds.mins, *bounds.maxes)\n                return [bounds]\n            lb, gb = bounds.split(node.split_dim, node.split)\n            return recurse(node.less, lb) + recurse(node.greater, gb)\n\n        node = self.kdtree_tx.tree\n        bounds = Rectangle(self.kdtree_tx.mins, self.kdtree_tx.maxes)\n        return recurse(node, bounds)\n\n    def _tile(\n        self,\n        width: Optional[float] = None,\n        height: Optional[float] = None,\n        max_size: Optional[int] = None,\n    ) -&gt; List[shapely.Polygon]:\n        \"\"\"Generate tiles based on either fixed dimensions or balanced\n        partitioning.\n\n        Args:\n            width: The width of each tile. Required if `max_size` is not provided. Defaults to None.\n            height: The height of each tile. Required if `max_size` is not provided. Defaults to None.\n            max_size: The maximum number of points in each tile. Required if `width` and\n                `height` are not provided. Defaults to None.\n\n        Returns:\n            List[shapely.Polygon]: A list of polygons representing the tiles.\n\n        Raises:\n            ValueError: If both `width`/`height` and `max_size` are provided or none are\n                provided.\n        \"\"\"\n        # Square tiling kwargs provided\n        if not max_size and (width and height):\n            return self._get_rectangular_tile_bounds(width, height)\n        # Balanced tiling kwargs provided or None\n        elif not (width or height):\n            return self._get_balanced_tile_bounds(max_size)\n        # Bad set of kwargs\n        else:\n            args = list(compress(locals().keys(), locals().values()))\n            args.remove(\"self\")\n            msg = (\n                \"Function requires either 'max_size' or both \"\n                f\"'width' and 'height'. Found: {', '.join(args)}.\"\n            )\n            logging.error(msg)\n            raise ValueError\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STInMemoryDataset.__init__","title":"__init__","text":"<pre><code>__init__(sample, extents, margin=10)\n</code></pre> <p>Initialize the STInMemoryDataset instance by loading transcripts and boundaries from parquet files and pre-loading a KDTree for fast spatial queries.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>STSampleParquet</code> <p>The ST sample containing paths to the data files.</p> required <code>extents</code> <code>Polygon</code> <p>The polygon defining the spatial extents for the dataset.</p> required <code>margin</code> <code>int</code> <p>The margin to buffer around the extents when filtering data. Defaults to 10.</p> <code>10</code> Source code in <code>src/segger/data/sample.py</code> <pre><code>def __init__(\n    self,\n    sample: STSampleParquet,\n    extents: shapely.Polygon,\n    margin: int = 10,\n):\n    \"\"\"Initialize the STInMemoryDataset instance by loading transcripts\n    and boundaries from parquet files and pre-loading a KDTree for fast\n    spatial queries.\n\n    Args:\n        sample: The ST sample containing paths to the data files.\n        extents: The polygon defining the spatial extents for the dataset.\n        margin: The margin to buffer around the extents when filtering data. Defaults to 10.\n    \"\"\"\n    # Set properties\n    self.sample = sample\n    self.extents = extents\n    self.margin = margin\n    self.settings = self.sample.settings\n\n    # Load data from parquet files\n    self._load_transcripts(self.sample._transcripts_filepath)\n    self._load_boundaries(self.sample._boundaries_filepath)\n\n    # Pre-load KDTrees\n    self.kdtree_tx = KDTree(\n        self.transcripts[self.settings.transcripts.xy], leafsize=100\n    )\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet","title":"STSampleParquet","text":"<p>A class to manage spatial transcriptomics data stored in parquet files.</p> <p>This class provides methods for loading, processing, and saving data related to ST samples. It supports parallel processing and efficient handling of transcript and boundary data.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>class STSampleParquet:\n    \"\"\"A class to manage spatial transcriptomics data stored in parquet files.\n\n    This class provides methods for loading, processing, and saving data related\n    to ST samples. It supports parallel processing and efficient handling of\n    transcript and boundary data.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_dir: os.PathLike,\n        n_workers: Optional[int] = 1,\n        scale_factor: Optional[float] = 1.0,\n        sample_type: str = None,\n        weights: pd.DataFrame = None,\n    ):\n        \"\"\"Initialize the STSampleParquet instance.\n\n        Args:\n            base_dir: The base directory containing the ST data.\n            n_workers: The number of workers for parallel processing. Defaults to 1.\n            sample_type: The sample type of the raw data, e.g., 'xenium' or 'merscope'. Defaults to None.\n            weights: DataFrame containing weights for transcript embedding. Defaults to None.\n            scale_factor: The scale factor to be used for expanding the boundary extents\n                during spatial queries. If not provided, the default from settings\n                will be used. Defaults to None.\n\n        Raises:\n            FileNotFoundError: If the base directory does not exist or the required files are\n                missing.\n        \"\"\"\n        # Setup paths and resource constraints\n        self._base_dir = Path(base_dir)\n        self.settings = utils.load_settings(sample_type)\n        transcripts_fn = self.settings.transcripts.filename\n        self._transcripts_filepath = self._base_dir / transcripts_fn\n        boundaries_fn = self.settings.boundaries.filename\n        self._boundaries_filepath = self._base_dir / boundaries_fn\n        self.n_workers = n_workers\n        self.settings.boundaries.scale_factor = 1\n        nuclear_column = getattr(self.settings.transcripts, \"nuclear_column\", None)\n        if nuclear_column is None or self.settings.boundaries.scale_factor != 1.0:\n            print(\n                \"Boundary-transcript overlap information has not been pre-computed. It will be calculated during tile generation.\"\n            )\n        # Set scale factor if provided\n        if scale_factor != 1.0:\n            self.settings.boundaries.scale_factor = scale_factor\n\n        # Ensure transcript IDs exist\n        utils.ensure_transcript_ids(\n            self._transcripts_filepath,\n            self.settings.transcripts.x,\n            self.settings.transcripts.y,\n            self.settings.transcripts.id,\n        )\n\n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.Logger(f\"STSample@{base_dir}\")\n\n        # Internal caches\n        self._extents = None\n        self._transcripts_metadata = None\n        self._boundaries_metadata = None\n\n        # Setup default embedding for transcripts\n        self._emb_genes = None\n        if weights is not None:\n            self._emb_genes = weights.index.to_list()\n        classes = self.transcripts_metadata[\"feature_names\"]\n        self._transcript_embedding = TranscriptEmbedding(np.array(classes), weights)\n\n    @classmethod\n    def _get_parquet_metadata(\n        cls,\n        filepath: os.PathLike,\n        columns: Optional[List[str]] = None,\n    ) -&gt; dict:\n        \"\"\"Read and return metadata from the parquet file.\n\n        Args:\n            filepath: The path to the parquet file.\n            columns: List of columns to extract metadata for. If None, all columns\n                are used. Defaults to None.\n\n        Returns:\n            A dictionary containing metadata such as the number of rows,\n            number of columns, and column sizes.\n\n        Raises:\n            FileNotFoundError: If the parquet file does not exist at the specified path.\n            KeyError: If any of the requested columns are not found in the parquet file.\n        \"\"\"\n        # Size in bytes of field dtypes\n        size_map = {\n            \"BOOLEAN\": 1,\n            \"INT32\": 4,\n            \"FLOAT\": 4,\n            \"INT64\": 8,\n            \"DOUBLE\": 8,\n            \"BYTE_ARRAY\": 8,\n            \"INT96\": 12,\n        }\n\n        # Read in metadata\n        metadata = pq.read_metadata(filepath)\n        if columns is None:\n            columns = metadata.schema.names\n        missing = set(columns) - set(metadata.schema.names)\n        if len(missing) &gt; 0:\n            msg = f\"Columns {', '.join(missing)} not found in schema.\"\n            raise KeyError(msg)\n\n        # Grab important fields from metadata\n        summary = dict()\n        summary[\"n_rows\"] = metadata.num_rows\n        summary[\"n_columns\"] = len(columns)\n        summary[\"column_sizes\"] = dict()\n        for c in columns:\n            # Error where 10X saved BOOLEAN field as INT32 in schema\n            if c == \"overlaps_nucleus\":\n                dtype = \"BOOLEAN\"\n            else:\n                i = metadata.schema.names.index(c)\n                dtype = metadata.schema[i].physical_type\n            summary[\"column_sizes\"][c] = size_map[dtype]\n\n        return summary\n\n    @cached_property\n    def transcripts_metadata(self) -&gt; dict:\n        \"\"\"Retrieve metadata for the transcripts stored in the sample.\n\n        Returns:\n            Metadata dictionary for transcripts including column sizes and\n            feature names.\n\n        Raises:\n            FileNotFoundError: If the transcript parquet file does not exist.\n        \"\"\"\n        if self._transcripts_metadata is None:\n            # Base metadata\n            metadata = STSampleParquet._get_parquet_metadata(\n                self._transcripts_filepath,\n                self.settings.transcripts.columns,\n            )\n            # Get filtered unique feature names\n            table = pq.read_table(self._transcripts_filepath)\n            names = pc.unique(table[self.settings.transcripts.label])\n            if self._emb_genes is not None:\n                # Filter substring is extended with the genes missing in the embedding\n                names_str = [\n                    x.decode(\"utf-8\") if isinstance(x, bytes) else x\n                    for x in names.to_pylist()\n                ]\n                missing_genes = list(set(names_str) - set(self._emb_genes))\n                logging.warning(f\"Number of missing genes: {len(missing_genes)}\")\n                self.settings.transcripts.filter_substrings.extend(missing_genes)\n            # pattern = \"|\".join(self.settings.transcripts.filter_substrings)\n            pattern = \"|\".join(f\"^{s}\" for s in self.settings.transcripts.filter_substrings)\n            mask = pc.invert(pc.match_substring_regex(names, pattern))\n            filtered_names = pc.filter(names, mask).to_pylist()\n            metadata[\"feature_names\"] = [\n                x.decode(\"utf-8\") if isinstance(x, bytes) else x for x in filtered_names\n            ]\n            self._transcripts_metadata = metadata\n        return self._transcripts_metadata\n\n    @cached_property\n    def boundaries_metadata(self) -&gt; dict:\n        \"\"\"Retrieve metadata for the boundaries stored in the sample.\n\n        Returns:\n            Metadata dictionary for boundaries including column sizes.\n\n        Raises:\n            FileNotFoundError: If the boundaries parquet file does not exist.\n        \"\"\"\n        if self._boundaries_metadata is None:\n            metadata = STSampleParquet._get_parquet_metadata(\n                self._boundaries_filepath,\n                self.settings.boundaries.columns,\n            )\n            self._boundaries_metadata = metadata\n        return self._boundaries_metadata\n\n    @property\n    def n_transcripts(self) -&gt; int:\n        \"\"\"Get the total number of transcripts in the sample.\n\n        Returns:\n            int: The number of transcripts.\n        \"\"\"\n        return self.transcripts_metadata[\"n_rows\"]\n\n    @cached_property\n    def extents(self) -&gt; shapely.Polygon:\n        \"\"\"Get the combined extents (bounding box) of the transcripts and boundaries.\n\n        Returns:\n            shapely.Polygon: The bounding box covering all transcripts and boundaries.\n        \"\"\"\n        if self._extents is None:\n            # Get individual extents\n            xy = self.settings.transcripts.xy\n            tx_extents = utils.get_xy_extents(self._transcripts_filepath, *xy)\n            xy = self.settings.boundaries.xy\n            bd_extents = utils.get_xy_extents(self._boundaries_filepath, *xy)\n\n            # Combine extents and get bounding box\n            extents = tx_extents.union(bd_extents)\n            self._extents = shapely.box(*extents.bounds)\n\n        return self._extents\n\n    def _get_balanced_regions(\n        self,\n    ) -&gt; List[shapely.Polygon]:\n        \"\"\"Split the sample extents into balanced regions for parallel processing.\n\n        See NDTree documentation for more information.\n\n        Returns:\n            List[shapely.Polygon]: A list of polygons representing the regions.\n        \"\"\"\n        # If no. workers is 1, return full extents\n        if self.n_workers == 1:\n            return [self.extents]\n\n        # Otherwise, split based on boundary distribution which is much smaller\n        # than transcripts DataFrame.\n        # Note: Assumes boundaries are distributed similarly to transcripts at\n        # a coarse level.\n        data = pd.read_parquet(\n            self._boundaries_filepath,\n            columns=self.settings.boundaries.xy,\n        ).values\n        ndtree = NDTree(data, self.n_workers)\n\n        return ndtree.boxes\n\n    @staticmethod\n    def _setup_directory(\n        data_dir: os.PathLike,\n    ):\n        \"\"\"Set up the directory structure for saving processed tiles.\n\n        Ensures that the necessary subdirectories for 'train', 'test', and\n        'val' are created under the provided base directory. If any of these\n        subdirectories already exist and are not empty, an error is raised.\n\n        Directory structure created:\n        ----------------------------\n        data_dir/\n            \u251c\u2500\u2500 train_tiles/\n            \u2502   \u2514\u2500\u2500 processed/\n            \u251c\u2500\u2500 test_tiles/\n            \u2502   \u2514\u2500\u2500 processed/\n            \u2514\u2500\u2500 val_tiles/\n                \u2514\u2500\u2500 processed/\n\n        Args:\n            data_dir: The path to the base directory where the data should be stored.\n\n        Raises:\n            AssertionError: If any of the 'processed' directories already contain files.\n        \"\"\"\n        data_dir = Path(data_dir)  # by default, convert to Path object\n        for tile_type in [\"train_tiles\", \"test_tiles\", \"val_tiles\"]:\n            for stage in [\"raw\", \"processed\"]:\n                tile_dir = data_dir / tile_type / stage\n                tile_dir.mkdir(parents=True, exist_ok=True)\n                if os.listdir(tile_dir):\n                    msg = f\"Directory '{tile_dir}' must be empty.\"\n                    raise AssertionError(msg)\n\n    def set_transcript_embedding(self, weights: pd.DataFrame):\n        \"\"\"Set the transcript embedding for the sample.\n\n        Args:\n            weights: A DataFrame containing the weights for each transcript.\n\n        Raises:\n            ValueError: If the provided weights do not match the number of transcript\n                features.\n        \"\"\"\n        classes = self._transcripts_metadata[\"feature_names\"]\n        self._transcript_embedding = TranscriptEmbedding(classes, weights)\n\n    def save(\n        self,\n        data_dir: os.PathLike,\n        k_bd: int = 3,\n        dist_bd: float = 15.0,\n        k_tx: int = 3,\n        dist_tx: float = 5.0,\n        k_tx_ex: int = 100,\n        dist_tx_ex: float = 20,\n        tile_size: Optional[int] = None,\n        tile_width: Optional[int] = None,\n        tile_height: Optional[int] = None,\n        neg_sampling_ratio: float = 5.0,\n        frac: float = 1.0,\n        val_prob: float = 0.1,\n        test_prob: float = 0.2,\n        mutually_exclusive_genes: Optional[List] = None,\n    ):\n        \"\"\"Save the tiles of the sample as PyTorch geometric datasets.\n\n        See documentation for 'STTile' for more information on dataset contents.\n\n        Note: This function requires either 'tile_size' OR both 'tile_width' and\n        'tile_height' to be provided.\n\n        Args:\n            data_dir: The directory where the dataset should be saved.\n            k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n            dist_bd: Maximum distance for boundary neighbors. Defaults to 15.0.\n            k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n            dist_tx: Maximum distance for transcript neighbors. Defaults to 5.0.\n            tile_size: If provided, specifies the size of the tile. Overrides `tile_width`\n                and `tile_height`. Defaults to None.\n            tile_width: Width of the tiles in pixels. Ignored if `tile_size` is provided. Defaults to None.\n            tile_height: Height of the tiles in pixels. Ignored if `tile_size` is provided. Defaults to None.\n            neg_sampling_ratio: Ratio of negative samples. Defaults to 5.0.\n            frac: Fraction of the dataset to process. Defaults to 1.0.\n            val_prob: Proportion of data for use for validation split. Defaults to 0.1.\n            test_prob: Proportion of data for use for test split. Defaults to 0.2.\n\n        Raises:\n            ValueError: If the 'frac' parameter is greater than 1.0 or if the calculated\n                number of tiles is zero.\n            AssertionError: If the specified directory structure is not properly set up.\n        \"\"\"\n        # Check inputs\n        try:\n            if frac &gt; 1:\n                msg = f\"Arg 'frac' should be &lt;= 1.0, but got {frac}.\"\n                raise ValueError(msg)\n            if tile_size is not None:\n                n_tiles = self.n_transcripts / tile_size / self.n_workers * frac\n                if int(n_tiles) == 0:\n                    msg = f\"Sampling parameters would yield 0 total tiles.\"\n                    raise ValueError(msg)\n        # Propagate errors to logging\n        except Exception as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        # Setup directory structure to save tiles\n        data_dir = Path(data_dir)\n        STSampleParquet._setup_directory(data_dir)\n\n        # Function to parallelize over workers\n        def func(region):\n            xm = STInMemoryDataset(sample=self, extents=region)\n            tiles = xm._tile(tile_width, tile_height, tile_size)\n            # print(tiles)\n            if frac &lt; 1:\n                tiles = random.sample(tiles, int(len(tiles) * frac))\n            for tile in tiles:\n                # Choose training, test, or validation datasets\n                data_type = np.random.choice(\n                    a=[\"train_tiles\", \"test_tiles\", \"val_tiles\"],\n                    p=[1 - (test_prob + val_prob), test_prob, val_prob],\n                )\n                xt = STTile(dataset=xm, extents=tile)\n                pyg_data = xt.to_pyg_dataset(\n                    k_bd=k_bd,\n                    dist_bd=dist_bd,\n                    k_tx=k_tx,\n                    dist_tx=dist_tx,\n                    k_tx_ex=k_tx_ex,\n                    dist_tx_ex=dist_tx_ex,\n                    neg_sampling_ratio=neg_sampling_ratio,\n                    mutually_exclusive_genes = mutually_exclusive_genes\n                )\n                if pyg_data is not None:\n                    if pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index.numel() == 0:\n                        # this tile is only for testing\n                        data_type = \"test_tiles\"\n                    filepath = data_dir / data_type / \"processed\" / f\"{xt.uid}.pt\"\n                    torch.save(pyg_data, filepath)\n\n        # TODO: Add Dask backend\n        regions = self._get_balanced_regions()\n        outs = []\n        for region in regions:\n            outs.append(func(region))\n        return outs\n\n    def save_debug(\n        self,\n        data_dir: os.PathLike,\n        k_bd: int = 3,\n        dist_bd: float = 15.0,\n        k_tx: int = 3,\n        dist_tx: float = 5.0,\n        k_tx_ex: int = 100,\n        dist_tx_ex: float = 20,\n        tile_width: Optional[float] = None,\n        tile_height: Optional[float] = None,\n        neg_sampling_ratio: float = 5.0,\n        frac: float = 1.0,\n        val_prob: float = 0.1,\n        test_prob: float = 0.2,\n    ):\n        \"\"\"Debug version of save method that processes tiles sequentially and prints\n        detailed information about the process.\n\n        Args:\n            data_dir: The directory where the dataset should be saved.\n            k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n            dist_bd: Maximum distance for boundary neighbors. Defaults to 15.0.\n            k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n            dist_tx: Maximum distance for transcript neighbors. Defaults to 5.0.\n            k_tx_ex: Number of nearest neighbors for transcript exclusion. Defaults to 100.\n            dist_tx_ex: Maximum distance for transcript exclusion. Defaults to 20.\n            tile_width: Width of the tiles in pixels. Defaults to None.\n            tile_height: Height of the tiles in pixels. Defaults to None.\n            neg_sampling_ratio: Ratio of negative samples. Defaults to 5.0.\n            frac: Fraction of the dataset to process. Defaults to 1.0.\n            val_prob: Proportion of data for use for validation split. Defaults to 0.1.\n            test_prob: Proportion of data for use for test split. Defaults to 0.2.\n        \"\"\"\n        print(\"\\n=== Starting Debug Tile Generation ===\")\n        print(f\"Parameters:\")\n        print(f\"- k_bd: {k_bd} (boundary neighbors)\")\n        print(f\"- dist_bd: {dist_bd} (boundary distance)\")\n        print(f\"- k_tx: {k_tx} (transcript neighbors)\")\n        print(f\"- dist_tx: {dist_tx} (transcript distance)\")\n        print(f\"- tile_width: {tile_width}\")\n        print(f\"- tile_height: {tile_height}\")\n        print(f\"- frac: {frac}\")\n        print(f\"- val_prob: {val_prob}\")\n        print(f\"- test_prob: {test_prob}\")\n\n        # Setup directory structure to save tiles\n        data_dir = Path(data_dir)\n        STSampleParquet._setup_directory(data_dir)\n        print(f\"\\nOutput directory: {data_dir}\")\n\n        # Get regions to process\n        regions = self._get_balanced_regions()\n        print(f\"\\nTotal regions to process: {len(regions)}\")\n        print(\"Region bounds:\")\n        for i, region in enumerate(regions):\n            print(f\"Region {i+1}: {region.bounds}\")\n\n        # Process each region sequentially\n        for region_idx, region in enumerate(regions):\n            print(f\"\\n=== Processing Region {region_idx + 1}/{len(regions)} ===\")\n            print(f\"Region bounds: {region.bounds}\")\n\n            xm = STInMemoryDataset(sample=self, extents=region)\n            tiles = xm._tile(tile_width, tile_height, None)\n            print(f\"Generated {len(tiles)} tiles for this region\")\n\n            if frac &lt; 1:\n                tiles = random.sample(tiles, int(len(tiles) * frac))\n                print(f\"After sampling: {len(tiles)} tiles\")\n\n            # Process each tile\n            for tile_idx, tile in enumerate(tiles):\n                print(f\"\\n--- Processing Tile {tile_idx + 1}/{len(tiles)} ---\")\n                print(f\"Tile bounds: {tile.bounds}\")\n\n                # Choose training, test, or validation datasets\n                data_type = np.random.choice(\n                    a=[\"train_tiles\", \"test_tiles\", \"val_tiles\"],\n                    p=[1 - (test_prob + val_prob), test_prob, val_prob],\n                )\n                print(f\"Assigned to: {data_type}\")\n\n                xt = STTile(dataset=xm, extents=tile)\n                print(f\"Tile UID: {xt.uid}\")\n\n                pyg_data = xt.to_pyg_dataset(\n                    k_bd=k_bd,\n                    dist_bd=dist_bd,\n                    k_tx=k_tx,\n                    dist_tx=dist_tx,\n                    k_tx_ex=k_tx_ex,\n                    dist_tx_ex=dist_tx_ex,\n                    neg_sampling_ratio=neg_sampling_ratio,\n                )\n\n                if pyg_data is not None:\n                    if pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index.numel() == 0:\n                        data_type = \"test_tiles\"\n                        print(\"No tx-belongs-bd edges found, reassigning to test_tiles\")\n\n                    filepath = data_dir / data_type / \"processed\" / f\"{xt.uid}.pt\"\n                    torch.save(pyg_data, filepath)\n                    print(f\"Saved to: {filepath}\")\n\n                    # Print some statistics about the generated data\n                    print(f\"Data statistics:\")\n                    print(f\"- Number of transcripts: {pyg_data['tx'].num_nodes}\")\n                    print(f\"- Number of boundaries: {pyg_data['bd'].num_nodes}\")\n                    print(\n                        f\"- Number of tx-tx edges: {pyg_data['tx', 'neighbors', 'tx'].edge_index.shape[1]}\"\n                    )\n                    print(\n                        f\"- Number of tx-bd edges: {pyg_data['tx', 'neighbors', 'bd'].edge_index.shape[1]}\"\n                    )\n                    # print(f\"- Number of tx-belongs-bd edges: {pyg_data['tx', 'belongs', 'bd'].edge_index.shape[1]}\")\n                else:\n                    print(\"Skipping tile - no valid data generated\")\n\n        print(\"\\n=== Debug Tile Generation Completed ===\")\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.boundaries_metadata","title":"boundaries_metadata  <code>cached</code> <code>property</code>","text":"<pre><code>boundaries_metadata\n</code></pre> <p>Retrieve metadata for the boundaries stored in the sample.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Metadata dictionary for boundaries including column sizes.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the boundaries parquet file does not exist.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.extents","title":"extents  <code>cached</code> <code>property</code>","text":"<pre><code>extents\n</code></pre> <p>Get the combined extents (bounding box) of the transcripts and boundaries.</p> <p>Returns:</p> Type Description <code>Polygon</code> <p>shapely.Polygon: The bounding box covering all transcripts and boundaries.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.n_transcripts","title":"n_transcripts  <code>property</code>","text":"<pre><code>n_transcripts\n</code></pre> <p>Get the total number of transcripts in the sample.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of transcripts.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.transcripts_metadata","title":"transcripts_metadata  <code>cached</code> <code>property</code>","text":"<pre><code>transcripts_metadata\n</code></pre> <p>Retrieve metadata for the transcripts stored in the sample.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Metadata dictionary for transcripts including column sizes and</p> <code>dict</code> <p>feature names.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the transcript parquet file does not exist.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.__init__","title":"__init__","text":"<pre><code>__init__(base_dir, n_workers=1, scale_factor=1.0, sample_type=None, weights=None)\n</code></pre> <p>Initialize the STSampleParquet instance.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>PathLike</code> <p>The base directory containing the ST data.</p> required <code>n_workers</code> <code>Optional[int]</code> <p>The number of workers for parallel processing. Defaults to 1.</p> <code>1</code> <code>sample_type</code> <code>str</code> <p>The sample type of the raw data, e.g., 'xenium' or 'merscope'. Defaults to None.</p> <code>None</code> <code>weights</code> <code>DataFrame</code> <p>DataFrame containing weights for transcript embedding. Defaults to None.</p> <code>None</code> <code>scale_factor</code> <code>Optional[float]</code> <p>The scale factor to be used for expanding the boundary extents during spatial queries. If not provided, the default from settings will be used. Defaults to None.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the base directory does not exist or the required files are missing.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def __init__(\n    self,\n    base_dir: os.PathLike,\n    n_workers: Optional[int] = 1,\n    scale_factor: Optional[float] = 1.0,\n    sample_type: str = None,\n    weights: pd.DataFrame = None,\n):\n    \"\"\"Initialize the STSampleParquet instance.\n\n    Args:\n        base_dir: The base directory containing the ST data.\n        n_workers: The number of workers for parallel processing. Defaults to 1.\n        sample_type: The sample type of the raw data, e.g., 'xenium' or 'merscope'. Defaults to None.\n        weights: DataFrame containing weights for transcript embedding. Defaults to None.\n        scale_factor: The scale factor to be used for expanding the boundary extents\n            during spatial queries. If not provided, the default from settings\n            will be used. Defaults to None.\n\n    Raises:\n        FileNotFoundError: If the base directory does not exist or the required files are\n            missing.\n    \"\"\"\n    # Setup paths and resource constraints\n    self._base_dir = Path(base_dir)\n    self.settings = utils.load_settings(sample_type)\n    transcripts_fn = self.settings.transcripts.filename\n    self._transcripts_filepath = self._base_dir / transcripts_fn\n    boundaries_fn = self.settings.boundaries.filename\n    self._boundaries_filepath = self._base_dir / boundaries_fn\n    self.n_workers = n_workers\n    self.settings.boundaries.scale_factor = 1\n    nuclear_column = getattr(self.settings.transcripts, \"nuclear_column\", None)\n    if nuclear_column is None or self.settings.boundaries.scale_factor != 1.0:\n        print(\n            \"Boundary-transcript overlap information has not been pre-computed. It will be calculated during tile generation.\"\n        )\n    # Set scale factor if provided\n    if scale_factor != 1.0:\n        self.settings.boundaries.scale_factor = scale_factor\n\n    # Ensure transcript IDs exist\n    utils.ensure_transcript_ids(\n        self._transcripts_filepath,\n        self.settings.transcripts.x,\n        self.settings.transcripts.y,\n        self.settings.transcripts.id,\n    )\n\n    # Setup logging\n    logging.basicConfig(level=logging.INFO)\n    self.logger = logging.Logger(f\"STSample@{base_dir}\")\n\n    # Internal caches\n    self._extents = None\n    self._transcripts_metadata = None\n    self._boundaries_metadata = None\n\n    # Setup default embedding for transcripts\n    self._emb_genes = None\n    if weights is not None:\n        self._emb_genes = weights.index.to_list()\n    classes = self.transcripts_metadata[\"feature_names\"]\n    self._transcript_embedding = TranscriptEmbedding(np.array(classes), weights)\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.save","title":"save","text":"<pre><code>save(data_dir, k_bd=3, dist_bd=15.0, k_tx=3, dist_tx=5.0, k_tx_ex=100, dist_tx_ex=20, tile_size=None, tile_width=None, tile_height=None, neg_sampling_ratio=5.0, frac=1.0, val_prob=0.1, test_prob=0.2, mutually_exclusive_genes=None)\n</code></pre> <p>Save the tiles of the sample as PyTorch geometric datasets.</p> <p>See documentation for 'STTile' for more information on dataset contents.</p> <p>Note: This function requires either 'tile_size' OR both 'tile_width' and 'tile_height' to be provided.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>PathLike</code> <p>The directory where the dataset should be saved.</p> required <code>k_bd</code> <code>int</code> <p>Number of nearest neighbors for boundary nodes. Defaults to 3.</p> <code>3</code> <code>dist_bd</code> <code>float</code> <p>Maximum distance for boundary neighbors. Defaults to 15.0.</p> <code>15.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for transcript nodes. Defaults to 3.</p> <code>3</code> <code>dist_tx</code> <code>float</code> <p>Maximum distance for transcript neighbors. Defaults to 5.0.</p> <code>5.0</code> <code>tile_size</code> <code>Optional[int]</code> <p>If provided, specifies the size of the tile. Overrides <code>tile_width</code> and <code>tile_height</code>. Defaults to None.</p> <code>None</code> <code>tile_width</code> <code>Optional[int]</code> <p>Width of the tiles in pixels. Ignored if <code>tile_size</code> is provided. Defaults to None.</p> <code>None</code> <code>tile_height</code> <code>Optional[int]</code> <p>Height of the tiles in pixels. Ignored if <code>tile_size</code> is provided. Defaults to None.</p> <code>None</code> <code>neg_sampling_ratio</code> <code>float</code> <p>Ratio of negative samples. Defaults to 5.0.</p> <code>5.0</code> <code>frac</code> <code>float</code> <p>Fraction of the dataset to process. Defaults to 1.0.</p> <code>1.0</code> <code>val_prob</code> <code>float</code> <p>Proportion of data for use for validation split. Defaults to 0.1.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Proportion of data for use for test split. Defaults to 0.2.</p> <code>0.2</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'frac' parameter is greater than 1.0 or if the calculated number of tiles is zero.</p> <code>AssertionError</code> <p>If the specified directory structure is not properly set up.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def save(\n    self,\n    data_dir: os.PathLike,\n    k_bd: int = 3,\n    dist_bd: float = 15.0,\n    k_tx: int = 3,\n    dist_tx: float = 5.0,\n    k_tx_ex: int = 100,\n    dist_tx_ex: float = 20,\n    tile_size: Optional[int] = None,\n    tile_width: Optional[int] = None,\n    tile_height: Optional[int] = None,\n    neg_sampling_ratio: float = 5.0,\n    frac: float = 1.0,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n    mutually_exclusive_genes: Optional[List] = None,\n):\n    \"\"\"Save the tiles of the sample as PyTorch geometric datasets.\n\n    See documentation for 'STTile' for more information on dataset contents.\n\n    Note: This function requires either 'tile_size' OR both 'tile_width' and\n    'tile_height' to be provided.\n\n    Args:\n        data_dir: The directory where the dataset should be saved.\n        k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n        dist_bd: Maximum distance for boundary neighbors. Defaults to 15.0.\n        k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n        dist_tx: Maximum distance for transcript neighbors. Defaults to 5.0.\n        tile_size: If provided, specifies the size of the tile. Overrides `tile_width`\n            and `tile_height`. Defaults to None.\n        tile_width: Width of the tiles in pixels. Ignored if `tile_size` is provided. Defaults to None.\n        tile_height: Height of the tiles in pixels. Ignored if `tile_size` is provided. Defaults to None.\n        neg_sampling_ratio: Ratio of negative samples. Defaults to 5.0.\n        frac: Fraction of the dataset to process. Defaults to 1.0.\n        val_prob: Proportion of data for use for validation split. Defaults to 0.1.\n        test_prob: Proportion of data for use for test split. Defaults to 0.2.\n\n    Raises:\n        ValueError: If the 'frac' parameter is greater than 1.0 or if the calculated\n            number of tiles is zero.\n        AssertionError: If the specified directory structure is not properly set up.\n    \"\"\"\n    # Check inputs\n    try:\n        if frac &gt; 1:\n            msg = f\"Arg 'frac' should be &lt;= 1.0, but got {frac}.\"\n            raise ValueError(msg)\n        if tile_size is not None:\n            n_tiles = self.n_transcripts / tile_size / self.n_workers * frac\n            if int(n_tiles) == 0:\n                msg = f\"Sampling parameters would yield 0 total tiles.\"\n                raise ValueError(msg)\n    # Propagate errors to logging\n    except Exception as e:\n        self.logger.error(str(e), exc_info=True)\n        raise e\n\n    # Setup directory structure to save tiles\n    data_dir = Path(data_dir)\n    STSampleParquet._setup_directory(data_dir)\n\n    # Function to parallelize over workers\n    def func(region):\n        xm = STInMemoryDataset(sample=self, extents=region)\n        tiles = xm._tile(tile_width, tile_height, tile_size)\n        # print(tiles)\n        if frac &lt; 1:\n            tiles = random.sample(tiles, int(len(tiles) * frac))\n        for tile in tiles:\n            # Choose training, test, or validation datasets\n            data_type = np.random.choice(\n                a=[\"train_tiles\", \"test_tiles\", \"val_tiles\"],\n                p=[1 - (test_prob + val_prob), test_prob, val_prob],\n            )\n            xt = STTile(dataset=xm, extents=tile)\n            pyg_data = xt.to_pyg_dataset(\n                k_bd=k_bd,\n                dist_bd=dist_bd,\n                k_tx=k_tx,\n                dist_tx=dist_tx,\n                k_tx_ex=k_tx_ex,\n                dist_tx_ex=dist_tx_ex,\n                neg_sampling_ratio=neg_sampling_ratio,\n                mutually_exclusive_genes = mutually_exclusive_genes\n            )\n            if pyg_data is not None:\n                if pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index.numel() == 0:\n                    # this tile is only for testing\n                    data_type = \"test_tiles\"\n                filepath = data_dir / data_type / \"processed\" / f\"{xt.uid}.pt\"\n                torch.save(pyg_data, filepath)\n\n    # TODO: Add Dask backend\n    regions = self._get_balanced_regions()\n    outs = []\n    for region in regions:\n        outs.append(func(region))\n    return outs\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.save_debug","title":"save_debug","text":"<pre><code>save_debug(data_dir, k_bd=3, dist_bd=15.0, k_tx=3, dist_tx=5.0, k_tx_ex=100, dist_tx_ex=20, tile_width=None, tile_height=None, neg_sampling_ratio=5.0, frac=1.0, val_prob=0.1, test_prob=0.2)\n</code></pre> <p>Debug version of save method that processes tiles sequentially and prints detailed information about the process.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>PathLike</code> <p>The directory where the dataset should be saved.</p> required <code>k_bd</code> <code>int</code> <p>Number of nearest neighbors for boundary nodes. Defaults to 3.</p> <code>3</code> <code>dist_bd</code> <code>float</code> <p>Maximum distance for boundary neighbors. Defaults to 15.0.</p> <code>15.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for transcript nodes. Defaults to 3.</p> <code>3</code> <code>dist_tx</code> <code>float</code> <p>Maximum distance for transcript neighbors. Defaults to 5.0.</p> <code>5.0</code> <code>k_tx_ex</code> <code>int</code> <p>Number of nearest neighbors for transcript exclusion. Defaults to 100.</p> <code>100</code> <code>dist_tx_ex</code> <code>float</code> <p>Maximum distance for transcript exclusion. Defaults to 20.</p> <code>20</code> <code>tile_width</code> <code>Optional[float]</code> <p>Width of the tiles in pixels. Defaults to None.</p> <code>None</code> <code>tile_height</code> <code>Optional[float]</code> <p>Height of the tiles in pixels. Defaults to None.</p> <code>None</code> <code>neg_sampling_ratio</code> <code>float</code> <p>Ratio of negative samples. Defaults to 5.0.</p> <code>5.0</code> <code>frac</code> <code>float</code> <p>Fraction of the dataset to process. Defaults to 1.0.</p> <code>1.0</code> <code>val_prob</code> <code>float</code> <p>Proportion of data for use for validation split. Defaults to 0.1.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Proportion of data for use for test split. Defaults to 0.2.</p> <code>0.2</code> Source code in <code>src/segger/data/sample.py</code> <pre><code>def save_debug(\n    self,\n    data_dir: os.PathLike,\n    k_bd: int = 3,\n    dist_bd: float = 15.0,\n    k_tx: int = 3,\n    dist_tx: float = 5.0,\n    k_tx_ex: int = 100,\n    dist_tx_ex: float = 20,\n    tile_width: Optional[float] = None,\n    tile_height: Optional[float] = None,\n    neg_sampling_ratio: float = 5.0,\n    frac: float = 1.0,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n):\n    \"\"\"Debug version of save method that processes tiles sequentially and prints\n    detailed information about the process.\n\n    Args:\n        data_dir: The directory where the dataset should be saved.\n        k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n        dist_bd: Maximum distance for boundary neighbors. Defaults to 15.0.\n        k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n        dist_tx: Maximum distance for transcript neighbors. Defaults to 5.0.\n        k_tx_ex: Number of nearest neighbors for transcript exclusion. Defaults to 100.\n        dist_tx_ex: Maximum distance for transcript exclusion. Defaults to 20.\n        tile_width: Width of the tiles in pixels. Defaults to None.\n        tile_height: Height of the tiles in pixels. Defaults to None.\n        neg_sampling_ratio: Ratio of negative samples. Defaults to 5.0.\n        frac: Fraction of the dataset to process. Defaults to 1.0.\n        val_prob: Proportion of data for use for validation split. Defaults to 0.1.\n        test_prob: Proportion of data for use for test split. Defaults to 0.2.\n    \"\"\"\n    print(\"\\n=== Starting Debug Tile Generation ===\")\n    print(f\"Parameters:\")\n    print(f\"- k_bd: {k_bd} (boundary neighbors)\")\n    print(f\"- dist_bd: {dist_bd} (boundary distance)\")\n    print(f\"- k_tx: {k_tx} (transcript neighbors)\")\n    print(f\"- dist_tx: {dist_tx} (transcript distance)\")\n    print(f\"- tile_width: {tile_width}\")\n    print(f\"- tile_height: {tile_height}\")\n    print(f\"- frac: {frac}\")\n    print(f\"- val_prob: {val_prob}\")\n    print(f\"- test_prob: {test_prob}\")\n\n    # Setup directory structure to save tiles\n    data_dir = Path(data_dir)\n    STSampleParquet._setup_directory(data_dir)\n    print(f\"\\nOutput directory: {data_dir}\")\n\n    # Get regions to process\n    regions = self._get_balanced_regions()\n    print(f\"\\nTotal regions to process: {len(regions)}\")\n    print(\"Region bounds:\")\n    for i, region in enumerate(regions):\n        print(f\"Region {i+1}: {region.bounds}\")\n\n    # Process each region sequentially\n    for region_idx, region in enumerate(regions):\n        print(f\"\\n=== Processing Region {region_idx + 1}/{len(regions)} ===\")\n        print(f\"Region bounds: {region.bounds}\")\n\n        xm = STInMemoryDataset(sample=self, extents=region)\n        tiles = xm._tile(tile_width, tile_height, None)\n        print(f\"Generated {len(tiles)} tiles for this region\")\n\n        if frac &lt; 1:\n            tiles = random.sample(tiles, int(len(tiles) * frac))\n            print(f\"After sampling: {len(tiles)} tiles\")\n\n        # Process each tile\n        for tile_idx, tile in enumerate(tiles):\n            print(f\"\\n--- Processing Tile {tile_idx + 1}/{len(tiles)} ---\")\n            print(f\"Tile bounds: {tile.bounds}\")\n\n            # Choose training, test, or validation datasets\n            data_type = np.random.choice(\n                a=[\"train_tiles\", \"test_tiles\", \"val_tiles\"],\n                p=[1 - (test_prob + val_prob), test_prob, val_prob],\n            )\n            print(f\"Assigned to: {data_type}\")\n\n            xt = STTile(dataset=xm, extents=tile)\n            print(f\"Tile UID: {xt.uid}\")\n\n            pyg_data = xt.to_pyg_dataset(\n                k_bd=k_bd,\n                dist_bd=dist_bd,\n                k_tx=k_tx,\n                dist_tx=dist_tx,\n                k_tx_ex=k_tx_ex,\n                dist_tx_ex=dist_tx_ex,\n                neg_sampling_ratio=neg_sampling_ratio,\n            )\n\n            if pyg_data is not None:\n                if pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index.numel() == 0:\n                    data_type = \"test_tiles\"\n                    print(\"No tx-belongs-bd edges found, reassigning to test_tiles\")\n\n                filepath = data_dir / data_type / \"processed\" / f\"{xt.uid}.pt\"\n                torch.save(pyg_data, filepath)\n                print(f\"Saved to: {filepath}\")\n\n                # Print some statistics about the generated data\n                print(f\"Data statistics:\")\n                print(f\"- Number of transcripts: {pyg_data['tx'].num_nodes}\")\n                print(f\"- Number of boundaries: {pyg_data['bd'].num_nodes}\")\n                print(\n                    f\"- Number of tx-tx edges: {pyg_data['tx', 'neighbors', 'tx'].edge_index.shape[1]}\"\n                )\n                print(\n                    f\"- Number of tx-bd edges: {pyg_data['tx', 'neighbors', 'bd'].edge_index.shape[1]}\"\n                )\n                # print(f\"- Number of tx-belongs-bd edges: {pyg_data['tx', 'belongs', 'bd'].edge_index.shape[1]}\")\n            else:\n                print(\"Skipping tile - no valid data generated\")\n\n    print(\"\\n=== Debug Tile Generation Completed ===\")\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.set_transcript_embedding","title":"set_transcript_embedding","text":"<pre><code>set_transcript_embedding(weights)\n</code></pre> <p>Set the transcript embedding for the sample.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>DataFrame</code> <p>A DataFrame containing the weights for each transcript.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided weights do not match the number of transcript features.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def set_transcript_embedding(self, weights: pd.DataFrame):\n    \"\"\"Set the transcript embedding for the sample.\n\n    Args:\n        weights: A DataFrame containing the weights for each transcript.\n\n    Raises:\n        ValueError: If the provided weights do not match the number of transcript\n            features.\n    \"\"\"\n    classes = self._transcripts_metadata[\"feature_names\"]\n    self._transcript_embedding = TranscriptEmbedding(classes, weights)\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile","title":"STTile","text":"<p>A class representing a tile of a ST sample.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>STInMemoryDataset</code> <p>The ST dataset containing data.</p> required <code>extents</code> <code>Polygon</code> <p>The extents of the tile in the sample.</p> required <code>boundaries</code> <p>Filtered boundaries within the tile extents.</p> required <code>transcripts</code> <p>Filtered transcripts within the tile extents.</p> required Source code in <code>src/segger/data/sample.py</code> <pre><code>class STTile:\n    \"\"\"A class representing a tile of a ST sample.\n\n    Args:\n        dataset: The ST dataset containing data.\n        extents: The extents of the tile in the sample.\n        boundaries: Filtered boundaries within the tile extents.\n        transcripts: Filtered transcripts within the tile extents.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: STInMemoryDataset,\n        extents: shapely.Polygon,\n    ):\n        \"\"\"Initialize a STTile instance.\n\n        Args:\n            dataset: The ST dataset containing data.\n            extents: The extents of the tile in the sample.\n\n        Note:\n            The `boundaries` and `transcripts` attributes are cached to avoid the\n            overhead of filtering when tiles are instantiated. This is particularly\n            useful in multiprocessing settings where generating tiles in parallel\n            could lead to high overhead.\n\n        Internal Args:\n            _boundaries: Cached DataFrame of filtered boundaries. Initially set to None.\n            _transcripts: Cached DataFrame of filtered transcripts. Initially set to None.\n        \"\"\"\n        self.dataset = dataset\n        self.extents = extents\n        self.margin = dataset.margin\n        self.settings = self.dataset.settings\n\n        # Internal caches for filtered data\n        self._boundaries = None\n        self._transcripts = None\n\n    @property\n    def uid(self) -&gt; str:\n        \"\"\"Generate a unique identifier for the tile based on its extents.\n\n        This UID is particularly useful for saving or indexing tiles in distributed\n        processing environments.\n\n        The UID is constructed using the minimum and maximum x and y coordinates\n        of the tile's bounding box, representing its position and size in the\n        sample.\n\n        Returns:\n            str: A unique identifier string in the format\n                'x=&lt;x_min&gt;_y=&lt;y_min&gt;_w=&lt;width&gt;_h=&lt;height&gt;' where:\n                - `&lt;x_min&gt;`: Minimum x-coordinate of the tile's extents.\n                - `&lt;y_min&gt;`: Minimum y-coordinate of the tile's extents.\n                - `&lt;width&gt;`: Width of the tile.\n                - `&lt;height&gt;`: Height of the tile.\n\n        Example:\n            If the tile's extents are bounded by (x_min, y_min) = (100, 200) and\n            (x_max, y_max) = (150, 250), the generated UID would be:\n            'x=100_y=200_w=50_h=50'\n        \"\"\"\n        x_min, y_min, x_max, y_max = map(int, self.extents.bounds)\n        uid = f\"tiles_x={x_min}_y={y_min}_w={x_max-x_min}_h={y_max-y_min}\"\n        return uid\n\n    @cached_property\n    def boundaries(self) -&gt; pd.DataFrame:\n        \"\"\"Return the filtered boundaries within the tile extents, cached for\n        efficiency.\n\n        The boundaries are computed only once and cached. If the boundaries\n        have not been computed yet, they are computed using\n        `get_filtered_boundaries()`.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the filtered boundaries within the tile\n                extents.\n        \"\"\"\n        if self._boundaries is None:\n            self._boundaries = self.get_filtered_boundaries()\n        return self._boundaries\n\n    @cached_property\n    def transcripts(self) -&gt; pd.DataFrame:\n        \"\"\"Return the filtered transcripts within the tile extents, cached for\n        efficiency.\n\n        The transcripts are computed only once and cached. If the transcripts\n        have not been computed yet, they are computed using\n        `get_filtered_transcripts()`.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the filtered transcripts within the tile\n                extents.\n        \"\"\"\n        if self._transcripts is None:\n            self._transcripts = self.get_filtered_transcripts()\n        return self._transcripts\n\n    def get_filtered_boundaries(self) -&gt; pd.DataFrame:\n        \"\"\"Filter the boundaries in the sample to include only those within\n        the specified tile extents.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the filtered boundaries within the tile\n                extents.\n        \"\"\"\n        filtered_boundaries = utils.filter_boundaries(\n            boundaries=self.dataset.boundaries,\n            inset=self.extents,\n            outset=self.extents.buffer(self.margin, join_style=\"mitre\"),\n            x=self.settings.boundaries.x,\n            y=self.settings.boundaries.y,\n            label=self.settings.boundaries.label,\n        )\n        return filtered_boundaries\n\n    def get_filtered_transcripts(self) -&gt; pd.DataFrame:\n        \"\"\"Filter the transcripts in the sample to include only those within\n        the specified tile extents.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the filtered transcripts within the tile\n                extents.\n        \"\"\"\n\n        # Buffer tile bounds to include transcripts around boundary\n        outset = self.extents.buffer(self.margin, join_style=\"mitre\")\n        xmin, ymin, xmax, ymax = outset.bounds\n\n        # Get transcripts inside buffered region\n        x, y = self.settings.transcripts.xy\n        mask = self.dataset.transcripts[x].between(xmin, xmax)\n        mask &amp;= self.dataset.transcripts[y].between(ymin, ymax)\n        filtered_transcripts = self.dataset.transcripts[mask]\n\n        return filtered_transcripts\n\n    def get_transcript_props(self) -&gt; torch.Tensor:\n        \"\"\"Encode transcript features in a sparse format.\n\n        Returns:\n            torch.Tensor: A sparse tensor containing the encoded transcript features.\n\n        Note:\n            The intention is for this function to simplify testing new strategies\n            for 'tx' node representations. For example, the encoder can be any type\n            of encoder that transforms the transcript labels into a numerical\n            matrix (in sparse format).\n        \"\"\"\n        # Encode transcript features in sparse format\n        embedding = self.dataset.sample._transcript_embedding\n        label = self.settings.transcripts.label\n        props = embedding.embed(self.transcripts[label])\n\n        return props\n\n    @staticmethod\n    def get_polygon_props(\n        polygons: gpd.GeoSeries,\n        area: bool = True,\n        convexity: bool = True,\n        elongation: bool = True,\n        circularity: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Compute geometric properties of polygons.\n\n        Args:\n            polygons: A GeoSeries containing polygon geometries.\n            area: If True, compute the area of each polygon. Defaults to True.\n            convexity: If True, compute the convexity of each polygon. Defaults to True.\n            elongation: If True, compute the elongation of each polygon. Defaults to True.\n            circularity: If True, compute the circularity of each polygon. Defaults to True.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the computed properties for each polygon.\n        \"\"\"\n        props = pd.DataFrame(index=polygons.index, dtype=float)\n        if area:\n            props[\"area\"] = polygons.area\n        if convexity:\n            props[\"convexity\"] = polygons.convex_hull.area / polygons.area\n        if elongation:\n            rects = polygons.minimum_rotated_rectangle()\n            props[\"elongation\"] = rects.area / polygons.envelope.area\n        if circularity:\n            r = polygons.minimum_bounding_radius()\n            props[\"circularity\"] = polygons.area / r**2\n\n        return props\n\n    @staticmethod\n    def get_kdtree_edge_index(\n        index_coords: np.ndarray,\n        query_coords: np.ndarray,\n        k: int,\n        max_distance: float,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute the k-nearest neighbor edge indices using a KDTree.\n\n        Args:\n            index_coords: An array of shape (n_samples, n_features) representing the\n                coordinates of the points to be indexed.\n            query_coords: An array of shape (m_samples, n_features) representing the\n                coordinates of the query points.\n            k: The number of nearest neighbors to find for each query point.\n            max_distance: The maximum distance to consider for neighbors.\n\n        Returns:\n            torch.Tensor: An array of shape (2, n_edges) containing the edge indices. Each\n                column represents an edge between two points, where the first row\n                contains the source indices and the second row contains the target\n                indices.\n        \"\"\"\n        # KDTree search\n        tree = KDTree(index_coords)\n        dist, idx = tree.query(query_coords, k, max_distance)\n\n        # To sparse adjacency\n        edge_index = np.argwhere(dist != np.inf).T\n        edge_index[1] = idx[dist != np.inf]\n        edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n\n        return edge_index\n\n    def get_boundary_props(\n        self,\n        area: bool = True,\n        convexity: bool = True,\n        elongation: bool = True,\n        circularity: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute geometric properties of boundary polygons.\n\n        Args:\n            area: If True, compute the area of each boundary polygon. Defaults to True.\n            convexity: If True, compute the convexity of each boundary polygon. Defaults to True.\n            elongation: If True, compute the elongation of each boundary polygon. Defaults to True.\n            circularity: If True, compute the circularity of each boundary polygon. Defaults to True.\n\n        Returns:\n            torch.Tensor: A tensor containing the computed properties for each boundary\n                polygon.\n\n        Note:\n            The intention is for this function to simplify testing new strategies\n            for 'bd' node representations. You can just change the function body to\n            return another torch.Tensor without worrying about changes to the rest\n            of the code.\n        \"\"\"\n        # Get polygons from coordinates\n        # Use getattr to check for the geometry column\n        geometry_column = getattr(self.settings.boundaries, 'geometry', None)\n        if geometry_column and geometry_column in self.boundaries.columns:\n            polygons = self.boundaries[geometry_column]\n        else:\n            polygons = self.boundaries['geometry']  # Assign None if the geometry column does not exist\n        # Geometric properties of polygons\n        props = self.get_polygon_props(polygons)\n        props = torch.as_tensor(props.values).float()\n\n        return props\n\n    def canonical_edges(edge_index):\n        \"\"\"Sort edge indices to ensure canonical ordering.\n\n        Args:\n            edge_index: The edge index tensor to sort.\n\n        Returns:\n            torch.Tensor: The sorted edge index tensor.\n        \"\"\"\n        return torch.sort(edge_index, dim=0)[0]\n\n    def to_pyg_dataset(\n        self,\n        # train: bool,\n        neg_sampling_ratio: float = 10,\n        k_bd: int = 3,\n        dist_bd: float = 15,\n        k_tx: int = 3,\n        dist_tx: float = 5,\n        k_tx_ex: int = 100,\n        dist_tx_ex: float = 20,\n        area: bool = True,\n        convexity: bool = True,\n        elongation: bool = True,\n        circularity: bool = True,\n        mutually_exclusive_genes: Optional[List] = None,\n    ) -&gt; HeteroData:\n        \"\"\"Convert the sample data to a PyG HeteroData object.\n\n        Args:\n            neg_sampling_ratio: Ratio of negative samples. Defaults to 10.\n            k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n            dist_bd: Maximum distance for boundary neighbors. Defaults to 15.\n            k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n            dist_tx: Maximum distance for transcript neighbors. Defaults to 5.\n            k_tx_ex: Number of nearest neighbors for transcript exclusion. Defaults to 100.\n            dist_tx_ex: Maximum distance for transcript exclusion. Defaults to 20.\n            area: If True, compute area of boundary polygons. Defaults to True.\n            convexity: If True, compute convexity of boundary polygons. Defaults to True.\n            elongation: If True, compute elongation of boundary polygons. Defaults to True.\n            circularity: If True, compute circularity of boundary polygons. Defaults to True.\n            mutually_exclusive_genes: List of mutually exclusive gene pairs. Defaults to None.\n\n        Returns:\n            HeteroData: A PyTorch Geometric HeteroData object containing the sample data.\n        \"\"\"\n        # Initialize an empty HeteroData object\n        pyg_data = HeteroData()\n\n        # Set up Transcript nodes\n        # Get transcript IDs - use getattr to safely check for id attribute\n        transcript_id_column = getattr(self.settings.transcripts, \"id\", None)\n        if transcript_id_column is None:\n            raise ValueError(\n                \"Transcript IDs not found in DataFrame. Please run add_transcript_ids() \"\n                \"as a preprocessing step before creating the dataset.\"\n            )\n\n        # Assign IDs to PyG data\n        pyg_data[\"tx\"].id = torch.tensor(\n            self.transcripts[transcript_id_column].values, dtype=torch.long\n        )\n        pyg_data[\"tx\"].pos = torch.tensor(\n            self.transcripts[self.settings.transcripts.xyz].values,\n            dtype=torch.float32,\n        )\n        pyg_data[\"tx\"].x = self.get_transcript_props()\n\n\n\n        # Set up Transcript-Transcript neighbor edges\n        nbrs_edge_idx = self.get_kdtree_edge_index(\n            self.transcripts[self.settings.transcripts.xyz],\n            self.transcripts[self.settings.transcripts.xyz],\n            k=k_tx,\n            max_distance=dist_tx,\n        )\n\n        # If there are no tx-neighbors-tx edges, skip saving tile\n        if nbrs_edge_idx.shape[1] == 0:\n            return None\n\n        pyg_data[\"tx\", \"neighbors\", \"tx\"].edge_index = nbrs_edge_idx\n\n\n        if mutually_exclusive_genes is not None:\n            # Get potential repulsive edges (k-nearest neighbors within distance)\n            # --- Step 1: Get repulsive edges (mutually exclusive genes) ---\n            repels_edge_idx = self.get_kdtree_edge_index(\n                self.transcripts[self.settings.transcripts.xyz],\n                self.transcripts[self.settings.transcripts.xyz],\n                k=k_tx_ex,\n                max_distance=dist_tx_ex,\n            )\n            gene_ids = self.transcripts[self.settings.transcripts.label].tolist()\n\n            # Filter repels_edge_idx to only keep mutually exclusive gene pairs\n            src_genes = [gene_ids[i] for i in repels_edge_idx[0].tolist()]\n            dst_genes = [gene_ids[i] for i in repels_edge_idx[1].tolist()]\n            mask = [\n                tuple(sorted((a, b))) in mutually_exclusive_genes if a != b else False\n                for a, b in zip(src_genes, dst_genes)\n            ]\n            repels_edge_idx = repels_edge_idx[:, torch.tensor(mask)]\n\n            # --- Step 2: Get attractive edges (same gene, at least one node in repels) ---\n            # Nodes involved in repels (for filtering nbrs_edge_idx)\n            repels_nodes = torch.cat([repels_edge_idx[0], repels_edge_idx[1]]).unique()\n\n            # Filter nbrs_edge_idx: keep edges where (1) same gene AND (2) at least one node in repels\n            attractive_mask = torch.zeros(nbrs_edge_idx.shape[1], dtype=torch.bool)\n            for i, (src, dst) in enumerate(nbrs_edge_idx.t().tolist()):\n                if (src != dst) and (gene_ids[src] == gene_ids[dst]) and (src in repels_nodes or dst in repels_nodes):\n                    attractive_mask[i] = True\n            attractive_edge_idx = nbrs_edge_idx[:, attractive_mask]\n\n            # --- Step 3: Combine repels (label=0) and attractive (label=1) edges ---\n            edge_label_index = torch.cat([repels_edge_idx, attractive_edge_idx], dim=1)\n            edge_label = torch.cat([\n                torch.zeros(repels_edge_idx.shape[1], dtype=torch.long),  # 0 for repels\n                torch.ones(attractive_edge_idx.shape[1], dtype=torch.long)  # 1 for attracts\n            ])\n\n            # --- Step 4: Store in PyG data object ---\n            pyg_data[\"tx\", \"attracts\", \"tx\"].edge_label_index = edge_label_index\n            pyg_data[\"tx\", \"attracts\", \"tx\"].edge_label = edge_label\n\n\n        # Set up Boundary nodes\n        # Check if boundaries have geometries\n        geometry_column = getattr(self.settings.boundaries, 'geometry', None)\n        if geometry_column and geometry_column in self.boundaries.columns:\n            polygons = gpd.GeoSeries(self.boundaries[geometry_column], index=self.boundaries.index)\n        else:\n            # Fallback: compute polygons\n            polygons = utils.get_polygons_from_xy(\n                self.boundaries,\n                x=self.settings.boundaries.x,\n                y=self.settings.boundaries.y,\n                label=self.settings.boundaries.label,\n                scale_factor=self.settings.boundaries.scale_factor,\n            )\n\n        # Ensure self.boundaries is a GeoDataFrame with correct geometry\n        self.boundaries = gpd.GeoDataFrame(self.boundaries.copy(), geometry=polygons)\n        centroids = polygons.centroid.get_coordinates()\n        pyg_data[\"bd\"].id = polygons.index.to_numpy()\n        pyg_data[\"bd\"].pos = torch.tensor(centroids.values, dtype=torch.float32)\n        pyg_data[\"bd\"].x = self.get_boundary_props(\n            area, convexity, elongation, circularity\n        )\n\n        # Set up Boundary-Transcript neighbor edges\n        dist = np.sqrt(polygons.area.max()) * 10  # heuristic distance\n        nbrs_edge_idx = self.get_kdtree_edge_index(\n            centroids,\n            self.transcripts[self.settings.transcripts.xy],\n            k=k_bd,\n            max_distance=dist,\n        )\n        pyg_data[\"tx\", \"neighbors\", \"bd\"].edge_index = nbrs_edge_idx\n\n        # If there are no tx-neighbors-bd edges, we put the tile automatically in test set\n        if nbrs_edge_idx.numel() == 0:\n            # logging.warning(f\"No tx-neighbors-bd edges found in tile {self.uid}.\")\n            pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.tensor(\n                [], dtype=torch.long\n            )\n            return pyg_data\n\n        # Now we identify and split the tx-belongs-bd edges\n        edge_type = (\"tx\", \"belongs\", \"bd\")\n\n        # Find nuclear transcripts\n        tx_cell_ids = self.transcripts[self.settings.boundaries.id]\n        cell_ids_map = {idx: i for (i, idx) in enumerate(polygons.index)}\n\n        # Get nuclear column and value from settings\n        nuclear_column = getattr(self.settings.transcripts, \"nuclear_column\", None)\n        nuclear_value = getattr(self.settings.transcripts, \"nuclear_value\", None)\n\n        if nuclear_column is None or self.settings.boundaries.scale_factor != 1.0:\n            is_nuclear = utils.compute_nuclear_transcripts(\n                polygons=polygons,\n                transcripts=self.transcripts,\n                x_col=self.settings.transcripts.x,\n                y_col=self.settings.transcripts.y,\n                nuclear_column=nuclear_column,\n                nuclear_value=nuclear_value,\n            )\n        else:\n            is_nuclear = self.transcripts[nuclear_column].eq(nuclear_value)\n        is_nuclear &amp;= tx_cell_ids.isin(polygons.index)\n\n        # # Set up overlap edges\n        # row_idx = np.where(is_nuclear)[0]\n        # col_idx = tx_cell_ids.iloc[row_idx].map(cell_ids_map)\n        # blng_edge_idx = torch.tensor(np.stack([row_idx, col_idx])).long()\n        # pyg_data[edge_type].edge_index = blng_edge_idx\n\n        # # If there are no tx-belongs-bd edges, flag tile as test only (cannot be used for training)\n        # if blng_edge_idx.numel() == 0:\n        #     return pyg_data\n\n        #         # If there are tx-bd edges, add negative edges for training\n        # transform = RandomLinkSplit(\n        #     num_val=0,\n        #     num_test=0,\n        #     is_undirected=True,\n        #     edge_types=[edge_type],\n        #     neg_sampling_ratio=neg_sampling_ratio,\n        # )\n        # pyg_data, _, _ = transform(pyg_data)\n\n        # # Refilter negative edges to include only transcripts in the\n        # # original positive edges (still need a memory-efficient solution)\n        # edges = pyg_data[edge_type]\n        # mask = edges.edge_label_index[0].unsqueeze(1) == edges.edge_index[0].unsqueeze(\n        #     0\n        # )\n        # mask = torch.nonzero(torch.any(mask, 1)).squeeze()\n        # edges.edge_label_index = edges.edge_label_index[:, mask]\n        # edges.edge_label = edges.edge_label[mask]\n\n        # return pyg_data\n\n\n        # Set up overlap edges\n        row_idx = np.where(is_nuclear)[0]\n        col_idx = tx_cell_ids.iloc[row_idx].map(cell_ids_map)\n        blng_edge_idx = torch.tensor(np.stack([row_idx, col_idx])).long()\n        pyg_data[edge_type].edge_index = blng_edge_idx\n\n        # If there are no tx-belongs-bd edges, flag tile as test only (cannot be used for training)\n        if blng_edge_idx.numel() == 0:\n            return pyg_data\n\n        # If there are tx-bd edges, add negative edges for training\n        pos_edges = blng_edge_idx  # shape (2, num_pos)\n        num_pos = pos_edges.shape[1]\n\n        # Negative edges (tx-neighbors-bd) - EXCLUDE positives\n        neg_candidates = nbrs_edge_idx  # shape (2, num_candidates)\n\n        # --- Fast Negative Filtering (PyTorch-only) ---\n        # Reshape edges for broadcasting: (2, num_pos) vs (2, num_candidates, 1)\n        pos_expanded = pos_edges.unsqueeze(2)  # shape (2, num_pos, 1)\n        neg_expanded = neg_candidates.unsqueeze(1)  # shape (2, 1, num_candidates)\n\n        # Compare all edges in one go (broadcasting)\n        matches = (pos_expanded == neg_expanded).all(dim=0)  # shape (num_pos, num_candidates)\n        is_negative = ~matches.any(dim=0)  # shape (num_candidates,)\n\n        # Filter negatives\n        neg_edges = neg_candidates[:, is_negative]  # shape (2, num_filtered_neg)\n        num_neg = neg_edges.shape[1]\n\n        # --- Combine and label ---\n        edge_label_index = torch.cat([neg_edges, pos_edges], dim=1)\n        edge_label = torch.cat([\n            torch.zeros(num_neg, dtype=torch.float),\n            torch.ones(num_pos, dtype=torch.float)\n        ])\n\n        mask = edge_label_index[0].unsqueeze(1) == blng_edge_idx[0].unsqueeze(0)\n        mask = torch.nonzero(torch.any(mask, 1)).squeeze()\n        edge_label_index = edge_label_index[:, mask]\n        edge_label = edge_label[mask]\n\n        pyg_data[edge_type].edge_label_index = edge_label_index\n        pyg_data[edge_type].edge_label = edge_label\n\n        return pyg_data\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.boundaries","title":"boundaries  <code>cached</code> <code>property</code>","text":"<pre><code>boundaries\n</code></pre> <p>Return the filtered boundaries within the tile extents, cached for efficiency.</p> <p>The boundaries are computed only once and cached. If the boundaries have not been computed yet, they are computed using <code>get_filtered_boundaries()</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered boundaries within the tile extents.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.transcripts","title":"transcripts  <code>cached</code> <code>property</code>","text":"<pre><code>transcripts\n</code></pre> <p>Return the filtered transcripts within the tile extents, cached for efficiency.</p> <p>The transcripts are computed only once and cached. If the transcripts have not been computed yet, they are computed using <code>get_filtered_transcripts()</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered transcripts within the tile extents.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.uid","title":"uid  <code>property</code>","text":"<pre><code>uid\n</code></pre> <p>Generate a unique identifier for the tile based on its extents.</p> <p>This UID is particularly useful for saving or indexing tiles in distributed processing environments.</p> <p>The UID is constructed using the minimum and maximum x and y coordinates of the tile's bounding box, representing its position and size in the sample.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique identifier string in the format 'x=_y=_w=_h=' where: - <code>&lt;x_min&gt;</code>: Minimum x-coordinate of the tile's extents. - <code>&lt;y_min&gt;</code>: Minimum y-coordinate of the tile's extents. - <code>&lt;width&gt;</code>: Width of the tile. - <code>&lt;height&gt;</code>: Height of the tile. Example <p>If the tile's extents are bounded by (x_min, y_min) = (100, 200) and (x_max, y_max) = (150, 250), the generated UID would be: 'x=100_y=200_w=50_h=50'</p>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.__init__","title":"__init__","text":"<pre><code>__init__(dataset, extents)\n</code></pre> <p>Initialize a STTile instance.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>STInMemoryDataset</code> <p>The ST dataset containing data.</p> required <code>extents</code> <code>Polygon</code> <p>The extents of the tile in the sample.</p> required Note <p>The <code>boundaries</code> and <code>transcripts</code> attributes are cached to avoid the overhead of filtering when tiles are instantiated. This is particularly useful in multiprocessing settings where generating tiles in parallel could lead to high overhead.</p> Internal Args <p>_boundaries: Cached DataFrame of filtered boundaries. Initially set to None. _transcripts: Cached DataFrame of filtered transcripts. Initially set to None.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def __init__(\n    self,\n    dataset: STInMemoryDataset,\n    extents: shapely.Polygon,\n):\n    \"\"\"Initialize a STTile instance.\n\n    Args:\n        dataset: The ST dataset containing data.\n        extents: The extents of the tile in the sample.\n\n    Note:\n        The `boundaries` and `transcripts` attributes are cached to avoid the\n        overhead of filtering when tiles are instantiated. This is particularly\n        useful in multiprocessing settings where generating tiles in parallel\n        could lead to high overhead.\n\n    Internal Args:\n        _boundaries: Cached DataFrame of filtered boundaries. Initially set to None.\n        _transcripts: Cached DataFrame of filtered transcripts. Initially set to None.\n    \"\"\"\n    self.dataset = dataset\n    self.extents = extents\n    self.margin = dataset.margin\n    self.settings = self.dataset.settings\n\n    # Internal caches for filtered data\n    self._boundaries = None\n    self._transcripts = None\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.canonical_edges","title":"canonical_edges","text":"<pre><code>canonical_edges(edge_index)\n</code></pre> <p>Sort edge indices to ensure canonical ordering.</p> <p>Parameters:</p> Name Type Description Default <code>edge_index</code> <p>The edge index tensor to sort.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The sorted edge index tensor.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def canonical_edges(edge_index):\n    \"\"\"Sort edge indices to ensure canonical ordering.\n\n    Args:\n        edge_index: The edge index tensor to sort.\n\n    Returns:\n        torch.Tensor: The sorted edge index tensor.\n    \"\"\"\n    return torch.sort(edge_index, dim=0)[0]\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_boundary_props","title":"get_boundary_props","text":"<pre><code>get_boundary_props(area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Compute geometric properties of boundary polygons.</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>bool</code> <p>If True, compute the area of each boundary polygon. Defaults to True.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>If True, compute the convexity of each boundary polygon. Defaults to True.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>If True, compute the elongation of each boundary polygon. Defaults to True.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>If True, compute the circularity of each boundary polygon. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the computed properties for each boundary polygon.</p> Note <p>The intention is for this function to simplify testing new strategies for 'bd' node representations. You can just change the function body to return another torch.Tensor without worrying about changes to the rest of the code.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_boundary_props(\n    self,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Compute geometric properties of boundary polygons.\n\n    Args:\n        area: If True, compute the area of each boundary polygon. Defaults to True.\n        convexity: If True, compute the convexity of each boundary polygon. Defaults to True.\n        elongation: If True, compute the elongation of each boundary polygon. Defaults to True.\n        circularity: If True, compute the circularity of each boundary polygon. Defaults to True.\n\n    Returns:\n        torch.Tensor: A tensor containing the computed properties for each boundary\n            polygon.\n\n    Note:\n        The intention is for this function to simplify testing new strategies\n        for 'bd' node representations. You can just change the function body to\n        return another torch.Tensor without worrying about changes to the rest\n        of the code.\n    \"\"\"\n    # Get polygons from coordinates\n    # Use getattr to check for the geometry column\n    geometry_column = getattr(self.settings.boundaries, 'geometry', None)\n    if geometry_column and geometry_column in self.boundaries.columns:\n        polygons = self.boundaries[geometry_column]\n    else:\n        polygons = self.boundaries['geometry']  # Assign None if the geometry column does not exist\n    # Geometric properties of polygons\n    props = self.get_polygon_props(polygons)\n    props = torch.as_tensor(props.values).float()\n\n    return props\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_filtered_boundaries","title":"get_filtered_boundaries","text":"<pre><code>get_filtered_boundaries()\n</code></pre> <p>Filter the boundaries in the sample to include only those within the specified tile extents.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered boundaries within the tile extents.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_filtered_boundaries(self) -&gt; pd.DataFrame:\n    \"\"\"Filter the boundaries in the sample to include only those within\n    the specified tile extents.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered boundaries within the tile\n            extents.\n    \"\"\"\n    filtered_boundaries = utils.filter_boundaries(\n        boundaries=self.dataset.boundaries,\n        inset=self.extents,\n        outset=self.extents.buffer(self.margin, join_style=\"mitre\"),\n        x=self.settings.boundaries.x,\n        y=self.settings.boundaries.y,\n        label=self.settings.boundaries.label,\n    )\n    return filtered_boundaries\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_filtered_transcripts","title":"get_filtered_transcripts","text":"<pre><code>get_filtered_transcripts()\n</code></pre> <p>Filter the transcripts in the sample to include only those within the specified tile extents.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered transcripts within the tile extents.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_filtered_transcripts(self) -&gt; pd.DataFrame:\n    \"\"\"Filter the transcripts in the sample to include only those within\n    the specified tile extents.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered transcripts within the tile\n            extents.\n    \"\"\"\n\n    # Buffer tile bounds to include transcripts around boundary\n    outset = self.extents.buffer(self.margin, join_style=\"mitre\")\n    xmin, ymin, xmax, ymax = outset.bounds\n\n    # Get transcripts inside buffered region\n    x, y = self.settings.transcripts.xy\n    mask = self.dataset.transcripts[x].between(xmin, xmax)\n    mask &amp;= self.dataset.transcripts[y].between(ymin, ymax)\n    filtered_transcripts = self.dataset.transcripts[mask]\n\n    return filtered_transcripts\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_kdtree_edge_index","title":"get_kdtree_edge_index  <code>staticmethod</code>","text":"<pre><code>get_kdtree_edge_index(index_coords, query_coords, k, max_distance)\n</code></pre> <p>Compute the k-nearest neighbor edge indices using a KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>index_coords</code> <code>ndarray</code> <p>An array of shape (n_samples, n_features) representing the coordinates of the points to be indexed.</p> required <code>query_coords</code> <code>ndarray</code> <p>An array of shape (m_samples, n_features) representing the coordinates of the query points.</p> required <code>k</code> <code>int</code> <p>The number of nearest neighbors to find for each query point.</p> required <code>max_distance</code> <code>float</code> <p>The maximum distance to consider for neighbors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: An array of shape (2, n_edges) containing the edge indices. Each column represents an edge between two points, where the first row contains the source indices and the second row contains the target indices.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>@staticmethod\ndef get_kdtree_edge_index(\n    index_coords: np.ndarray,\n    query_coords: np.ndarray,\n    k: int,\n    max_distance: float,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the k-nearest neighbor edge indices using a KDTree.\n\n    Args:\n        index_coords: An array of shape (n_samples, n_features) representing the\n            coordinates of the points to be indexed.\n        query_coords: An array of shape (m_samples, n_features) representing the\n            coordinates of the query points.\n        k: The number of nearest neighbors to find for each query point.\n        max_distance: The maximum distance to consider for neighbors.\n\n    Returns:\n        torch.Tensor: An array of shape (2, n_edges) containing the edge indices. Each\n            column represents an edge between two points, where the first row\n            contains the source indices and the second row contains the target\n            indices.\n    \"\"\"\n    # KDTree search\n    tree = KDTree(index_coords)\n    dist, idx = tree.query(query_coords, k, max_distance)\n\n    # To sparse adjacency\n    edge_index = np.argwhere(dist != np.inf).T\n    edge_index[1] = idx[dist != np.inf]\n    edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n\n    return edge_index\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_polygon_props","title":"get_polygon_props  <code>staticmethod</code>","text":"<pre><code>get_polygon_props(polygons, area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Compute geometric properties of polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>GeoSeries</code> <p>A GeoSeries containing polygon geometries.</p> required <code>area</code> <code>bool</code> <p>If True, compute the area of each polygon. Defaults to True.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>If True, compute the convexity of each polygon. Defaults to True.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>If True, compute the elongation of each polygon. Defaults to True.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>If True, compute the circularity of each polygon. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the computed properties for each polygon.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>@staticmethod\ndef get_polygon_props(\n    polygons: gpd.GeoSeries,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute geometric properties of polygons.\n\n    Args:\n        polygons: A GeoSeries containing polygon geometries.\n        area: If True, compute the area of each polygon. Defaults to True.\n        convexity: If True, compute the convexity of each polygon. Defaults to True.\n        elongation: If True, compute the elongation of each polygon. Defaults to True.\n        circularity: If True, compute the circularity of each polygon. Defaults to True.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the computed properties for each polygon.\n    \"\"\"\n    props = pd.DataFrame(index=polygons.index, dtype=float)\n    if area:\n        props[\"area\"] = polygons.area\n    if convexity:\n        props[\"convexity\"] = polygons.convex_hull.area / polygons.area\n    if elongation:\n        rects = polygons.minimum_rotated_rectangle()\n        props[\"elongation\"] = rects.area / polygons.envelope.area\n    if circularity:\n        r = polygons.minimum_bounding_radius()\n        props[\"circularity\"] = polygons.area / r**2\n\n    return props\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_transcript_props","title":"get_transcript_props","text":"<pre><code>get_transcript_props()\n</code></pre> <p>Encode transcript features in a sparse format.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A sparse tensor containing the encoded transcript features.</p> Note <p>The intention is for this function to simplify testing new strategies for 'tx' node representations. For example, the encoder can be any type of encoder that transforms the transcript labels into a numerical matrix (in sparse format).</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_transcript_props(self) -&gt; torch.Tensor:\n    \"\"\"Encode transcript features in a sparse format.\n\n    Returns:\n        torch.Tensor: A sparse tensor containing the encoded transcript features.\n\n    Note:\n        The intention is for this function to simplify testing new strategies\n        for 'tx' node representations. For example, the encoder can be any type\n        of encoder that transforms the transcript labels into a numerical\n        matrix (in sparse format).\n    \"\"\"\n    # Encode transcript features in sparse format\n    embedding = self.dataset.sample._transcript_embedding\n    label = self.settings.transcripts.label\n    props = embedding.embed(self.transcripts[label])\n\n    return props\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.to_pyg_dataset","title":"to_pyg_dataset","text":"<pre><code>to_pyg_dataset(neg_sampling_ratio=10, k_bd=3, dist_bd=15, k_tx=3, dist_tx=5, k_tx_ex=100, dist_tx_ex=20, area=True, convexity=True, elongation=True, circularity=True, mutually_exclusive_genes=None)\n</code></pre> <p>Convert the sample data to a PyG HeteroData object.</p> <p>Parameters:</p> Name Type Description Default <code>neg_sampling_ratio</code> <code>float</code> <p>Ratio of negative samples. Defaults to 10.</p> <code>10</code> <code>k_bd</code> <code>int</code> <p>Number of nearest neighbors for boundary nodes. Defaults to 3.</p> <code>3</code> <code>dist_bd</code> <code>float</code> <p>Maximum distance for boundary neighbors. Defaults to 15.</p> <code>15</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for transcript nodes. Defaults to 3.</p> <code>3</code> <code>dist_tx</code> <code>float</code> <p>Maximum distance for transcript neighbors. Defaults to 5.</p> <code>5</code> <code>k_tx_ex</code> <code>int</code> <p>Number of nearest neighbors for transcript exclusion. Defaults to 100.</p> <code>100</code> <code>dist_tx_ex</code> <code>float</code> <p>Maximum distance for transcript exclusion. Defaults to 20.</p> <code>20</code> <code>area</code> <code>bool</code> <p>If True, compute area of boundary polygons. Defaults to True.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>If True, compute convexity of boundary polygons. Defaults to True.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>If True, compute elongation of boundary polygons. Defaults to True.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>If True, compute circularity of boundary polygons. Defaults to True.</p> <code>True</code> <code>mutually_exclusive_genes</code> <code>Optional[List]</code> <p>List of mutually exclusive gene pairs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HeteroData</code> <code>HeteroData</code> <p>A PyTorch Geometric HeteroData object containing the sample data.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def to_pyg_dataset(\n    self,\n    # train: bool,\n    neg_sampling_ratio: float = 10,\n    k_bd: int = 3,\n    dist_bd: float = 15,\n    k_tx: int = 3,\n    dist_tx: float = 5,\n    k_tx_ex: int = 100,\n    dist_tx_ex: float = 20,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n    mutually_exclusive_genes: Optional[List] = None,\n) -&gt; HeteroData:\n    \"\"\"Convert the sample data to a PyG HeteroData object.\n\n    Args:\n        neg_sampling_ratio: Ratio of negative samples. Defaults to 10.\n        k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n        dist_bd: Maximum distance for boundary neighbors. Defaults to 15.\n        k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n        dist_tx: Maximum distance for transcript neighbors. Defaults to 5.\n        k_tx_ex: Number of nearest neighbors for transcript exclusion. Defaults to 100.\n        dist_tx_ex: Maximum distance for transcript exclusion. Defaults to 20.\n        area: If True, compute area of boundary polygons. Defaults to True.\n        convexity: If True, compute convexity of boundary polygons. Defaults to True.\n        elongation: If True, compute elongation of boundary polygons. Defaults to True.\n        circularity: If True, compute circularity of boundary polygons. Defaults to True.\n        mutually_exclusive_genes: List of mutually exclusive gene pairs. Defaults to None.\n\n    Returns:\n        HeteroData: A PyTorch Geometric HeteroData object containing the sample data.\n    \"\"\"\n    # Initialize an empty HeteroData object\n    pyg_data = HeteroData()\n\n    # Set up Transcript nodes\n    # Get transcript IDs - use getattr to safely check for id attribute\n    transcript_id_column = getattr(self.settings.transcripts, \"id\", None)\n    if transcript_id_column is None:\n        raise ValueError(\n            \"Transcript IDs not found in DataFrame. Please run add_transcript_ids() \"\n            \"as a preprocessing step before creating the dataset.\"\n        )\n\n    # Assign IDs to PyG data\n    pyg_data[\"tx\"].id = torch.tensor(\n        self.transcripts[transcript_id_column].values, dtype=torch.long\n    )\n    pyg_data[\"tx\"].pos = torch.tensor(\n        self.transcripts[self.settings.transcripts.xyz].values,\n        dtype=torch.float32,\n    )\n    pyg_data[\"tx\"].x = self.get_transcript_props()\n\n\n\n    # Set up Transcript-Transcript neighbor edges\n    nbrs_edge_idx = self.get_kdtree_edge_index(\n        self.transcripts[self.settings.transcripts.xyz],\n        self.transcripts[self.settings.transcripts.xyz],\n        k=k_tx,\n        max_distance=dist_tx,\n    )\n\n    # If there are no tx-neighbors-tx edges, skip saving tile\n    if nbrs_edge_idx.shape[1] == 0:\n        return None\n\n    pyg_data[\"tx\", \"neighbors\", \"tx\"].edge_index = nbrs_edge_idx\n\n\n    if mutually_exclusive_genes is not None:\n        # Get potential repulsive edges (k-nearest neighbors within distance)\n        # --- Step 1: Get repulsive edges (mutually exclusive genes) ---\n        repels_edge_idx = self.get_kdtree_edge_index(\n            self.transcripts[self.settings.transcripts.xyz],\n            self.transcripts[self.settings.transcripts.xyz],\n            k=k_tx_ex,\n            max_distance=dist_tx_ex,\n        )\n        gene_ids = self.transcripts[self.settings.transcripts.label].tolist()\n\n        # Filter repels_edge_idx to only keep mutually exclusive gene pairs\n        src_genes = [gene_ids[i] for i in repels_edge_idx[0].tolist()]\n        dst_genes = [gene_ids[i] for i in repels_edge_idx[1].tolist()]\n        mask = [\n            tuple(sorted((a, b))) in mutually_exclusive_genes if a != b else False\n            for a, b in zip(src_genes, dst_genes)\n        ]\n        repels_edge_idx = repels_edge_idx[:, torch.tensor(mask)]\n\n        # --- Step 2: Get attractive edges (same gene, at least one node in repels) ---\n        # Nodes involved in repels (for filtering nbrs_edge_idx)\n        repels_nodes = torch.cat([repels_edge_idx[0], repels_edge_idx[1]]).unique()\n\n        # Filter nbrs_edge_idx: keep edges where (1) same gene AND (2) at least one node in repels\n        attractive_mask = torch.zeros(nbrs_edge_idx.shape[1], dtype=torch.bool)\n        for i, (src, dst) in enumerate(nbrs_edge_idx.t().tolist()):\n            if (src != dst) and (gene_ids[src] == gene_ids[dst]) and (src in repels_nodes or dst in repels_nodes):\n                attractive_mask[i] = True\n        attractive_edge_idx = nbrs_edge_idx[:, attractive_mask]\n\n        # --- Step 3: Combine repels (label=0) and attractive (label=1) edges ---\n        edge_label_index = torch.cat([repels_edge_idx, attractive_edge_idx], dim=1)\n        edge_label = torch.cat([\n            torch.zeros(repels_edge_idx.shape[1], dtype=torch.long),  # 0 for repels\n            torch.ones(attractive_edge_idx.shape[1], dtype=torch.long)  # 1 for attracts\n        ])\n\n        # --- Step 4: Store in PyG data object ---\n        pyg_data[\"tx\", \"attracts\", \"tx\"].edge_label_index = edge_label_index\n        pyg_data[\"tx\", \"attracts\", \"tx\"].edge_label = edge_label\n\n\n    # Set up Boundary nodes\n    # Check if boundaries have geometries\n    geometry_column = getattr(self.settings.boundaries, 'geometry', None)\n    if geometry_column and geometry_column in self.boundaries.columns:\n        polygons = gpd.GeoSeries(self.boundaries[geometry_column], index=self.boundaries.index)\n    else:\n        # Fallback: compute polygons\n        polygons = utils.get_polygons_from_xy(\n            self.boundaries,\n            x=self.settings.boundaries.x,\n            y=self.settings.boundaries.y,\n            label=self.settings.boundaries.label,\n            scale_factor=self.settings.boundaries.scale_factor,\n        )\n\n    # Ensure self.boundaries is a GeoDataFrame with correct geometry\n    self.boundaries = gpd.GeoDataFrame(self.boundaries.copy(), geometry=polygons)\n    centroids = polygons.centroid.get_coordinates()\n    pyg_data[\"bd\"].id = polygons.index.to_numpy()\n    pyg_data[\"bd\"].pos = torch.tensor(centroids.values, dtype=torch.float32)\n    pyg_data[\"bd\"].x = self.get_boundary_props(\n        area, convexity, elongation, circularity\n    )\n\n    # Set up Boundary-Transcript neighbor edges\n    dist = np.sqrt(polygons.area.max()) * 10  # heuristic distance\n    nbrs_edge_idx = self.get_kdtree_edge_index(\n        centroids,\n        self.transcripts[self.settings.transcripts.xy],\n        k=k_bd,\n        max_distance=dist,\n    )\n    pyg_data[\"tx\", \"neighbors\", \"bd\"].edge_index = nbrs_edge_idx\n\n    # If there are no tx-neighbors-bd edges, we put the tile automatically in test set\n    if nbrs_edge_idx.numel() == 0:\n        # logging.warning(f\"No tx-neighbors-bd edges found in tile {self.uid}.\")\n        pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.tensor(\n            [], dtype=torch.long\n        )\n        return pyg_data\n\n    # Now we identify and split the tx-belongs-bd edges\n    edge_type = (\"tx\", \"belongs\", \"bd\")\n\n    # Find nuclear transcripts\n    tx_cell_ids = self.transcripts[self.settings.boundaries.id]\n    cell_ids_map = {idx: i for (i, idx) in enumerate(polygons.index)}\n\n    # Get nuclear column and value from settings\n    nuclear_column = getattr(self.settings.transcripts, \"nuclear_column\", None)\n    nuclear_value = getattr(self.settings.transcripts, \"nuclear_value\", None)\n\n    if nuclear_column is None or self.settings.boundaries.scale_factor != 1.0:\n        is_nuclear = utils.compute_nuclear_transcripts(\n            polygons=polygons,\n            transcripts=self.transcripts,\n            x_col=self.settings.transcripts.x,\n            y_col=self.settings.transcripts.y,\n            nuclear_column=nuclear_column,\n            nuclear_value=nuclear_value,\n        )\n    else:\n        is_nuclear = self.transcripts[nuclear_column].eq(nuclear_value)\n    is_nuclear &amp;= tx_cell_ids.isin(polygons.index)\n\n    # # Set up overlap edges\n    # row_idx = np.where(is_nuclear)[0]\n    # col_idx = tx_cell_ids.iloc[row_idx].map(cell_ids_map)\n    # blng_edge_idx = torch.tensor(np.stack([row_idx, col_idx])).long()\n    # pyg_data[edge_type].edge_index = blng_edge_idx\n\n    # # If there are no tx-belongs-bd edges, flag tile as test only (cannot be used for training)\n    # if blng_edge_idx.numel() == 0:\n    #     return pyg_data\n\n    #         # If there are tx-bd edges, add negative edges for training\n    # transform = RandomLinkSplit(\n    #     num_val=0,\n    #     num_test=0,\n    #     is_undirected=True,\n    #     edge_types=[edge_type],\n    #     neg_sampling_ratio=neg_sampling_ratio,\n    # )\n    # pyg_data, _, _ = transform(pyg_data)\n\n    # # Refilter negative edges to include only transcripts in the\n    # # original positive edges (still need a memory-efficient solution)\n    # edges = pyg_data[edge_type]\n    # mask = edges.edge_label_index[0].unsqueeze(1) == edges.edge_index[0].unsqueeze(\n    #     0\n    # )\n    # mask = torch.nonzero(torch.any(mask, 1)).squeeze()\n    # edges.edge_label_index = edges.edge_label_index[:, mask]\n    # edges.edge_label = edges.edge_label[mask]\n\n    # return pyg_data\n\n\n    # Set up overlap edges\n    row_idx = np.where(is_nuclear)[0]\n    col_idx = tx_cell_ids.iloc[row_idx].map(cell_ids_map)\n    blng_edge_idx = torch.tensor(np.stack([row_idx, col_idx])).long()\n    pyg_data[edge_type].edge_index = blng_edge_idx\n\n    # If there are no tx-belongs-bd edges, flag tile as test only (cannot be used for training)\n    if blng_edge_idx.numel() == 0:\n        return pyg_data\n\n    # If there are tx-bd edges, add negative edges for training\n    pos_edges = blng_edge_idx  # shape (2, num_pos)\n    num_pos = pos_edges.shape[1]\n\n    # Negative edges (tx-neighbors-bd) - EXCLUDE positives\n    neg_candidates = nbrs_edge_idx  # shape (2, num_candidates)\n\n    # --- Fast Negative Filtering (PyTorch-only) ---\n    # Reshape edges for broadcasting: (2, num_pos) vs (2, num_candidates, 1)\n    pos_expanded = pos_edges.unsqueeze(2)  # shape (2, num_pos, 1)\n    neg_expanded = neg_candidates.unsqueeze(1)  # shape (2, 1, num_candidates)\n\n    # Compare all edges in one go (broadcasting)\n    matches = (pos_expanded == neg_expanded).all(dim=0)  # shape (num_pos, num_candidates)\n    is_negative = ~matches.any(dim=0)  # shape (num_candidates,)\n\n    # Filter negatives\n    neg_edges = neg_candidates[:, is_negative]  # shape (2, num_filtered_neg)\n    num_neg = neg_edges.shape[1]\n\n    # --- Combine and label ---\n    edge_label_index = torch.cat([neg_edges, pos_edges], dim=1)\n    edge_label = torch.cat([\n        torch.zeros(num_neg, dtype=torch.float),\n        torch.ones(num_pos, dtype=torch.float)\n    ])\n\n    mask = edge_label_index[0].unsqueeze(1) == blng_edge_idx[0].unsqueeze(0)\n    mask = torch.nonzero(torch.any(mask, 1)).squeeze()\n    edge_label_index = edge_label_index[:, mask]\n    edge_label = edge_label[mask]\n\n    pyg_data[edge_type].edge_label_index = edge_label_index\n    pyg_data[edge_type].edge_label = edge_label\n\n    return pyg_data\n</code></pre>"},{"location":"api/data/sample/#usage-examples","title":"Usage Examples","text":""},{"location":"api/data/sample/#basic-data-loading","title":"Basic Data Loading","text":"<pre><code>from segger.data.sample import STSampleParquet\n\n# Load a spatial transcriptomics sample\nsample = STSampleParquet(\n    base_dir=\"/path/to/xenium/data\",\n    n_workers=4,\n    sample_type=\"xenium\"\n)\n\n# Get sample information\nprint(f\"Transcripts: {sample.n_transcripts}\")\nprint(f\"Spatial extents: {sample.extents}\")\nprint(f\"Feature names: {sample.transcripts_metadata['feature_names'][:5]}\")\n</code></pre>"},{"location":"api/data/sample/#spatial-tiling-and-processing","title":"Spatial Tiling and Processing","text":"<pre><code># Save processed tiles\nsample.save(\n    data_dir=\"./processed_data\",\n    tile_size=1000,  # 1000 transcripts per tile\n    k_bd=3,          # 3 boundary neighbors\n    k_tx=5,          # 5 transcript neighbors\n    dist_bd=15.0,    # 15 pixel boundary distance\n    dist_tx=5.0,     # 5 pixel transcript distance\n    frac=0.8,        # Process 80% of data\n    val_prob=0.1,    # 10% validation\n    test_prob=0.2    # 20% test\n)\n</code></pre>"},{"location":"api/data/sample/#in-memory-dataset-processing","title":"In-Memory Dataset Processing","text":"<pre><code>from segger.data.sample import STInMemoryDataset\n\n# Create dataset for a specific region\ndataset = STInMemoryDataset(\n    sample=sample,\n    extents=region_polygon,\n    margin=10\n)\n\n# Generate tiles\ntiles = dataset._tile(\n    width=100,    # 100 pixel width\n    height=100    # 100 pixel height\n)\n\nprint(f\"Generated {len(tiles)} tiles\")\n</code></pre>"},{"location":"api/data/sample/#individual-tile-processing","title":"Individual Tile Processing","text":"<pre><code>from segger.data.sample import STTile\n\n# Process individual tile\ntile = STTile(dataset=dataset, extents=tile_polygon)\n\n# Get tile data\ntranscripts = tile.transcripts\nboundaries = tile.boundaries\n\n# Convert to PyG format\npyg_data = tile.to_pyg_dataset(\n    k_bd=3,\n    dist_bd=15,\n    k_tx=5,\n    dist_tx=5,\n    area=True,\n    convexity=True,\n    elongation=True,\n    circularity=True\n)\n\nprint(f\"Tile UID: {tile.uid}\")\nprint(f\"Transcripts: {len(transcripts)}\")\nprint(f\"Boundaries: {len(boundaries)}\")\n</code></pre>"},{"location":"api/data/transcript_embedding/","title":"segger.data.transcript_embedding","text":"<p>The <code>transcript_embedding</code> module provides utilities for encoding transcript features into numerical representations suitable for machine learning models. This module handles the conversion of gene names and transcript labels into embeddings that can be used in graph neural networks.</p>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding","title":"TranscriptEmbedding","text":"<p>               Bases: <code>Module</code></p> <p>Utility class to handle transcript embeddings in PyTorch so that they are optionally learnable in the future.</p> <p>Default behavior is to use the index of gene names.</p> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>class TranscriptEmbedding(torch.nn.Module):\n    \"\"\"Utility class to handle transcript embeddings in PyTorch so that they are\n    optionally learnable in the future.\n\n    Default behavior is to use the index of gene names.\n    \"\"\"\n\n    # TODO: Add documentation\n    @staticmethod\n    def _check_inputs(\n        classes: ArrayLike,\n        weights: Union[pd.DataFrame, None],\n    ):\n        \"\"\"Check input arguments for validity.\n\n        Args:\n            classes: A 1D array of unique class names.\n            weights: Optional DataFrame containing weights for each class. Defaults to None.\n\n        Raises:\n            ValueError: If classes is not 1D, contains duplicates, or if weights DataFrame\n                is missing entries for some classes.\n        \"\"\"\n        # Classes is a 1D array\n        if len(classes.shape) &gt; 1:\n            msg = (\n                \"'classes' should be a 1D array, got an array of shape \"\n                f\"{classes.shape} instead.\"\n            )\n            raise ValueError(msg)\n        # Items appear exactly once\n        if len(classes) != len(set(classes)):\n            msg = (\n                \"All embedding classes must be unique. One or more items in \"\n                \"'classes' appears twice.\"\n            )\n            raise ValueError(msg)\n        # All classes have an entry in weights\n        elif weights is not None:\n            missing = set(classes).difference(weights.index)\n            if len(missing) &gt; 0:\n                msg = (\n                    f\"Index of 'weights' DataFrame is missing {len(missing)} \"\n                    \"entries compared to classes.\"\n                )\n                raise ValueError(msg)\n\n    # TODO: Add documentation\n    def __init__(\n        self,\n        classes: ArrayLike,\n        weights: Optional[pd.DataFrame] = None,\n    ):\n        \"\"\"Initialize the TranscriptEmbedding module.\n\n        Args:\n            classes: A 1D array of unique class names.\n            weights: Optional DataFrame containing weights for each class. Defaults to None.\n        \"\"\"\n        # check input arguments\n        TranscriptEmbedding._check_inputs(classes, weights)\n        # Setup as PyTorch module\n        super(TranscriptEmbedding, self).__init__()\n        self._encoder = LabelEncoder().fit(classes)\n        if weights is None:\n            self._weights = None\n        else:\n            self._weights = Tensor(weights.loc[classes].values)\n\n    # TODO: Add documentation\n    def embed(self, classes: ArrayLike):\n        \"\"\"Embed transcript classes into numerical representations.\n\n        Args:\n            classes: Array of class names to embed.\n\n        Returns:\n            Union[LongTensor, torch.Tensor]: If no weights provided, returns indices.\n                If weights provided, returns embedded representations.\n        \"\"\"\n        indices = LongTensor(self._encoder.transform(classes))\n        # Default, one-hot encoding\n        if self._weights is None:\n            return indices  # F.one_hot(indices, len(self._encoder.classes_))\n        else:\n            return F.embedding(indices, self._weights)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding._encoder","title":"_encoder  <code>instance-attribute</code>","text":"<pre><code>_encoder = fit(classes)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding._weights","title":"_weights  <code>instance-attribute</code>","text":"<pre><code>_weights = None\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(classes, weights=None)\n</code></pre> <p>Initialize the TranscriptEmbedding module.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>ArrayLike</code> <p>A 1D array of unique class names.</p> required <code>weights</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame containing weights for each class. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>def __init__(\n    self,\n    classes: ArrayLike,\n    weights: Optional[pd.DataFrame] = None,\n):\n    \"\"\"Initialize the TranscriptEmbedding module.\n\n    Args:\n        classes: A 1D array of unique class names.\n        weights: Optional DataFrame containing weights for each class. Defaults to None.\n    \"\"\"\n    # check input arguments\n    TranscriptEmbedding._check_inputs(classes, weights)\n    # Setup as PyTorch module\n    super(TranscriptEmbedding, self).__init__()\n    self._encoder = LabelEncoder().fit(classes)\n    if weights is None:\n        self._weights = None\n    else:\n        self._weights = Tensor(weights.loc[classes].values)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding._check_inputs","title":"_check_inputs  <code>staticmethod</code>","text":"<pre><code>_check_inputs(classes, weights)\n</code></pre> <p>Check input arguments for validity.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>ArrayLike</code> <p>A 1D array of unique class names.</p> required <code>weights</code> <code>Union[DataFrame, None]</code> <p>Optional DataFrame containing weights for each class. Defaults to None.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If classes is not 1D, contains duplicates, or if weights DataFrame is missing entries for some classes.</p> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>@staticmethod\ndef _check_inputs(\n    classes: ArrayLike,\n    weights: Union[pd.DataFrame, None],\n):\n    \"\"\"Check input arguments for validity.\n\n    Args:\n        classes: A 1D array of unique class names.\n        weights: Optional DataFrame containing weights for each class. Defaults to None.\n\n    Raises:\n        ValueError: If classes is not 1D, contains duplicates, or if weights DataFrame\n            is missing entries for some classes.\n    \"\"\"\n    # Classes is a 1D array\n    if len(classes.shape) &gt; 1:\n        msg = (\n            \"'classes' should be a 1D array, got an array of shape \"\n            f\"{classes.shape} instead.\"\n        )\n        raise ValueError(msg)\n    # Items appear exactly once\n    if len(classes) != len(set(classes)):\n        msg = (\n            \"All embedding classes must be unique. One or more items in \"\n            \"'classes' appears twice.\"\n        )\n        raise ValueError(msg)\n    # All classes have an entry in weights\n    elif weights is not None:\n        missing = set(classes).difference(weights.index)\n        if len(missing) &gt; 0:\n            msg = (\n                f\"Index of 'weights' DataFrame is missing {len(missing)} \"\n                \"entries compared to classes.\"\n            )\n            raise ValueError(msg)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding.embed","title":"embed","text":"<pre><code>embed(classes)\n</code></pre> <p>Embed transcript classes into numerical representations.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>ArrayLike</code> <p>Array of class names to embed.</p> required <p>Returns:</p> Type Description <p>Union[LongTensor, torch.Tensor]: If no weights provided, returns indices. If weights provided, returns embedded representations.</p> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>def embed(self, classes: ArrayLike):\n    \"\"\"Embed transcript classes into numerical representations.\n\n    Args:\n        classes: Array of class names to embed.\n\n    Returns:\n        Union[LongTensor, torch.Tensor]: If no weights provided, returns indices.\n            If weights provided, returns embedded representations.\n    \"\"\"\n    indices = LongTensor(self._encoder.transform(classes))\n    # Default, one-hot encoding\n    if self._weights is None:\n        return indices  # F.one_hot(indices, len(self._encoder.classes_))\n    else:\n        return F.embedding(indices, self._weights)\n</code></pre>"},{"location":"api/data/transcript_embedding/#overview","title":"Overview","text":"<p>The <code>TranscriptEmbedding</code> class is designed to handle transcript feature encoding in a flexible and extensible way. It supports both simple index-based encoding and weighted embeddings, making it suitable for various machine learning applications.</p>"},{"location":"api/data/transcript_embedding/#usage-examples","title":"Usage Examples","text":""},{"location":"api/data/transcript_embedding/#basic-index-based-encoding","title":"Basic Index-based Encoding","text":"<pre><code>from segger.data.transcript_embedding import TranscriptEmbedding\nimport pandas as pd\n\n# Create a list of gene names\ngene_names = [\"GENE1\", \"GENE2\", \"GENE3\", \"GENE4\"]\n\n# Initialize embedding without weights (index-based)\nembedding = TranscriptEmbedding(classes=gene_names)\n\n# Encode transcript labels\ntranscript_labels = [\"GENE1\", \"GENE3\", \"GENE2\"]\nencoded = embedding.embed(transcript_labels)\n# Returns: tensor([0, 2, 1])\n</code></pre>"},{"location":"api/data/transcript_embedding/#weighted-embeddings","title":"Weighted Embeddings","text":"<pre><code>import pandas as pd\n\n# Create weights DataFrame\nweights_df = pd.DataFrame({\n    'weight1': [0.1, 0.2, 0.3, 0.4],\n    'weight2': [0.5, 0.6, 0.7, 0.8]\n}, index=[\"GENE1\", \"GENE2\", \"GENE3\", \"GENE4\"])\n\n# Initialize embedding with weights\nembedding = TranscriptEmbedding(\n    classes=gene_names,\n    weights=weights_df\n)\n\n# Encode transcript labels\ntranscript_labels = [\"GENE1\", \"GENE3\"]\nencoded = embedding.embed(transcript_labels)\n# Returns: tensor([[0.1, 0.5], [0.3, 0.7]])\n</code></pre>"},{"location":"api/data/transcript_embedding/#integration-with-pytorch","title":"Integration with PyTorch","text":"<pre><code>import torch\nfrom segger.data.transcript_embedding import TranscriptEmbedding\n\n# Create embedding module\nembedding = TranscriptEmbedding(classes=gene_names)\n\n# Use in a neural network\nclass TranscriptEncoder(torch.nn.Module):\n    def __init__(self, gene_names):\n        super().__init__()\n        self.embedding = TranscriptEmbedding(gene_names)\n        self.projection = torch.nn.Linear(len(gene_names), 128)\n\n    def forward(self, transcript_labels):\n        encoded = self.embedding.embed(transcript_labels)\n        projected = self.projection(encoded)\n        return projected\n\n# Initialize and use\nencoder = TranscriptEncoder(gene_names)\noutput = encoder([\"GENE1\", \"GENE2\"])\n</code></pre>"},{"location":"api/data/constants/","title":"segger.data.constants","text":""},{"location":"api/data/constants/#segger.data.constants.MerscopeKeys","title":"MerscopeKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/constants/#segger.data.constants.SpatialDataKeys","title":"SpatialDataKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/constants/#segger.data.constants.SpatialTranscriptomicsKeys","title":"SpatialTranscriptomicsKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Unified keys for spatial transcriptomics data, supporting multiple platforms.</p>"},{"location":"api/data/constants/#segger.data.constants.XeniumKeys","title":"XeniumKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for 10X Genomics Xenium formatted dataset.</p>"},{"location":"api/data/io/","title":"segger.data.io","text":""},{"location":"api/data/io/#segger.data.io.MerscopeKeys","title":"MerscopeKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/io/#segger.data.io.MerscopeSample","title":"MerscopeSample","text":"<pre><code>MerscopeSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(\n        transcripts_df,\n        transcripts_radius,\n        boundaries_graph,\n        embedding_df,\n        MerscopeKeys,\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.MerscopeSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on specific criteria for Merscope using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <p>dd.DataFrame The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <p>float, optional The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame The filtered Dask DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def filter_transcripts(\n    self, transcripts_df: dd.DataFrame, min_qv: float = 20.0\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on specific criteria for Merscope using Dask.\n\n    Parameters:\n        transcripts_df : dd.DataFrame\n            The Dask DataFrame containing transcript data.\n        min_qv : float, optional\n            The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame\n            The filtered Dask DataFrame.\n    \"\"\"\n    # Add custom Merscope-specific filtering logic if needed\n    # For now, apply only the quality value filter\n    return transcripts_df[transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialDataKeys","title":"SpatialDataKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/io/#segger.data.io.SpatialDataSample","title":"SpatialDataSample","text":"<pre><code>SpatialDataSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, feature_name=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    feature_name: str | None = None,\n    verbose: bool = True,\n):\n    if feature_name is not None:\n        # luca: just a quick hack for now, I propose to use dataclasses instead of enums to address this\n        SpatialDataKeys.FEATURE_NAME._value_ = feature_name\n    else:\n        raise ValueError(\n            \"the automatic determination of a feature_name from a SpatialData object is not enabled yet\"\n        )\n\n    super().__init__(\n        transcripts_df,\n        transcripts_radius,\n        boundaries_graph,\n        embedding_df,\n        SpatialDataKeys,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialDataSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def filter_transcripts(\n    self, transcripts_df: dd.DataFrame, min_qv: float = 20.0\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[\n            self.keys.FEATURE_NAME.value\n        ].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(\n        filter_codewords\n    )\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>segger/data/utils.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    transform: Callable = None,\n    pre_transform: Callable = None,\n    pre_filter: Callable = None,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(\n        os.path.join(self.processed_dir, self.processed_file_names[idx])\n    )\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsKeys","title":"SpatialTranscriptomicsKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Unified keys for spatial transcriptomics data, supporting multiple platforms.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample","title":"SpatialTranscriptomicsSample","text":"<pre><code>SpatialTranscriptomicsSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, keys=None, verbose=True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Initialize the SpatialTranscriptomicsSample class.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>A DataFrame containing transcript data.</p> <code>None</code> <code>transcripts_radius</code> <code>int</code> <p>Radius for transcripts in the analysis.</p> <code>10</code> <code>boundaries_graph</code> <code>bool</code> <p>Whether to include boundaries (e.g., nucleus, cell) graph information.</p> <code>False</code> <code>keys</code> <code>Dict</code> <p>The enum class containing key mappings specific to the dataset.</p> <code>None</code> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: pd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    keys: Dict = None,\n    verbose: bool = True,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsSample class.\n\n    Args:\n        transcripts_df (pd.DataFrame, optional): A DataFrame containing transcript data.\n        transcripts_radius (int, optional): Radius for transcripts in the analysis.\n        boundaries_graph (bool, optional): Whether to include boundaries (e.g., nucleus, cell) graph information.\n        keys (Dict, optional): The enum class containing key mappings specific to the dataset.\n    \"\"\"\n    self.transcripts_df = transcripts_df\n    self.transcripts_radius = transcripts_radius\n    self.boundaries_graph = boundaries_graph\n    self.keys = keys\n    self.embedding_df = embedding_df\n    self.current_embedding = \"token\"\n    self.verbose = verbose\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.build_pyg_data_from_tile","title":"build_pyg_data_from_tile","text":"<pre><code>build_pyg_data_from_tile(boundaries_df, transcripts_df, r_tx=5.0, k_tx=3, method='kd_tree', gpu=False, workers=1, scale_boundaries=1.0)\n</code></pre> <p>Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundaries data (e.g., nucleus, cell).</p> required <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcripts data.</p> required <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>HeteroData</code> <code>HeteroData</code> <p>PyG Heterogeneous Data object.</p> Source code in <code>segger/data/io.py</code> <pre><code>def build_pyg_data_from_tile(\n    self,\n    boundaries_df: dd.DataFrame,\n    transcripts_df: dd.DataFrame,\n    r_tx: float = 5.0,\n    k_tx: int = 3,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n    scale_boundaries: float = 1.0,\n) -&gt; HeteroData:\n    \"\"\"\n    Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): Dask DataFrame containing boundaries data (e.g., nucleus, cell).\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcripts data.\n        r_tx (float): Radius for building the transcript-to-transcript graph.\n        k_tx (int): Number of nearest neighbors for the tx-tx graph.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        HeteroData: PyG Heterogeneous Data object.\n    \"\"\"\n    # Initialize the PyG HeteroData object\n    data = HeteroData()\n\n    # Lazily compute boundaries geometries using Dask\n    if self.verbose:\n        print(\"Computing boundaries geometries...\")\n    bd_gdf = self.compute_boundaries_geometries(\n        boundaries_df, scale_factor=scale_boundaries\n    )\n    bd_gdf = bd_gdf[bd_gdf[\"geometry\"].notnull()]\n\n    # Add boundary node data to PyG HeteroData lazily\n    data[\"bd\"].id = bd_gdf[self.keys.CELL_ID.value].values\n    data[\"bd\"].pos = torch.as_tensor(\n        bd_gdf[[\"centroid_x\", \"centroid_y\"]].values.astype(float)\n    )\n\n    if data[\"bd\"].pos.isnan().any():\n        raise ValueError(data[\"bd\"].id[data[\"bd\"].pos.isnan().any(1)])\n\n    bd_x = bd_gdf.iloc[:, 4:]\n    data[\"bd\"].x = torch.as_tensor(bd_x.to_numpy(), dtype=torch.float32)\n\n    # Extract the transcript coordinates lazily\n    if self.verbose:\n        print(\"Preparing transcript features and positions...\")\n    x_xyz = transcripts_df[\n        [self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]\n    ].to_numpy()\n    data[\"tx\"].id = torch.as_tensor(\n        transcripts_df[self.keys.TRANSCRIPTS_ID.value].values.astype(int)\n    )\n    data[\"tx\"].pos = torch.tensor(x_xyz, dtype=torch.float32)\n\n    # Lazily prepare transcript embeddings (if available)\n    if self.verbose:\n        print(\"Preparing transcript embeddings..\")\n    token_encoding = self.tx_encoder.transform(\n        transcripts_df[self.keys.FEATURE_NAME.value]\n    )\n    transcripts_df[\"token\"] = (\n        token_encoding  # Store the integer tokens in the 'token' column\n    )\n    data[\"tx\"].token = torch.as_tensor(token_encoding).int()\n    # Handle additional embeddings lazily as well\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        embeddings = delayed(\n            lambda df: self.embedding_df.loc[\n                df[self.keys.FEATURE_NAME.value].values\n            ].values\n        )(transcripts_df)\n    else:\n        embeddings = token_encoding\n    if hasattr(embeddings, \"compute\"):\n        embeddings = embeddings.compute()\n    x_features = torch.as_tensor(embeddings).int()\n    data[\"tx\"].x = x_features\n\n    # Check if the overlap column exists, if not, compute it lazily using Dask\n    if self.keys.OVERLAPS_BOUNDARY.value not in transcripts_df.columns:\n        if self.verbose:\n            print(f\"Computing overlaps for transcripts...\")\n        transcripts_df = self.compute_transcript_overlap_with_boundaries(\n            transcripts_df, polygons_gdf=bd_gdf, scale_factor=1.0\n        )\n\n    # Connect transcripts with their corresponding boundaries (e.g., nuclei, cells)\n    if self.verbose:\n        print(\"Connecting transcripts with boundaries...\")\n    overlaps = transcripts_df[self.keys.OVERLAPS_BOUNDARY.value].values\n    valid_cell_ids = bd_gdf[self.keys.CELL_ID.value].values\n    ind = np.where(\n        overlaps &amp; transcripts_df[self.keys.CELL_ID.value].isin(valid_cell_ids)\n    )[0]\n    tx_bd_edge_index = np.column_stack(\n        (\n            ind,\n            np.searchsorted(\n                valid_cell_ids, transcripts_df.iloc[ind][self.keys.CELL_ID.value]\n            ),\n        )\n    )\n\n    # Add transcript-boundary edge index to PyG HeteroData\n    data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.as_tensor(\n        tx_bd_edge_index.T, dtype=torch.long\n    )\n\n    # Compute transcript-to-transcript (tx-tx) edges using Dask (lazy computation)\n    if self.verbose:\n        print(\"Computing tx-tx edges...\")\n    tx_positions = transcripts_df[\n        [self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]\n    ].values\n    delayed_tx_edge_index = delayed(get_edge_index)(\n        tx_positions,\n        tx_positions,\n        k=k_tx,\n        dist=r_tx,\n        method=method,\n        gpu=gpu,\n        workers=workers,\n    )\n    tx_edge_index = delayed_tx_edge_index.compute()\n\n    # Add the tx-tx edge index to the PyG HeteroData object\n    data[\"tx\", \"neighbors\", \"tx\"].edge_index = torch.as_tensor(\n        tx_edge_index.T, dtype=torch.long\n    )\n\n    if self.verbose:\n        print(\"Finished building PyG data for the tile.\")\n    return data\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.compute_boundaries_geometries","title":"compute_boundaries_geometries","text":"<pre><code>compute_boundaries_geometries(boundaries_df=None, polygons_gdf=None, scale_factor=1.0, area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>The dataframe containing boundaries data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <code>area</code> <code>bool</code> <p>Whether to compute area.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>Whether to compute convexity.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>Whether to compute elongation.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>Whether to compute circularity.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.</p> Source code in <code>segger/data/io.py</code> <pre><code>def compute_boundaries_geometries(\n    self,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.\n\n    Parameters:\n        boundaries_df (dd.DataFrame, optional): The dataframe containing boundaries data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n        area (bool, optional): Whether to compute area.\n        convexity (bool, optional): Whether to compute convexity.\n        elongation (bool, optional): Whether to compute elongation.\n        circularity (bool, optional): Whether to compute circularity.\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\n                \"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\"\n            )\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        if self.verbose:\n            print(\n                f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\"\n            )\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    # Check if the generated polygons_gdf is empty\n    if polygons_gdf.shape[0] == 0:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(\n                f\"Polygons are available. Proceeding with geometrical computations.\"\n            )\n\n    # Compute additional geometrical properties\n    polygons = polygons_gdf.geometry\n\n    # Compute additional geometrical properties\n    if area:\n        if self.verbose:\n            print(\"Computing area...\")\n        polygons_gdf[\"area\"] = polygons.area\n    if convexity:\n        if self.verbose:\n            print(\"Computing convexity...\")\n        polygons_gdf[\"convexity\"] = polygons.convex_hull.area / polygons.area\n    if elongation:\n        if self.verbose:\n            print(\"Computing elongation...\")\n        r = polygons.minimum_rotated_rectangle()\n        polygons_gdf[\"elongation\"] = (r.length * r.length) / r.area\n    if circularity:\n        if self.verbose:\n            print(\"Computing circularity...\")\n        r = polygons_gdf.minimum_bounding_radius()\n        polygons_gdf[\"circularity\"] = polygons.area / (r * r)\n\n    if self.verbose:\n        print(\"Geometrical computations completed.\")\n\n    return polygons_gdf.reset_index(drop=True)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.compute_transcript_overlap_with_boundaries","title":"compute_transcript_overlap_with_boundaries","text":"<pre><code>compute_transcript_overlap_with_boundaries(transcripts_df, boundaries_df=None, polygons_gdf=None, scale_factor=1.0)\n</code></pre> <p>Computes the overlap of transcript locations with scaled boundary polygons and assigns corresponding cell IDs to the transcripts using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcript data.</p> required <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.</p> Source code in <code>segger/data/io.py</code> <pre><code>def compute_transcript_overlap_with_boundaries(\n    self,\n    transcripts_df: dd.DataFrame,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Computes the overlap of transcript locations with scaled boundary polygons\n    and assigns corresponding cell IDs to the transcripts using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcript data.\n        boundaries_df (dd.DataFrame, optional): Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\n                \"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\"\n            )\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    if polygons_gdf.empty:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(f\"Polygons are available. Proceeding with overlap computation.\")\n\n    # Create a delayed function to check if a point is within any polygon\n    def check_overlap(transcript, polygons_gdf):\n        x = transcript[self.keys.TRANSCRIPTS_X.value]\n        y = transcript[self.keys.TRANSCRIPTS_Y.value]\n        point = Point(x, y)\n\n        overlap = False\n        cell_id = None\n\n        # Check for point containment lazily within polygons\n        for _, polygon in polygons_gdf.iterrows():\n            if polygon.geometry.contains(point):\n                overlap = True\n                cell_id = polygon[self.keys.CELL_ID.value]\n                break\n\n        return overlap, cell_id\n\n    # Apply the check_overlap function in parallel to each row using Dask's map_partitions\n    if self.verbose:\n        print(\n            f\"Starting overlap computation for transcripts with the boundary polygons.\"\n        )\n    if isinstance(transcripts_df, pd.DataFrame):\n        # luca: I found this bug here\n        warnings.warn(\n            \"BUG! This function expects Dask DataFrames, not Pandas DataFrames.\"\n        )\n        # if we want to really have the below working in parallel, we need to add n_partitions&gt;1 here\n        transcripts_df = dd.from_pandas(transcripts_df, npartitions=1)\n        transcripts_df.compute().columns\n    transcripts_df = transcripts_df.map_partitions(\n        lambda df: df.assign(\n            **{\n                self.keys.OVERLAPS_BOUNDARY.value: df.apply(\n                    lambda row: delayed(check_overlap)(row, polygons_gdf)[0], axis=1\n                ),\n                self.keys.CELL_ID.value: df.apply(\n                    lambda row: delayed(check_overlap)(row, polygons_gdf)[1], axis=1\n                ),\n            }\n        )\n    )\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.create_scaled_polygon","title":"create_scaled_polygon  <code>staticmethod</code>","text":"<pre><code>create_scaled_polygon(group, scale_factor, keys)\n</code></pre> <p>Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>Group of boundary coordinates (for a specific cell).</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons.</p> required <code>keys</code> <code>Dict or Enum</code> <p>A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.</p> Source code in <code>segger/data/io.py</code> <pre><code>@staticmethod\ndef create_scaled_polygon(\n    group: pd.DataFrame, scale_factor: float, keys\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.\n\n    Parameters:\n        group (pd.DataFrame): Group of boundary coordinates (for a specific cell).\n        scale_factor (float): The factor by which to scale the polygons.\n        keys (Dict or Enum): A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.\n    \"\"\"\n    # Extract coordinates and cell ID from the group using keys\n    x_coords = group[keys[\"vertex_x\"]]\n    y_coords = group[keys[\"vertex_y\"]]\n    cell_id = group[keys[\"cell_id\"]].iloc[0]\n\n    # Ensure there are at least 3 points to form a polygon\n    if len(x_coords) &gt;= 3:\n\n        polygon = Polygon(zip(x_coords, y_coords))\n        if polygon.is_valid and not polygon.is_empty:\n            # Scale the polygon by the provided factor\n            scaled_polygon = polygon.buffer(scale_factor)\n            if scaled_polygon.is_valid and not scaled_polygon.is_empty:\n                return gpd.GeoDataFrame(\n                    {\"geometry\": [scaled_polygon], keys[\"cell_id\"]: [cell_id]},\n                    geometry=\"geometry\",\n                    crs=\"EPSG:4326\",\n                )\n    # Return an empty GeoDataFrame if no valid polygon is created\n    return gpd.GeoDataFrame(\n        {\"geometry\": [None], keys[\"cell_id\"]: [cell_id]},\n        geometry=\"geometry\",\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.filter_transcripts","title":"filter_transcripts  <code>abstractmethod</code>","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Abstract method to filter transcripts based on dataset-specific criteria.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>segger/data/io.py</code> <pre><code>@abstractmethod\ndef filter_transcripts(\n    self, transcripts_df: pd.DataFrame, min_qv: float = 20.0\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Abstract method to filter transcripts based on dataset-specific criteria.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.generate_and_scale_polygons","title":"generate_and_scale_polygons","text":"<pre><code>generate_and_scale_polygons(boundaries_df, scale_factor=1.0)\n</code></pre> <p>Generate and scale polygons from boundary coordinates using Dask. Keeps class structure intact by using static method for the core polygon generation.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>DataFrame containing boundary coordinates.</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.</p> Source code in <code>segger/data/io.py</code> <pre><code>def generate_and_scale_polygons(\n    self, boundaries_df: dd.DataFrame, scale_factor: float = 1.0\n) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Generate and scale polygons from boundary coordinates using Dask.\n    Keeps class structure intact by using static method for the core polygon generation.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): DataFrame containing boundary coordinates.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.\n    \"\"\"\n    # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n\n    # Extract required columns from self.keys\n    cell_id_column = self.keys.CELL_ID.value\n    vertex_x_column = self.keys.BOUNDARIES_VERTEX_X.value\n    vertex_y_column = self.keys.BOUNDARIES_VERTEX_Y.value\n\n    create_polygon = self.create_scaled_polygon\n    # Use a lambda to wrap the static method call and avoid passing the function object directly to Dask\n    polygons_ddf = boundaries_df.groupby(cell_id_column).apply(\n        lambda group: create_polygon(\n            group=group,\n            scale_factor=scale_factor,\n            keys={  # Pass keys as a dict for the lambda function\n                \"vertex_x\": vertex_x_column,\n                \"vertex_y\": vertex_y_column,\n                \"cell_id\": cell_id_column,\n            },\n        )\n    )\n\n    # Lazily compute centroids for each polygon\n    if self.verbose:\n        print(\"Adding centroids to the polygons...\")\n    polygons_ddf[\"centroid_x\"] = polygons_ddf.geometry.centroid.x\n    polygons_ddf[\"centroid_y\"] = polygons_ddf.geometry.centroid.y\n\n    polygons_ddf = polygons_ddf.drop_duplicates()\n    # polygons_ddf = polygons_ddf.to_crs(\"EPSG:3857\")\n\n    return polygons_ddf\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.load_boundaries","title":"load_boundaries","text":"<pre><code>load_boundaries(path, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load boundaries data lazily using Dask, filtering by the specified bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the boundaries file.</p> required <code>file_format</code> <code>str</code> <p>Format of the file to load. Only 'parquet' is supported in this refactor.</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered boundaries DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def load_boundaries(\n    self,\n    path: Path,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load boundaries data lazily using Dask, filtering by the specified bounding box.\n\n    Parameters:\n        path (Path): Path to the boundaries file.\n        file_format (str, optional): Format of the file to load. Only 'parquet' is supported in this refactor.\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered boundaries DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(f\"Unsupported file format: {file_format}\")\n\n    self.boundaries_path = path\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.BOUNDARIES_VERTEX_X.value,\n        self.keys.BOUNDARIES_VERTEX_Y.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&gt;=\", x_min),\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&lt;=\", x_max),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&gt;=\", y_min),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(path).columns)\n    if \"geometry\" in columns:\n        bbox = (x_min, y_min, x_max, y_max)\n        # TODO: check that SpatialData objects write the \"bbox covering metadata\" to the parquet file\n        gdf = dgpd.read_parquet(path, bbox=bbox)\n        id_col, x_col, y_col = (\n            self.keys.CELL_ID.value,\n            self.keys.BOUNDARIES_VERTEX_X.value,\n            self.keys.BOUNDARIES_VERTEX_Y.value,\n        )\n\n        # Function to expand each polygon into a list of vertices\n        def expand_polygon(row):\n            expanded_data = []\n            polygon = row[\"geometry\"]\n            if polygon.geom_type == \"Polygon\":\n                exterior_coords = polygon.exterior.coords\n                for x, y in exterior_coords:\n                    expanded_data.append({id_col: row.name, x_col: x, y_col: y})\n            else:\n                # Instead of expanding the gdf and then having code later to recreate it (when computing the pyg graph)\n                # we could directly have this function returning a Dask GeoDataFrame. This means that we don't need\n                # to implement this else black\n                raise ValueError(f\"Unsupported geometry type: {polygon.geom_type}\")\n            return expanded_data\n\n        # Apply the function to each partition and collect results\n        def process_partition(df):\n            expanded_data = [expand_polygon(row) for _, row in df.iterrows()]\n            # Flatten the list of lists\n            flattened_data = [item for sublist in expanded_data for item in sublist]\n            return pd.DataFrame(flattened_data)\n\n        # Use map_partitions to apply the function and convert it into a Dask DataFrame\n        boundaries_df = gdf.map_partitions(\n            process_partition, meta={id_col: str, x_col: float, y_col: float}\n        )\n    else:\n        boundaries_df = dd.read_parquet(\n            path, columns=columns_to_read, filters=filters\n        )\n\n        # Convert the cell IDs to strings lazily\n        boundaries_df[self.keys.CELL_ID.value] = boundaries_df[\n            self.keys.CELL_ID.value\n        ].apply(\n            lambda x: str(x) if pd.notnull(x) else None, meta=(\"cell_id\", \"object\")\n        )\n\n    if self.verbose:\n        print(\n            f\"Loaded boundaries from '{path}' within bounding box ({x_min}, {x_max}, {y_min}, {y_max}).\"\n        )\n\n    return boundaries_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.load_transcripts","title":"load_transcripts","text":"<pre><code>load_transcripts(base_path=None, sample=None, transcripts_filename=None, path=None, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load transcripts from a Parquet file using Dask for efficient chunked processing, only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>The base directory path where samples are stored.</p> <code>None</code> <code>sample</code> <code>str</code> <p>The sample name or identifier.</p> <code>None</code> <code>transcripts_filename</code> <code>str</code> <p>The filename of the transcripts file (default is derived from the dataset keys).</p> <code>None</code> <code>path</code> <code>Path</code> <p>Specific path to the transcripts file.</p> <code>None</code> <code>file_format</code> <code>str</code> <p>Format of the file to load (default is 'parquet').</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered transcripts DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def load_transcripts(\n    self,\n    base_path: Path = None,\n    sample: str = None,\n    transcripts_filename: str = None,\n    path: Path = None,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n    # additional_embeddings: Optional[Dict[str, pd.DataFrame]] = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load transcripts from a Parquet file using Dask for efficient chunked processing,\n    only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.\n\n    Parameters:\n        base_path (Path, optional): The base directory path where samples are stored.\n        sample (str, optional): The sample name or identifier.\n        transcripts_filename (str, optional): The filename of the transcripts file (default is derived from the dataset keys).\n        path (Path, optional): Specific path to the transcripts file.\n        file_format (str, optional): Format of the file to load (default is 'parquet').\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered transcripts DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(\"This version only supports parquet files with Dask.\")\n\n    # Set the file path for transcripts\n    transcripts_filename = transcripts_filename or self.keys.TRANSCRIPTS_FILE.value\n    file_path = path or (base_path / sample / transcripts_filename)\n    self.transcripts_path = file_path\n\n    # Set metadata\n    # self.set_metadata()\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Check for available columns in the file's metadata (without loading the data)\n    parquet_metadata = dd.read_parquet(file_path, meta_only=True)\n    available_columns = parquet_metadata.columns\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.TRANSCRIPTS_ID.value,\n        self.keys.TRANSCRIPTS_X.value,\n        self.keys.TRANSCRIPTS_Y.value,\n        self.keys.FEATURE_NAME.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Check if the QUALITY_VALUE key exists in the dataset, and add it to the columns list if present\n    if self.keys.QUALITY_VALUE.value in available_columns:\n        columns_to_read.append(self.keys.QUALITY_VALUE.value)\n\n    if self.keys.OVERLAPS_BOUNDARY.value in available_columns:\n        columns_to_read.append(self.keys.OVERLAPS_BOUNDARY.value)\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.TRANSCRIPTS_X.value, \"&gt;=\", x_min),\n        (self.keys.TRANSCRIPTS_X.value, \"&lt;=\", x_max),\n        (self.keys.TRANSCRIPTS_Y.value, \"&gt;=\", y_min),\n        (self.keys.TRANSCRIPTS_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(file_path).columns)\n    transcripts_df = dd.read_parquet(\n        file_path, columns=columns_to_read, filters=filters\n    ).compute()\n\n    # Convert transcript and cell IDs to strings lazily\n    transcripts_df[self.keys.TRANSCRIPTS_ID.value] = transcripts_df[\n        self.keys.TRANSCRIPTS_ID.value\n    ].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n    transcripts_df[self.keys.CELL_ID.value] = transcripts_df[\n        self.keys.CELL_ID.value\n    ].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n\n    # Convert feature names from bytes to strings if necessary\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[\n            self.keys.FEATURE_NAME.value\n        ].astype(str)\n\n    # Apply dataset-specific filtering (e.g., quality filtering for Xenium)\n    transcripts_df = self.filter_transcripts(transcripts_df)\n\n    # Handle additional embeddings if provided\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        valid_genes = self.embedding_df.index\n        # Lazily count the number of rows in the DataFrame before filtering\n        initial_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        # Filter the DataFrame lazily based on valid genes from embeddings\n        transcripts_df = transcripts_df[\n            transcripts_df[self.keys.FEATURE_NAME.value].isin(valid_genes)\n        ]\n        final_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        if self.verbose:\n            print(\n                f\"Dropped {initial_count - final_count} transcripts not found in embedding.\"\n            )\n\n    # Ensure that the 'OVERLAPS_BOUNDARY' column is boolean if it exists\n    if self.keys.OVERLAPS_BOUNDARY.value in transcripts_df.columns:\n        transcripts_df[self.keys.OVERLAPS_BOUNDARY.value] = transcripts_df[\n            self.keys.OVERLAPS_BOUNDARY.value\n        ].astype(bool)\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.save_dataset_for_segger","title":"save_dataset_for_segger","text":"<pre><code>save_dataset_for_segger(processed_dir, x_size=1000, y_size=1000, d_x=900, d_y=900, margin_x=None, margin_y=None, compute_labels=True, r_tx=5, k_tx=3, val_prob=0.1, test_prob=0.2, neg_sampling_ratio_approx=5, sampling_rate=1, num_workers=1, scale_boundaries=1.0, method='kd_tree', gpu=False, workers=1)\n</code></pre> <p>Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.</p> <p>Parameters:</p> Name Type Description Default <code>processed_dir</code> <code>Path</code> <p>Directory to save the processed dataset.</p> required <code>x_size</code> <code>float</code> <p>Width of each tile.</p> <code>1000</code> <code>y_size</code> <code>float</code> <p>Height of each tile.</p> <code>1000</code> <code>d_x</code> <code>float</code> <p>Step size in the x direction for tiles.</p> <code>900</code> <code>d_y</code> <code>float</code> <p>Step size in the y direction for tiles.</p> <code>900</code> <code>margin_x</code> <code>float</code> <p>Margin in the x direction to include transcripts.</p> <code>None</code> <code>margin_y</code> <code>float</code> <p>Margin in the y direction to include transcripts.</p> <code>None</code> <code>compute_labels</code> <code>bool</code> <p>Whether to compute edge labels for tx_belongs_bd edges.</p> <code>True</code> <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>val_prob</code> <code>float</code> <p>Probability of assigning a tile to the validation set.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Probability of assigning a tile to the test set.</p> <code>0.2</code> <code>neg_sampling_ratio_approx</code> <code>float</code> <p>Approximate ratio of negative samples.</p> <code>5</code> <code>sampling_rate</code> <code>float</code> <p>Rate of sampling tiles.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use to compute the neighborhood graph (per tile).</p> <code>1</code> Source code in <code>segger/data/io.py</code> <pre><code>def save_dataset_for_segger(\n    self,\n    processed_dir: Path,\n    x_size: float = 1000,\n    y_size: float = 1000,\n    d_x: float = 900,\n    d_y: float = 900,\n    margin_x: float = None,\n    margin_y: float = None,\n    compute_labels: bool = True,\n    r_tx: float = 5,\n    k_tx: int = 3,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n    neg_sampling_ratio_approx: float = 5,\n    sampling_rate: float = 1,\n    num_workers: int = 1,\n    scale_boundaries: float = 1.0,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n) -&gt; None:\n    \"\"\"\n    Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.\n\n    Parameters:\n        processed_dir (Path): Directory to save the processed dataset.\n        x_size (float, optional): Width of each tile.\n        y_size (float, optional): Height of each tile.\n        d_x (float, optional): Step size in the x direction for tiles.\n        d_y (float, optional): Step size in the y direction for tiles.\n        margin_x (float, optional): Margin in the x direction to include transcripts.\n        margin_y (float, optional): Margin in the y direction to include transcripts.\n        compute_labels (bool, optional): Whether to compute edge labels for tx_belongs_bd edges.\n        r_tx (float, optional): Radius for building the transcript-to-transcript graph.\n        k_tx (int, optional): Number of nearest neighbors for the tx-tx graph.\n        val_prob (float, optional): Probability of assigning a tile to the validation set.\n        test_prob (float, optional): Probability of assigning a tile to the test set.\n        neg_sampling_ratio_approx (float, optional): Approximate ratio of negative samples.\n        sampling_rate (float, optional): Rate of sampling tiles.\n        num_workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use to compute the neighborhood graph (per tile).\n\n    \"\"\"\n    # Prepare directories for storing processed tiles\n    self._prepare_directories(processed_dir)\n\n    # Get x and y coordinate ranges for tiling\n    x_range, y_range = self._get_ranges(d_x, d_y)\n\n    # Generate parameters for each tile\n    tile_params = self._generate_tile_params(\n        x_range,\n        y_range,\n        x_size,\n        y_size,\n        margin_x,\n        margin_y,\n        compute_labels,\n        r_tx,\n        k_tx,\n        val_prob,\n        test_prob,\n        neg_sampling_ratio_approx,\n        sampling_rate,\n        processed_dir,\n        scale_boundaries,\n        method,\n        gpu,\n        workers,\n    )\n\n    # Process each tile using Dask to parallelize the task\n    if self.verbose:\n        print(\"Starting tile processing...\")\n    tasks = [delayed(self._process_tile)(params) for params in tile_params]\n\n    with ProgressBar():\n        # Use Dask to process all tiles in parallel\n        dask.compute(*tasks, num_workers=num_workers)\n    if self.verbose:\n        print(\"Tile processing completed.\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_embedding","title":"set_embedding","text":"<pre><code>set_embedding(embedding_name)\n</code></pre> <p>Set the current embedding type for the transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_name</code> <p>str The name of the embedding to use.</p> required Source code in <code>segger/data/io.py</code> <pre><code>def set_embedding(self, embedding_name: str) -&gt; None:\n    \"\"\"\n    Set the current embedding type for the transcripts.\n\n    Parameters:\n        embedding_name : str\n            The name of the embedding to use.\n\n    \"\"\"\n    if embedding_name in self.embeddings_dict:\n        self.current_embedding = embedding_name\n    else:\n        raise ValueError(\n            f\"Embedding {embedding_name} not found in embeddings_dict.\"\n        )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_file_paths","title":"set_file_paths","text":"<pre><code>set_file_paths(transcripts_path, boundaries_path)\n</code></pre> <p>Set the paths for the transcript and boundary files.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_path</code> <code>Path</code> <p>Path to the Parquet file containing transcripts data.</p> required <code>boundaries_path</code> <code>Path</code> <p>Path to the Parquet file containing boundaries data.</p> required Source code in <code>segger/data/io.py</code> <pre><code>def set_file_paths(self, transcripts_path: Path, boundaries_path: Path) -&gt; None:\n    \"\"\"\n    Set the paths for the transcript and boundary files.\n\n    Parameters:\n        transcripts_path (Path): Path to the Parquet file containing transcripts data.\n        boundaries_path (Path): Path to the Parquet file containing boundaries data.\n    \"\"\"\n    self.transcripts_path = transcripts_path\n    self.boundaries_path = boundaries_path\n\n    if self.verbose:\n        print(f\"Set transcripts file path to {transcripts_path}\")\n    if self.verbose:\n        print(f\"Set boundaries file path to {boundaries_path}\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_metadata","title":"set_metadata","text":"<pre><code>set_metadata()\n</code></pre> <p>Set metadata for the transcript dataset, including bounding box limits and unique gene names, without reading the entire Parquet file. Additionally, return integer tokens for unique gene names instead of one-hot encodings and store the lookup table for later mapping.</p> Source code in <code>segger/data/io.py</code> <pre><code>def set_metadata(self) -&gt; None:\n    \"\"\"\n    Set metadata for the transcript dataset, including bounding box limits and unique gene names,\n    without reading the entire Parquet file. Additionally, return integer tokens for unique gene names\n    instead of one-hot encodings and store the lookup table for later mapping.\n    \"\"\"\n    # Load the Parquet file metadata\n    parquet_file = pq.read_table(self.transcripts_path)\n\n    # Get the column names for X, Y, and feature names from the class's keys\n    x_col = self.keys.TRANSCRIPTS_X.value\n    y_col = self.keys.TRANSCRIPTS_Y.value\n    feature_col = self.keys.FEATURE_NAME.value\n\n    # Initialize variables to track min/max values for X and Y\n    x_min, x_max, y_min, y_max = (\n        float(\"inf\"),\n        float(\"-inf\"),\n        float(\"inf\"),\n        float(\"-inf\"),\n    )\n\n    # Extract unique gene names and ensure they're strings\n    gene_set = set()\n\n    # Define the filter for unwanted codewords\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    row_group_size = 4_000_000\n    start = 0\n    n = len(parquet_file)\n    while start &lt; n:\n        chunk = parquet_file.slice(start, start + row_group_size)\n        start += row_group_size\n\n        # Update the bounding box values (min/max)\n        x_values = chunk[x_col].to_pandas()\n        y_values = chunk[y_col].to_pandas()\n\n        x_min = min(x_min, x_values.min())\n        x_max = max(x_max, x_values.max())\n        y_min = min(y_min, y_values.min())\n        y_max = max(y_max, y_values.max())\n\n        # Convert feature values (gene names) to strings and filter out unwanted codewords\n        feature_values = (\n            chunk[feature_col]\n            .to_pandas()\n            .apply(\n                lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else str(x),\n            )\n        )\n\n        # Filter out unwanted codewords\n        filtered_genes = feature_values[\n            ~feature_values.str.startswith(filter_codewords)\n        ]\n\n        # Update the unique gene set\n        gene_set.update(filtered_genes.unique())\n\n    # Set bounding box limits\n    self.x_min = x_min\n    self.x_max = x_max\n    self.y_min = y_min\n    self.y_max = y_max\n\n    if self.verbose:\n        print(\n            f\"Bounding box limits set: x_min={self.x_min}, x_max={self.x_max}, y_min={self.y_min}, y_max={self.y_max}\"\n        )\n\n    # Convert the set of unique genes into a sorted list for consistent ordering\n    self.unique_genes = sorted(gene_set)\n    if self.verbose:\n        print(\n            f\"Extracted {len(self.unique_genes)} unique gene names for integer tokenization.\"\n        )\n\n    # Initialize a LabelEncoder to convert unique genes into integer tokens\n    self.tx_encoder = LabelEncoder()\n\n    # Fit the LabelEncoder on the unique genes\n    self.tx_encoder.fit(self.unique_genes)\n\n    # Store the integer tokens mapping to gene names\n    self.gene_to_token_map = dict(\n        zip(\n            self.tx_encoder.classes_,\n            self.tx_encoder.transform(self.tx_encoder.classes_),\n        )\n    )\n\n    if self.verbose:\n        print(\n            \"Integer tokens have been computed and stored based on unique gene names.\"\n        )\n\n    # Optional: Create a reverse mapping for lookup purposes (token to gene)\n    self.token_to_gene_map = {v: k for k, v in self.gene_to_token_map.items()}\n\n    if self.verbose:\n        print(\n            \"Lookup tables (gene_to_token_map and token_to_gene_map) have been created.\"\n        )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.XeniumKeys","title":"XeniumKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for 10X Genomics Xenium formatted dataset.</p>"},{"location":"api/data/io/#segger.data.io.XeniumSample","title":"XeniumSample","text":"<pre><code>XeniumSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(\n        transcripts_df,\n        transcripts_radius,\n        boundaries_graph,\n        embedding_df,\n        XeniumKeys,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.XeniumSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def filter_transcripts(\n    self, transcripts_df: dd.DataFrame, min_qv: float = 20.0\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[\n            self.keys.FEATURE_NAME.value\n        ].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(\n        filter_codewords\n    )\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(\n    adata: ad.AnnData, celltype_column: str\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(\n        gene_expression_binary, index=adata.obs_names, columns=adata.var_names\n    )\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[\n        assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_cytoplasmic = (\n        len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    )\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[\n        non_assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_non_assigned_cytoplasmic = (\n        len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    )\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (\n        gene_group_assigned.size() / (gene_group_all.size() + 1) * 100\n    ).reset_index(names=\"percent_assigned\")\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (\n        cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100\n    ).reset_index(name=\"percent_cytoplasmic\")\n    gene_metrics = pd.merge(\n        gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\"\n    ).fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/io/#segger.data.io.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(\n        columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"}\n    )[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(\n            cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\"\n        )\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = (\n        df[df[cell_id_col].astype(str) == \"UNASSIGNED\"]\n        .groupby(\"feature_name\")[\"feature_name\"]\n        .count()\n    )\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = (\n        var_df.index.map(unassigned_counts).fillna(0).astype(int)\n    )\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(\n        adata.obs,\n        cell_summary.loc[adata.obs_names, :],\n        left_index=True,\n        right_index=True,\n    )\n\n    return adata\n</code></pre>"},{"location":"api/data/io/#segger.data.io.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/io/#segger.data.io.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/io/#segger.data.io.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(\n            coords_1, coords_2, k=k, dist=dist, workers=workers\n        )\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(\n        coords_2, k=k, distance_upper_bound=dist, workers=workers\n    )\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(\n                np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T\n            )\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/io/#segger.data.io.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/io/#segger.data.io.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/data/utils/","title":"segger.data.utils","text":""},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>segger/data/utils.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    transform: Callable = None,\n    pre_transform: Callable = None,\n    pre_filter: Callable = None,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(\n        os.path.join(self.processed_dir, self.processed_file_names[idx])\n    )\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(\n    adata: ad.AnnData, celltype_column: str\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(\n        gene_expression_binary, index=adata.obs_names, columns=adata.var_names\n    )\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[\n        assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_cytoplasmic = (\n        len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    )\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[\n        non_assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_non_assigned_cytoplasmic = (\n        len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    )\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (\n        gene_group_assigned.size() / (gene_group_all.size() + 1) * 100\n    ).reset_index(names=\"percent_assigned\")\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (\n        cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100\n    ).reset_index(name=\"percent_cytoplasmic\")\n    gene_metrics = pd.merge(\n        gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\"\n    ).fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(\n        columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"}\n    )[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(\n            cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\"\n        )\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = (\n        df[df[cell_id_col].astype(str) == \"UNASSIGNED\"]\n        .groupby(\"feature_name\")[\"feature_name\"]\n        .count()\n    )\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = (\n        var_df.index.map(unassigned_counts).fillna(0).astype(int)\n    )\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(\n        adata.obs,\n        cell_summary.loc[adata.obs_names, :],\n        left_index=True,\n        right_index=True,\n    )\n\n    return adata\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/utils/#segger.data.utils.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/utils/#segger.data.utils.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(\n            coords_1, coords_2, k=k, dist=dist, workers=workers\n        )\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(\n        coords_2, k=k, distance_upper_bound=dist, workers=workers\n    )\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(\n                np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T\n            )\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/models/","title":"segger.models","text":"<p>Models module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/prediction/","title":"segger.prediction","text":"<p>prediction module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/training/","title":"segger.training","text":"<p>training module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/validation/","title":"segger.validation","text":"<p>This module handles validation utilities for the Segger tool.</p>"},{"location":"api/validation/#submodules","title":"Submodules","text":"<ul> <li>Utils</li> <li>Xenium Explorer</li> </ul>"},{"location":"api/validation/#api-documentation","title":"API Documentation","text":""},{"location":"notebooks/benchmark_bc/","title":"Benchmark bc","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport scanpy as sc\nimport numpy as np\nfrom typing import Dict\nfrom segger.validation.utils import *\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from pathlib import Path import scanpy as sc import numpy as np from typing import Dict from segger.validation.utils import * In\u00a0[\u00a0]: Copied! <pre># Define paths and output directories\nbenchmarks_path = Path(\n    \"/dkfz/cluster/gpu/data/OE0606/elihei/segger_experiments/data_tidy/benchmarks/xe_rep1_bc\"\n)\noutput_path = benchmarks_path / \"results+\"\nfigures_path = output_path / \"figures\"\nfigures_path.mkdir(parents=True, exist_ok=True)  # Ensure the figures directory exists\n</pre> # Define paths and output directories benchmarks_path = Path(     \"/dkfz/cluster/gpu/data/OE0606/elihei/segger_experiments/data_tidy/benchmarks/xe_rep1_bc\" ) output_path = benchmarks_path / \"results+\" figures_path = output_path / \"figures\" figures_path.mkdir(parents=True, exist_ok=True)  # Ensure the figures directory exists In\u00a0[\u00a0]: Copied! <pre># Define colors for segmentation methods\nmethod_colors = {\n    \"segger\": \"#D55E00\",\n    \"segger_n0\": \"#E69F00\",\n    \"segger_n1\": \"#F0E442\",\n    \"segger_embedding\": \"#C72228\",\n    \"Baysor\": \"#000075\",\n    \"Baysor_n0\": \"#0F4A9C\",\n    \"Baysor_n1\": \"#0072B2\",\n    \"10X\": \"#8B008B\",\n    \"10X-nucleus\": \"#CC79A7\",\n    # 'BIDCell': '#009E73'\n}\n</pre> # Define colors for segmentation methods method_colors = {     \"segger\": \"#D55E00\",     \"segger_n0\": \"#E69F00\",     \"segger_n1\": \"#F0E442\",     \"segger_embedding\": \"#C72228\",     \"Baysor\": \"#000075\",     \"Baysor_n0\": \"#0F4A9C\",     \"Baysor_n1\": \"#0072B2\",     \"10X\": \"#8B008B\",     \"10X-nucleus\": \"#CC79A7\",     # 'BIDCell': '#009E73' } In\u00a0[\u00a0]: Copied! <pre># Define colors for cell types\nmajor_colors = {\n    \"B-cells\": \"#d8f55e\",\n    \"CAFs\": \"#532C8A\",\n    \"Cancer Epithelial\": \"#C72228\",\n    \"Endothelial\": \"#9e6762\",\n    \"Myeloid\": \"#ffe012\",\n    \"T-cells\": \"#3cb44b\",\n    \"Normal Epithelial\": \"#0F4A9C\",\n    \"PVL\": \"#c09d9a\",\n    \"Plasmablasts\": \"#000075\",\n}\n</pre> # Define colors for cell types major_colors = {     \"B-cells\": \"#d8f55e\",     \"CAFs\": \"#532C8A\",     \"Cancer Epithelial\": \"#C72228\",     \"Endothelial\": \"#9e6762\",     \"Myeloid\": \"#ffe012\",     \"T-cells\": \"#3cb44b\",     \"Normal Epithelial\": \"#0F4A9C\",     \"PVL\": \"#c09d9a\",     \"Plasmablasts\": \"#000075\", } In\u00a0[\u00a0]: Copied! <pre># Define segmentation file paths\nsegmentation_paths = {\n    \"segger\": benchmarks_path / \"adata_segger.h5ad\",\n    \"Baysor\": benchmarks_path / \"adata_baysor.h5ad\",\n    \"10X\": benchmarks_path / \"adata_10X.h5ad\",\n    \"10X-nucleus\": benchmarks_path / \"adata_10X_nuc.h5ad\",\n    \"BIDCell\": benchmarks_path / \"adata_BIDCell.h5ad\",\n}\n</pre> # Define segmentation file paths segmentation_paths = {     \"segger\": benchmarks_path / \"adata_segger.h5ad\",     \"Baysor\": benchmarks_path / \"adata_baysor.h5ad\",     \"10X\": benchmarks_path / \"adata_10X.h5ad\",     \"10X-nucleus\": benchmarks_path / \"adata_10X_nuc.h5ad\",     \"BIDCell\": benchmarks_path / \"adata_BIDCell.h5ad\", } In\u00a0[\u00a0]: Copied! <pre># Load the segmentations and the scRNAseq data\nsegmentations_dict = load_segmentations(segmentation_paths)\nsegmentations_dict = {\n    k: segmentations_dict[k] for k in method_colors.keys() if k in segmentations_dict\n}\nscRNAseq_adata = sc.read(benchmarks_path / \"scRNAseq.h5ad\")\n</pre> # Load the segmentations and the scRNAseq data segmentations_dict = load_segmentations(segmentation_paths) segmentations_dict = {     k: segmentations_dict[k] for k in method_colors.keys() if k in segmentations_dict } scRNAseq_adata = sc.read(benchmarks_path / \"scRNAseq.h5ad\") <p>Generate general statistics plots plot_general_statistics_plots(segmentations_dict, figures_path, method_colors)</p> In\u00a0[\u00a0]: Copied! <pre>plot_cell_counts(segmentations_dict, figures_path, palette=method_colors)\nplot_cell_area(segmentations_dict, figures_path, palette=method_colors)\n</pre> plot_cell_counts(segmentations_dict, figures_path, palette=method_colors) plot_cell_area(segmentations_dict, figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Find markers for scRNAseq data\nmarkers = find_markers(\n    scRNAseq_adata,\n    cell_type_column=\"celltype_major\",\n    pos_percentile=30,\n    neg_percentile=5,\n)\n</pre> # Find markers for scRNAseq data markers = find_markers(     scRNAseq_adata,     cell_type_column=\"celltype_major\",     pos_percentile=30,     neg_percentile=5, ) <p>Annotate spatial segmentations with scRNAseq reference data for method in segmentation_paths.keys(): segmentations_dict[method] = annotate_query_with_reference( reference_adata=scRNAseq_adata, query_adata=segmentations_dict[method], transfer_column='celltype_major' ) segmentations_dict[method].write(segmentation_paths[method])</p> In\u00a0[\u00a0]: Copied! <pre>sc._settings.ScanpyConfig.figdir = figures_path\nsegmentations_dict[\"segger_embedding\"].obsm[\"spatial\"] = (\n    segmentations_dict[\"segger_embedding\"]\n    .obs[[\"cell_centroid_x\", \"cell_centroid_y\"]]\n    .values\n)\nsc.pl.spatial(\n    segmentations_dict[\"segger_embedding\"],\n    spot_size=10,\n    save=\"embedding.pdf\",\n    color=\"celltype_major\",\n    palette=major_colors,\n)\n</pre> sc._settings.ScanpyConfig.figdir = figures_path segmentations_dict[\"segger_embedding\"].obsm[\"spatial\"] = (     segmentations_dict[\"segger_embedding\"]     .obs[[\"cell_centroid_x\", \"cell_centroid_y\"]]     .values ) sc.pl.spatial(     segmentations_dict[\"segger_embedding\"],     spot_size=10,     save=\"embedding.pdf\",     color=\"celltype_major\",     palette=major_colors, ) In\u00a0[\u00a0]: Copied! <pre># Find mutually exclusive genes based on scRNAseq data\nexclusive_gene_pairs = find_mutually_exclusive_genes(\n    adata=scRNAseq_adata, markers=markers, cell_type_column=\"celltype_major\"\n)\n</pre> # Find mutually exclusive genes based on scRNAseq data exclusive_gene_pairs = find_mutually_exclusive_genes(     adata=scRNAseq_adata, markers=markers, cell_type_column=\"celltype_major\" ) In\u00a0[\u00a0]: Copied! <pre># Compute MECR for each segmentation method\nmecr_results = {}\nfor method in segmentations_dict.keys():\n    mecr_results[method] = compute_MECR(\n        segmentations_dict[method], exclusive_gene_pairs\n    )\n</pre> # Compute MECR for each segmentation method mecr_results = {} for method in segmentations_dict.keys():     mecr_results[method] = compute_MECR(         segmentations_dict[method], exclusive_gene_pairs     ) In\u00a0[\u00a0]: Copied! <pre># Compute quantized MECR area and counts using the mutually exclusive gene pairs\nquantized_mecr_area = {}\nquantized_mecr_counts = {}\n</pre> # Compute quantized MECR area and counts using the mutually exclusive gene pairs quantized_mecr_area = {} quantized_mecr_counts = {} In\u00a0[\u00a0]: Copied! <pre>for method in segmentations_dict.keys():\n    if \"cell_area\" in segmentations_dict[method].obs.columns:\n        quantized_mecr_area[method] = compute_quantized_mecr_area(\n            adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs\n        )\n    quantized_mecr_counts[method] = compute_quantized_mecr_counts(\n        adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs\n    )\n</pre> for method in segmentations_dict.keys():     if \"cell_area\" in segmentations_dict[method].obs.columns:         quantized_mecr_area[method] = compute_quantized_mecr_area(             adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs         )     quantized_mecr_counts[method] = compute_quantized_mecr_counts(         adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs     ) In\u00a0[\u00a0]: Copied! <pre># Plot MECR results\nplot_mecr_results(mecr_results, output_path=figures_path, palette=method_colors)\nplot_quantized_mecr_area(\n    quantized_mecr_area, output_path=figures_path, palette=method_colors\n)\nplot_quantized_mecr_counts(\n    quantized_mecr_counts, output_path=figures_path, palette=method_colors\n)\n</pre> # Plot MECR results plot_mecr_results(mecr_results, output_path=figures_path, palette=method_colors) plot_quantized_mecr_area(     quantized_mecr_area, output_path=figures_path, palette=method_colors ) plot_quantized_mecr_counts(     quantized_mecr_counts, output_path=figures_path, palette=method_colors ) In\u00a0[\u00a0]: Copied! <pre># Filter segmentation methods for contamination analysis\nnew_segmentations_dict = {\n    k: v\n    for k, v in segmentations_dict.items()\n    if k in [\"segger\", \"Baysor\", \"10X\", \"10X-nucleus\", \"BIDCell\"]\n}\n</pre> # Filter segmentation methods for contamination analysis new_segmentations_dict = {     k: v     for k, v in segmentations_dict.items()     if k in [\"segger\", \"Baysor\", \"10X\", \"10X-nucleus\", \"BIDCell\"] } In\u00a0[\u00a0]: Copied! <pre># Compute contamination results\ncontamination_results = {}\nfor method, adata in new_segmentations_dict.items():\n    if (\n        \"cell_centroid_x\" in adata.obs.columns\n        and \"cell_centroid_y\" in adata.obs.columns\n    ):\n        contamination_results[method] = calculate_contamination(\n            adata=adata,\n            markers=markers,  # Assuming you have a dictionary of markers for cell types\n            radius=15,\n            n_neighs=20,\n            celltype_column=\"celltype_major\",\n            num_cells=10000,\n        )\n</pre> # Compute contamination results contamination_results = {} for method, adata in new_segmentations_dict.items():     if (         \"cell_centroid_x\" in adata.obs.columns         and \"cell_centroid_y\" in adata.obs.columns     ):         contamination_results[method] = calculate_contamination(             adata=adata,             markers=markers,  # Assuming you have a dictionary of markers for cell types             radius=15,             n_neighs=20,             celltype_column=\"celltype_major\",             num_cells=10000,         ) In\u00a0[\u00a0]: Copied! <pre># Prepare contamination data for boxplots\nboxplot_data = []\nfor method, df in contamination_results.items():\n    melted_df = df.reset_index().melt(\n        id_vars=[\"Source Cell Type\"],\n        var_name=\"Target Cell Type\",\n        value_name=\"Contamination\",\n    )\n    melted_df[\"Segmentation Method\"] = method\n    boxplot_data.append(melted_df)\n</pre> # Prepare contamination data for boxplots boxplot_data = [] for method, df in contamination_results.items():     melted_df = df.reset_index().melt(         id_vars=[\"Source Cell Type\"],         var_name=\"Target Cell Type\",         value_name=\"Contamination\",     )     melted_df[\"Segmentation Method\"] = method     boxplot_data.append(melted_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all contamination dataframes into one\nboxplot_data = pd.concat(boxplot_data)\n</pre> # Concatenate all contamination dataframes into one boxplot_data = pd.concat(boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot contamination results\nplot_contamination_results(\n    contamination_results, output_path=figures_path, palette=method_colors\n)\nplot_contamination_boxplots(\n    boxplot_data, output_path=figures_path, palette=method_colors\n)\n</pre> # Plot contamination results plot_contamination_results(     contamination_results, output_path=figures_path, palette=method_colors ) plot_contamination_boxplots(     boxplot_data, output_path=figures_path, palette=method_colors ) In\u00a0[\u00a0]: Copied! <pre># Separate Segger into nucleus-positive and nucleus-negative cells\nsegmentations_dict[\"segger_n1\"] = segmentations_dict[\"segger\"][\n    segmentations_dict[\"segger\"].obs.has_nucleus\n]\nsegmentations_dict[\"segger_n0\"] = segmentations_dict[\"segger\"][\n    ~segmentations_dict[\"segger\"].obs.has_nucleus\n]\n</pre> # Separate Segger into nucleus-positive and nucleus-negative cells segmentations_dict[\"segger_n1\"] = segmentations_dict[\"segger\"][     segmentations_dict[\"segger\"].obs.has_nucleus ] segmentations_dict[\"segger_n0\"] = segmentations_dict[\"segger\"][     ~segmentations_dict[\"segger\"].obs.has_nucleus ] In\u00a0[\u00a0]: Copied! <pre># Compute clustering scores for all segmentation methods\nclustering_scores = {}\nfor method, adata in segmentations_dict.items():\n    ch_score, sh_score = compute_clustering_scores(\n        adata, cell_type_column=\"celltype_major\"\n    )\n    clustering_scores[method] = (ch_score, sh_score)\n</pre> # Compute clustering scores for all segmentation methods clustering_scores = {} for method, adata in segmentations_dict.items():     ch_score, sh_score = compute_clustering_scores(         adata, cell_type_column=\"celltype_major\"     )     clustering_scores[method] = (ch_score, sh_score) In\u00a0[\u00a0]: Copied! <pre># Plot UMAPs with clustering scores in the title\nplot_umaps_with_scores(\n    segmentations_dict, clustering_scores, figures_path, palette=major_colors\n)\n</pre> # Plot UMAPs with clustering scores in the title plot_umaps_with_scores(     segmentations_dict, clustering_scores, figures_path, palette=major_colors ) In\u00a0[\u00a0]: Copied! <pre># Compute neighborhood metrics for methods with spatial data\nfor method, adata in segmentations_dict.items():\n    if \"spatial\" in list(adata.obsm.keys()):\n        compute_neighborhood_metrics(adata, radius=15, celltype_column=\"celltype_major\")\n</pre> # Compute neighborhood metrics for methods with spatial data for method, adata in segmentations_dict.items():     if \"spatial\" in list(adata.obsm.keys()):         compute_neighborhood_metrics(adata, radius=15, celltype_column=\"celltype_major\") In\u00a0[\u00a0]: Copied! <pre># Prepare neighborhood entropy data for boxplots\nentropy_boxplot_data = []\nfor method, adata in segmentations_dict.items():\n    if \"neighborhood_entropy\" in adata.obs.columns:\n        entropy_df = pd.DataFrame(\n            {\n                \"Cell Type\": adata.obs[\"celltype_major\"],\n                \"Neighborhood Entropy\": adata.obs[\"neighborhood_entropy\"],\n                \"Segmentation Method\": method,\n            }\n        )\n        # Filter out NaN values, keeping only the subsetted cells\n        entropy_df = entropy_df.dropna(subset=[\"Neighborhood Entropy\"])\n        entropy_boxplot_data.append(entropy_df)\n</pre> # Prepare neighborhood entropy data for boxplots entropy_boxplot_data = [] for method, adata in segmentations_dict.items():     if \"neighborhood_entropy\" in adata.obs.columns:         entropy_df = pd.DataFrame(             {                 \"Cell Type\": adata.obs[\"celltype_major\"],                 \"Neighborhood Entropy\": adata.obs[\"neighborhood_entropy\"],                 \"Segmentation Method\": method,             }         )         # Filter out NaN values, keeping only the subsetted cells         entropy_df = entropy_df.dropna(subset=[\"Neighborhood Entropy\"])         entropy_boxplot_data.append(entropy_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all entropy dataframes into one\nentropy_boxplot_data = pd.concat(entropy_boxplot_data)\n</pre> # Concatenate all entropy dataframes into one entropy_boxplot_data = pd.concat(entropy_boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot neighborhood entropy boxplots\nplot_entropy_boxplots(entropy_boxplot_data, figures_path, palette=method_colors)\n</pre> # Plot neighborhood entropy boxplots plot_entropy_boxplots(entropy_boxplot_data, figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Find markers for sensitivity calculation\npurified_markers = find_markers(\n    scRNAseq_adata, \"celltype_major\", pos_percentile=20, percentage=75\n)\n</pre> # Find markers for sensitivity calculation purified_markers = find_markers(     scRNAseq_adata, \"celltype_major\", pos_percentile=20, percentage=75 ) In\u00a0[\u00a0]: Copied! <pre># Calculate sensitivity for each segmentation method\nsensitivity_results_per_method = {}\nfor method, adata in segmentations_dict.items():\n    sensitivity_results = calculate_sensitivity(\n        adata, purified_markers, max_cells_per_type=2000\n    )\n    sensitivity_results_per_method[method] = sensitivity_results\n</pre> # Calculate sensitivity for each segmentation method sensitivity_results_per_method = {} for method, adata in segmentations_dict.items():     sensitivity_results = calculate_sensitivity(         adata, purified_markers, max_cells_per_type=2000     )     sensitivity_results_per_method[method] = sensitivity_results In\u00a0[\u00a0]: Copied! <pre># Prepare data for sensitivity boxplots\nsensitivity_boxplot_data = []\nfor method, sensitivity_results in sensitivity_results_per_method.items():\n    for cell_type, sensitivities in sensitivity_results.items():\n        method_df = pd.DataFrame(\n            {\n                \"Cell Type\": cell_type,\n                \"Sensitivity\": sensitivities,\n                \"Segmentation Method\": method,\n            }\n        )\n        sensitivity_boxplot_data.append(method_df)\n</pre> # Prepare data for sensitivity boxplots sensitivity_boxplot_data = [] for method, sensitivity_results in sensitivity_results_per_method.items():     for cell_type, sensitivities in sensitivity_results.items():         method_df = pd.DataFrame(             {                 \"Cell Type\": cell_type,                 \"Sensitivity\": sensitivities,                 \"Segmentation Method\": method,             }         )         sensitivity_boxplot_data.append(method_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all sensitivity dataframes into one\nsensitivity_boxplot_data = pd.concat(sensitivity_boxplot_data)\n</pre> # Concatenate all sensitivity dataframes into one sensitivity_boxplot_data = pd.concat(sensitivity_boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot sensitivity boxplots\nplot_sensitivity_boxplots(sensitivity_boxplot_data, figures_path, palette=method_colors)\n</pre> # Plot sensitivity boxplots plot_sensitivity_boxplots(sensitivity_boxplot_data, figures_path, palette=method_colors)"},{"location":"notebooks/segger_tutorial/","title":"Introduction to Segger","text":"<p>Installing segger from the GitHub repository:</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/EliHei2/segger_dev.git\n%cd segger_dev\n!pip install \".[rapids12]\" -q\n</pre> !git clone https://github.com/EliHei2/segger_dev.git %cd segger_dev !pip install \".[rapids12]\" -q <p>Downloading the Xenium Human Pancreatic Dataset:</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir data_xenium\n%cd data_xenium\n!wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n!unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n%cd ..\n</pre> !mkdir data_xenium %cd data_xenium !wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip !unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip %cd .. In\u00a0[\u00a0]: Copied! <pre>from segger.data.parquet.sample import STSampleParquet\nfrom segger.training.segger_data_module import SeggerDataModule\nfrom segger.training.train import LitSegger\nfrom segger.prediction.predict_parquet import segment, load_model\nfrom lightning.pytorch.loggers import CSVLogger\nfrom pytorch_lightning import Trainer\nfrom pathlib import Path\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport scanpy as sc\n</pre> from segger.data.parquet.sample import STSampleParquet from segger.training.segger_data_module import SeggerDataModule from segger.training.train import LitSegger from segger.prediction.predict_parquet import segment, load_model from lightning.pytorch.loggers import CSVLogger from pytorch_lightning import Trainer from pathlib import Path import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import scanpy as sc In\u00a0[\u00a0]: Copied! <pre>xenium_data_dir = Path('data_xenium')\nsegger_data_dir = Path('data_segger')\n\nsample = STSampleParquet(\n    base_dir=xenium_data_dir,\n    n_workers=4,\n    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n    # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available\n)\n\nsample.save(\n      data_dir=segger_data_dir,\n      k_bd=3,\n      dist_bd=15.0,\n      k_tx=3,\n      dist_tx=5.0,\n      tile_width=120,\n      tile_height=120,\n      neg_sampling_ratio=5.0,\n      frac=1.0,\n      val_prob=0.1,\n      test_prob=0.2,\n)\n</pre> xenium_data_dir = Path('data_xenium') segger_data_dir = Path('data_segger')  sample = STSampleParquet(     base_dir=xenium_data_dir,     n_workers=4,     sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.     # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available )  sample.save(       data_dir=segger_data_dir,       k_bd=3,       dist_bd=15.0,       k_tx=3,       dist_tx=5.0,       tile_width=120,       tile_height=120,       neg_sampling_ratio=5.0,       frac=1.0,       val_prob=0.1,       test_prob=0.2, ) In\u00a0[\u00a0]: Copied! <pre>from segger.data.utils import calculate_gene_celltype_abundance_embedding\nscrnaseq_file = Path('my_scRNAseq_file.h5ad')\ncelltype_column = 'celltype_column'\ngene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n    sc.read(scrnaseq_file),\n    celltype_column\n)\n\nsample = STSampleParquet(\n    base_dir=xenium_data_dir,\n    n_workers=4,\n    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n    weights=gene_celltype_abundance_embedding, \n)\n</pre> from segger.data.utils import calculate_gene_celltype_abundance_embedding scrnaseq_file = Path('my_scRNAseq_file.h5ad') celltype_column = 'celltype_column' gene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(     sc.read(scrnaseq_file),     celltype_column )  sample = STSampleParquet(     base_dir=xenium_data_dir,     n_workers=4,     sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.     weights=gene_celltype_abundance_embedding,  ) In\u00a0[\u00a0]: Copied! <pre># Base directory to store Pytorch Lightning models\nmodels_dir = Path('models')\n\n# Initialize the Lightning data module\ndm = SeggerDataModule(\n    data_dir=segger_data_dir,\n    batch_size=2,\n    num_workers=2,\n)\n\ndm.setup()\n\nnum_tx_tokens = 500\n\n# If you use custom gene embeddings, use the following two lines instead:\n# num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes\n\n\nmodel = Segger(\n    # is_token_based=is_token_based,\n    num_tx_tokens=num_tx_tokens,\n    init_emb=8,\n    hidden_channels=64,\n    out_channels=16,\n    heads=4,\n    num_mid_layers=3,\n)\nmodel = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")\n\nbatch = dm.train[0]\nmodel.forward(batch.x_dict, batch.edge_index_dict)\n# Wrap the model in LitSegger\nls = LitSegger(model=model)\n\n# Initialize the Lightning trainer\ntrainer = Trainer(\n    accelerator='cuda',\n    strategy='auto',\n    precision='16-mixed',\n    devices=1, # set higher number if more gpus are available\n    max_epochs=100,\n    default_root_dir=models_dir,\n    logger=CSVLogger(models_dir),\n)\n</pre> # Base directory to store Pytorch Lightning models models_dir = Path('models')  # Initialize the Lightning data module dm = SeggerDataModule(     data_dir=segger_data_dir,     batch_size=2,     num_workers=2, )  dm.setup()  num_tx_tokens = 500  # If you use custom gene embeddings, use the following two lines instead: # num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes   model = Segger(     # is_token_based=is_token_based,     num_tx_tokens=num_tx_tokens,     init_emb=8,     hidden_channels=64,     out_channels=16,     heads=4,     num_mid_layers=3, ) model = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")  batch = dm.train[0] model.forward(batch.x_dict, batch.edge_index_dict) # Wrap the model in LitSegger ls = LitSegger(model=model)  # Initialize the Lightning trainer trainer = Trainer(     accelerator='cuda',     strategy='auto',     precision='16-mixed',     devices=1, # set higher number if more gpus are available     max_epochs=100,     default_root_dir=models_dir,     logger=CSVLogger(models_dir), ) In\u00a0[\u00a0]: Copied! <pre># Fit model\ntrainer.fit(\n    model=ls,\n    datamodule=dm\n)\n</pre> # Fit model trainer.fit(     model=ls,     datamodule=dm ) <p>Key parameters for training:</p> <ul> <li><code>--data_dir</code>: Directory containing the training data.</li> <li><code>--model_dir</code>: Directory in which to store models.</li> <li><code>--epochs</code>: Specifies the number of training epochs.</li> <li><code>--batch_size</code>: Batch sizes for training and validation data.</li> <li><code>--learning_rate</code>: The initial learning rate for the optimizer.</li> <li><code>--hidden_channels</code>: Number of hidden channels in the GNN layers.</li> <li><code>--heads</code>: Number of attention heads used in each graph convolutional layer.</li> <li><code>--init_emb</code>: Sets the dimensionality of the initial embeddings applied to the input node features (e.g., transcripts). A higher embedding dimension may capture more feature complexity but also requires more computation.</li> <li><code>--out_channels</code>: Specifies the number of output channels after the final graph attention layer, e.g. the final learned representations of the graph nodes.</li> </ul> <p>Additional Options for Training the Segger Model:</p> <ul> <li><code>--aggr</code>: This option controls the aggregation method used in the graph convolution layers.</li> <li><code>--accelerator</code>: Controls the hardware used for training, such as <code>cuda</code> for GPU training. This enables Segger to leverage GPU resources for faster training, especially useful for large datasets.</li> <li><code>--strategy</code>: Defines the distributed training strategy, with <code>auto</code> allowing PyTorch Lightning to automatically configure the best strategy based on the hardware setup.</li> <li><code>--precision</code>: Enables mixed precision training (e.g., <code>16-mixed</code>), which can speed up training and reduce memory usage while maintaining accuracy.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Evaluate results\nmodel_version = 0  # 'v_num' from training output above\nmodel_path = Path('../human_CRC') / 'lightning_logs' / f'version_{model_version}'\nmetrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)\n\nfig, ax = plt.subplots(1,1, figsize=(2,2))\n\nfor col in metrics.columns.difference(['epoch']):\n    metric = metrics[col].dropna()\n    ax.plot(metric.index, metric.values, label=col)\n\nax.legend(loc=(1, 0.33))\nax.set_ylim(0, 1)\nax.set_xlabel('Step')\n</pre> # Evaluate results model_version = 0  # 'v_num' from training output above model_path = Path('../human_CRC') / 'lightning_logs' / f'version_{model_version}' metrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)  fig, ax = plt.subplots(1,1, figsize=(2,2))  for col in metrics.columns.difference(['epoch']):     metric = metrics[col].dropna()     ax.plot(metric.index, metric.values, label=col)  ax.legend(loc=(1, 0.33)) ax.set_ylim(0, 1) ax.set_xlabel('Step') In\u00a0[\u00a0]: Copied! <pre># Evaluate results\nmodel_version = 0  # 'v_num' from training output above\nmodel_path = models_dir / 'lightning_logs' / f'version_{model_version}'\nmetrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)\n\nfig, ax = plt.subplots(1,1, figsize=(2,2))\n\nfor col in metrics.columns.difference(['epoch']):\n    metric = metrics[col].dropna()\n    ax.plot(metric.index, metric.values, label=col)\n\nax.legend(loc=(1, 0.33))\nax.set_ylim(0, 1)\nax.set_xlabel('Step')\n</pre> # Evaluate results model_version = 0  # 'v_num' from training output above model_path = models_dir / 'lightning_logs' / f'version_{model_version}' metrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)  fig, ax = plt.subplots(1,1, figsize=(2,2))  for col in metrics.columns.difference(['epoch']):     metric = metrics[col].dropna()     ax.plot(metric.index, metric.values, label=col)  ax.legend(loc=(1, 0.33)) ax.set_ylim(0, 1) ax.set_xlabel('Step') Out[\u00a0]: <pre>Text(0.5, 0, 'Step')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>dm = SeggerDataModule(\n    data_dir='data_segger',\n    batch_size=1,\n    num_workers=4,\n)\n\ndm.setup()\n\nmodel_version = 0\nmodel_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\"\nmodel = load_model(model_path / \"checkpoints\")\n\nreceptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}\n\nsegment(\n    model,\n    dm,\n    save_dir='benchmarks',\n    seg_tag='segger_output',\n    transcript_file='data_xenium/transcripts.parquet',\n    receptive_field=receptive_field,\n    min_transcripts=5,\n    cell_id_col='segger_cell_id',\n    use_cc=False,\n    knn_method='kd_tree',\n    verbose=True,\n)\n</pre> dm = SeggerDataModule(     data_dir='data_segger',     batch_size=1,     num_workers=4, )  dm.setup()  model_version = 0 model_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\" model = load_model(model_path / \"checkpoints\")  receptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}  segment(     model,     dm,     save_dir='benchmarks',     seg_tag='segger_output',     transcript_file='data_xenium/transcripts.parquet',     receptive_field=receptive_field,     min_transcripts=5,     cell_id_col='segger_cell_id',     use_cc=False,     knn_method='kd_tree',     verbose=True, )  <pre>Starting segmentation for segger_embedding_1001...\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(2,2))\nsns.histplot(\n    segmentation['score'],\n    bins=50,\n    ax=ax,\n)\nax.set_ylabel('Count')\nax.set_xlabel('Segger Similarity Score')\nax.set_yscale('log')\n</pre> fig, ax = plt.subplots(1,1, figsize=(2,2)) sns.histplot(     segmentation['score'],     bins=50,     ax=ax, ) ax.set_ylabel('Count') ax.set_xlabel('Segger Similarity Score') ax.set_yscale('log') In\u00a0[\u00a0]: Copied! <pre>import itertools\nimport pandas as pd\n</pre> import itertools import pandas as pd In\u00a0[\u00a0]: Copied! <pre>tuning_dir = Path('path/to/tutorial/tuning/')\nsampling_rate = 0.125\n</pre> tuning_dir = Path('path/to/tutorial/tuning/') sampling_rate = 0.125 In\u00a0[\u00a0]: Copied! <pre># Fixed function arguments used for each trial\ntranscripts_path = xenium_data_dir / 'transcripts.parquet'\n\nboundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'\n\ndataset_kwargs = dict(\n    x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,\n    num_workers=4, sampling_rate=sampling_rate,\n)\n\nmodel_kwargs = dict(\n    metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),\n    num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,\n    heads=2, num_mid_layers=2, aggr='sum',\n)\n\ntrainer_kwargs = dict(\n    accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,\n    max_epochs=100,\n)\n\npredict_kwargs = dict(score_cut=0.2, use_cc=True)\n</pre> # Fixed function arguments used for each trial transcripts_path = xenium_data_dir / 'transcripts.parquet'  boundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'  dataset_kwargs = dict(     x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,     num_workers=4, sampling_rate=sampling_rate, )  model_kwargs = dict(     metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),     num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,     heads=2, num_mid_layers=2, aggr='sum', )  trainer_kwargs = dict(     accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,     max_epochs=100, )  predict_kwargs = dict(score_cut=0.2, use_cc=True) In\u00a0[\u00a0]: Copied! <pre>def trainable(config):\n\n    receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}\n\n    # Dataset creation\n    xs = XeniumSample(verbose=False)\n    xs.set_file_paths(transcripts_path, boundaries_path)\n    xs.set_metadata()\n    try:\n        xs.save_dataset_for_segger(\n            processed_dir=config['data_dir'],\n            receptive_field=receptive_field,\n            **dataset_kwargs,\n        )\n    except:\n        pass\n\n    # Model training\n    ls = LitSegger(**model_kwargs)\n    dm = SeggerDataModule(\n        data_dir=config['data_dir'],\n        batch_size=2,\n        num_workers=dataset_kwargs['num_workers'],\n    )\n    trainer = Trainer(\n        default_root_dir=config['model_dir'],\n        logger=CSVLogger(config['model_dir']),\n        **trainer_kwargs,\n    )\n    trainer.fit(model=ls, datamodule=dm)\n\n    segmentation = predict(\n        load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),\n        dm.train_dataloader(),\n        receptive_field=receptive_field,\n        **predict_kwargs,\n    )\n\n    metrics = evaluate(segmentation)\n\n\ndef evaluate(segmentation: pd.DataFrame, score_cut: float) -&gt; pd.Series:\n\n    assigned = segmentation['score'] &gt; score_cut\n    metrics = pd.Series(dtype=float)\n    metrics['frac_assigned'] = assigned.mean()\n    cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()\n    assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()\n    cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()\n    metrics['cell_size_assigned'] = assigned_avg\n    metrics['cell_size_cc'] = cc_avg\n    return metrics\n</pre> def trainable(config):      receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}      # Dataset creation     xs = XeniumSample(verbose=False)     xs.set_file_paths(transcripts_path, boundaries_path)     xs.set_metadata()     try:         xs.save_dataset_for_segger(             processed_dir=config['data_dir'],             receptive_field=receptive_field,             **dataset_kwargs,         )     except:         pass      # Model training     ls = LitSegger(**model_kwargs)     dm = SeggerDataModule(         data_dir=config['data_dir'],         batch_size=2,         num_workers=dataset_kwargs['num_workers'],     )     trainer = Trainer(         default_root_dir=config['model_dir'],         logger=CSVLogger(config['model_dir']),         **trainer_kwargs,     )     trainer.fit(model=ls, datamodule=dm)      segmentation = predict(         load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),         dm.train_dataloader(),         receptive_field=receptive_field,         **predict_kwargs,     )      metrics = evaluate(segmentation)   def evaluate(segmentation: pd.DataFrame, score_cut: float) -&gt; pd.Series:      assigned = segmentation['score'] &gt; score_cut     metrics = pd.Series(dtype=float)     metrics['frac_assigned'] = assigned.mean()     cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()     assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()     cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()     metrics['cell_size_assigned'] = assigned_avg     metrics['cell_size_cc'] = cc_avg     return metrics In\u00a0[\u00a0]: Copied! <pre>param_space = {\n    \"k_bd\": [3, 5, 10],\n    \"dist_bd\": [5, 10, 15, 20],\n    \"k_tx\": [3, 5, 10],\n    \"dist_tx\": [3, 5, 10],\n}\n\nmetrics = []\n\nfor params in itertools.product(*param_space.values()):\n\n    config = dict(zip(param_space.keys(), params))\n\n    # Setup directories\n    trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])\n\n    data_dir = trial_dir / 'segger_data'\n    data_dir.mkdir(exist_ok=True, parents=True)\n    config['data_dir'] = data_dir\n\n    model_dir = trial_dir / 'models'\n    model_dir.mkdir(exist_ok=True, parents=True)\n    config['model_dir'] = model_dir\n\n    segmentation = trainable(config)\n    trial = evaluate(segmentation, predict_kwargs['score_cut'])\n    trial = pd.concat([pd.Series(config), trial])\n    metrics.append(trial)\n\nmetrics = pd.DataFrame(metrics)\n</pre> param_space = {     \"k_bd\": [3, 5, 10],     \"dist_bd\": [5, 10, 15, 20],     \"k_tx\": [3, 5, 10],     \"dist_tx\": [3, 5, 10], }  metrics = []  for params in itertools.product(*param_space.values()):      config = dict(zip(param_space.keys(), params))      # Setup directories     trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])      data_dir = trial_dir / 'segger_data'     data_dir.mkdir(exist_ok=True, parents=True)     config['data_dir'] = data_dir      model_dir = trial_dir / 'models'     model_dir.mkdir(exist_ok=True, parents=True)     config['model_dir'] = model_dir      segmentation = trainable(config)     trial = evaluate(segmentation, predict_kwargs['score_cut'])     trial = pd.concat([pd.Series(config), trial])     metrics.append(trial)  metrics = pd.DataFrame(metrics) In\u00a0[\u00a0]: Copied! <pre>metrics\n</pre> metrics"},{"location":"notebooks/segger_tutorial/#introduction-to-segger","title":"Introduction to Segger\u00b6","text":"<p>Important note (Dec 2024): As segger is currently undergoing constant development we highly recommend installing directly via github.</p> <p>Segger is a cutting-edge cell segmentation model specifically designed for single-molecule resolved spatial omics datasets. It addresses the challenge of accurately segmenting individual cells in complex imaging datasets, leveraging a unique approach based on graph neural networks (GNNs).</p> <p>The core idea behind Segger is to model both nuclei and transcripts as graph nodes, with edges connecting them based on their spatial proximity. This allows the model to learn from the co-occurrence of nucleic and cytoplasmic molecules, resulting in more refined and accurate cell boundaries. By using spatial information and GNNs, Segger achieves state-of-the-art performance in segmenting single cells in datasets such as 10X Xenium and MERSCOPE, outperforming traditional methods like Baysor and Cellpose.</p> <p>Segger's workflow consists of:</p> <ol> <li>Dataset creation: Converting raw transcriptomic data into a graph-based dataset.</li> <li>Training: Training the Segger model on the graph to learn cell boundaries.</li> <li>Prediction: Using the trained model to make predictions on new datasets.</li> </ol> <p>This tutorial will guide you through each step of the process, ensuring you can train and apply Segger for your own data.</p>"},{"location":"notebooks/segger_tutorial/#1-create-your-segger-dataset","title":"1. Create your Segger Dataset\u00b6","text":"<p>In this step, we generate the dataset required for Segger's cell segmentation tasks.</p> <p>Segger relies on spatial transcriptomics data, combining staining boundaries (e.g., nuclei or membrane stainings) and transcripts from single-cell resolved imaging datasets. These nuclei and transcript nodes are represented in a graph, and the spatial proximity of transcripts to nuclei is used to establish edges between them.</p> <p>To use Segger with a Xenium dataset, you need the <code>transcripts.parquet</code> and <code>nucleus_boundaries.parquet</code> (or <code>cell_boundaries.parquet</code>, in case the Xenium samples comes with the segmentation kit) files. The transcripts file contains spatial coordinates and information for each transcript, while the boundaries file defines the polygon boundaries of the nuclei or cells. These files enable segger to map transcripts to their respective nuclei and perform cell segmentation based on spatial relationships. Segger can also be extended to other platforms by modifying the column names or formats in the input files to match its expected structure, making it adaptable for various spatial transcriptomics technologies. See this for Xenium settings.</p>"},{"location":"notebooks/segger_tutorial/#11-fast-dataset-creation-with-segger","title":"1.1. Fast Dataset Creation with segger\u00b6","text":"<p>Segger introduces a fast and efficient pipeline for processing spatial transcriptomics data. This method accelerates dataset creation, particularly for large datasets, by using ND-tree-based spatial partitioning and parallel processing. This results in a much faster preparation of the dataset, which is saved in PyTorch Geometric (PyG) format, similar to the previous method.</p> <p>Note: The previous dataset creation method will soon be deprecated in favor of this optimized pipeline.</p> <p>The pipeline requires the following inputs:</p> <ul> <li>base_dir: The directory containing the raw dataset.</li> <li>data_dir: The directory where the processed dataset (tiles in PyG format) will be saved.</li> </ul> <p>The core improvements in this method come from the use of ND-tree partitioning, which splits the data efficiently into spatial regions, and parallel processing, which speeds up the handling of these regions across multiple CPU cores. For example, using this pipeline, the Xenium Human Pancreatic Dataset can be processed in just a few minutes when running with 16 workers.</p> <p>Below is an example of how to create a dataset using the faster Segger pipeline:</p>"},{"location":"notebooks/segger_tutorial/#parameters","title":"Parameters\u00b6","text":"<p>Here is a complete list of parameters you can use to control the dataset creation process:</p> <ul> <li>--base_dir: Directory containing the raw spatial transcriptomics dataset.</li> <li>--data_dir: Directory where the processed Segger dataset (in PyG format) will be saved.</li> <li>--sample_type: (Optional) Specifies the type of dataset (e.g., \"xenium\" or \"merscope\"). Defaults to None.</li> <li>--scrnaseq_file: Path to the scRNAseq file (default: None).</li> <li>--celltype_column: Column name for cell type annotations in the scRNAseq file (default: None).</li> <li>--k_bd: Number of nearest neighbors for boundary nodes (default: 3).</li> <li>--dist_bd: Maximum distance for boundary neighbors (default: 15.0).</li> <li>--k_tx: Number of nearest neighbors for transcript nodes (default: 3).</li> <li>--dist_tx: Maximum distance for transcript neighbors (default: 5.0).</li> <li>--tile_size: Specifies the size of the tile. If provided, it overrides both tile_width and tile_height.</li> <li>--tile_width: Width of the tiles in pixels (ignored if tile_size is provided).</li> <li>--tile_height: Height of the tiles in pixels (ignored if tile_size is provided).</li> <li>--neg_sampling_ratio: Ratio of negative samples (default: 5.0).</li> <li>--frac: Fraction of the dataset to process (default: 1.0).</li> <li>--val_prob: Proportion of data used for validation split (default: 0.1).</li> <li>--test_prob: Proportion of data used for testing split (default: 0.2).</li> <li>--n_workers: Number of workers for parallel processing (default: 1).</li> </ul>"},{"location":"notebooks/segger_tutorial/#12-using-custom-gene-embeddings","title":"1.2. Using custom gene embeddings\u00b6","text":"<p>In the default mode, segger initially tokenizes transcripts based on their gene type simply in a one-hot manner. However, one can use other genes embeddings (e.g., pre-trained embeddings). The following example shows how one can employ a cell-type-annotated scRNAseq reference of the same tissue type (not necessary same sample or experiment) to embed genes based on their abaundance in different cell types:</p>"},{"location":"notebooks/segger_tutorial/#2-train-your-segger-model","title":"2. Train your Segger Model\u00b6","text":"<p>The Segger model training process begins after the dataset has been created. This model is a heterogeneous graph neural network (GNN) designed to segment single cells by leveraging both nuclei and transcript data.</p> <p>Segger uses graph attention layers to propagate information across nodes (nuclei and transcripts) and refine cell boundaries. The model architecture includes initial embedding layers, attention-based graph convolutions, and residual connections for stable learning.</p> <p>Segger leverages the PyTorch Lightning framework to streamline the training and evaluation of its graph neural network (GNN). PyTorch Lightning simplifies the training process by abstracting away much of the boilerplate code, allowing users to focus on model development and experimentation. It also supports multi-GPU training, mixed-precision, and efficient scaling, making it an ideal framework for training complex models like Segger.</p>"},{"location":"notebooks/segger_tutorial/#troubleshooting-1","title":"Troubleshooting #1\u00b6","text":"<p>In the cell below, we are visualizing key metrics from the model training and validation process. The plot displays training loss, validation loss, F1 validation score, and AUROC validation score over training steps. We expect to see the loss curves decreasing over time, signaling the model's improvement, and the F1 and AUROC scores increasing, reflecting improved segmentation performance as the model learns.</p> <p>If training is not working effectively, you might observe the following in the plot displaying training loss, validation loss, F1 score, and AUROC:</p> <ul> <li>Training loss not decreasing: If the training loss remains high or fluctuates without a consistent downward trend, this indicates that the model is not learning effectively from the training data.</li> <li>Validation loss decreases, then increases: If validation loss decreases initially but starts to increase while training loss continues to drop, this could be a sign of overfitting, where the model is performing well on the training data but not generalizing to the validation data.</li> <li>F1 score and AUROC not improving: If these metrics remain flat or show inconsistent improvement, the model may be struggling to correctly segment cells or classify transcripts, indicating an issue with learning performance.</li> </ul>"},{"location":"notebooks/segger_tutorial/#3-make-predictions","title":"3. Make Predictions\u00b6","text":"<p>Once the Segger model is trained, it can be used to make predictions on seen (partially trained) data or be transfered to unseen data. This step involves using a trained checkpoint to predict cell boundaries and refine transcript-nuclei associations.</p>"},{"location":"notebooks/segger_tutorial/#requirements-for-the-faster-prediction-pipeline","title":"Requirements for the Faster Prediction Pipeline\u00b6","text":"<p>The pipeline requires the following inputs:</p> <ul> <li>segger_data_dir: The directory containing the processed Segger dataset (in PyG format).</li> <li>models_dir: The directory containing the trained Segger model checkpoints.</li> <li>benchmarks_dir: The directory where the segmentation results will be saved.</li> <li>transcripts_file: Path to the file containing the transcript data for prediction.</li> </ul>"},{"location":"notebooks/segger_tutorial/#running-the-faster-prediction-pipeline","title":"Running the Faster Prediction Pipeline\u00b6","text":"<p>Below is an example of how to run the faster Segger prediction pipeline using the command line:</p>"},{"location":"notebooks/segger_tutorial/#parameters","title":"Parameters\u00b6","text":"<p>Here is a detailed explanation of each parameter used in the faster prediction pipeline:</p> <ul> <li>--segger_data_dir: The directory containing the processed Segger dataset, saved as PyTorch Geometric data objects, that will be used for prediction.</li> <li>--models_dir: The directory containing the trained Segger model checkpoints. These checkpoints store the learned weights required for making predictions.</li> <li>--benchmarks_dir: The directory where the segmentation results will be saved.</li> <li>--transcripts_file: Path to the transcripts.parquet file.</li> <li>--batch_size: Specifies the batch size for processing during prediction. Larger batch sizes speed up inference but use more memory (default: 1).</li> <li>--num_workers: Number of workers to use for parallel data loading (default: 1).</li> <li>--model_version: Version of the trained model to load for predictions, based on the version number from the training logs (default: 0).</li> <li>--save_tag: A tag used to name and organize the segmentation results (default: segger_embedding).</li> <li>--min_transcripts: The minimum number of transcripts required for segmentation (default: 5).</li> <li>--cell_id_col: The name of the column that stores the cell IDs (default: segger_cell_id).</li> <li>--use_cc: Enables the use of connected components (CC) for grouping transcripts that are not associated with any nucleus (default: False).</li> <li>--knn_method: Method for KNN (K-Nearest Neighbors) computation. Only option is \"cuda\" for this pipeline (default: cuda).</li> <li>--file_format: The format for saving the output segmentation data. Only option is \"anndata\" for this pipeline (default: anndata).</li> <li>--k_bd: Number of nearest neighbors for boundary nodes during segmentation (default: 4).</li> <li>--dist_bd: Maximum distance for boundary nodes during segmentation (default: 12.0).</li> <li>--k_tx: Number of nearest neighbors for transcript nodes during segmentation (default: 5).</li> <li>--dist_tx: Maximum distance for transcript nodes during segmentation (default: 5.0).</li> </ul>"},{"location":"notebooks/segger_tutorial/#troubleshooting-2","title":"Troubleshooting #2\u00b6","text":"<p>In the cell below, we are visualizing the distribution of Segger similarity scores using a histogram. The Segger similarity score reflects how closely transcripts are associated with their respective nuclei in the segmentation process. Higher scores indicate stronger associations between transcripts and their nuclei, suggesting more accurate cell boundaries. Lower scores might indicate weaker associations, which could highlight potential segmentation errors or challenging regions in the data. We expect to see a large number of the scores clustering toward higher values, which would indicate strong overall performance of the model in associating transcripts with nuclei.</p> <p>The following would indicate potential issues with the model's predictions:</p> <ul> <li>A very large portion of scores near zero: If many scores are concentrated at the lower end of the scale, this suggests that the model is frequently failing to associate transcripts with their corresponding nuclei, indicating poor segmentation quality.</li> <li>No clear peak in the distribution: If the histogram is flat or shows a wide, spread-out distribution, this could indicate that the model is struggling to consistently assign similarity scores, which may be a sign that the training process did not optimize the model correctly.</li> </ul> <p>Both cases would suggest that the model requires further tuning, such as adjusting hyperparameters, data preprocessing, or the training procedure (see below)</p>"},{"location":"notebooks/segger_tutorial/#the-importance-of-the-receptive-field-in-segger","title":"The Importance of the Receptive Field in Segger\u00b6","text":"<p>The receptive field is a critical parameter in Segger, as it directly influences how the model interprets the spatial relationships between transcripts and nuclei. In the context of spatial transcriptomics, the receptive field determines the size of the neighborhood that each node (representing transcripts or nuclei) can \"see\" during graph construction and model training. Segger is particularly sensitive to the size of the receptive field because it affects the model's ability to propagate information across the graph. If the receptive field is too small, the model may fail to capture sufficient context for correct cell boundary delineation. Conversely, a very large receptive field may introduce noise by linking unrelated or distant nodes, reducing segmentation accuracy.</p>"},{"location":"notebooks/segger_tutorial/#parameters-affecting-the-receptive-field-in-segger","title":"Parameters affecting the receptive field in Segger:\u00b6","text":"<ul> <li><code>--r</code>: This parameter defines the radius used when connecting transcripts to nuclei. A larger <code>r</code> expands the receptive field, linking more distant nodes. Fine-tuning this parameter helps ensure that Segger captures the right level of spatial interaction in the dataset.</li> <li><code>--k_bd</code> and <code>--k_tx</code>: These control the number of nearest neighbors (nuclei and transcripts, respectively) considered in the graph. By increasing these values, the receptive field is effectively broadened, allowing more nodes to contribute to the information propagation.</li> <li><code>--dist_bd</code> and <code>--dist_tx</code>: These parameters specify the maximum distances used to connect nuclei (<code>dist_bd</code>) and transcripts (<code>dist_tx</code>) to their neighbors during graph construction. They directly affect the receptive field by defining the cut-off distance for forming edges in the graph. Larger distance values expand the receptive field, connecting nodes that are further apart spatially. Careful tuning of these values is necessary to ensure that Segger captures relevant spatial relationships without introducing noise.</li> </ul>"},{"location":"notebooks/segger_tutorial/#4-tune-parameters","title":"4. Tune Parameters\u00b6","text":""},{"location":"notebooks/segger_tutorial/#evaluating-receptive-field-parameters-with-grid-search","title":"Evaluating Receptive Field Parameters with Grid Search\u00b6","text":"<p>To evaluate the impact of different receptive field parameters in Segger, we use a grid search approach. The parameters <code>k_bd</code>, <code>k_tx</code>, <code>dist_bd</code>, and <code>dist_tx</code> (which control the number of neighbors and distances for nuclei and transcripts) are explored through various configurations defined in <code>param_space</code>. Each combination of these parameters is passed to the <code>trainable</code> function, which creates the dataset, trains the model, and makes predictions based on the specified receptive field.</p> <p>For each parameter combination:</p> <ol> <li>A dataset is created with the specified receptive field.</li> <li>The Segger model is trained on this dataset.</li> <li>Predictions are made, and segmentation results are evaluated using the custom <code>evaluate</code> function. This function computes metrics like the fraction of assigned transcripts and average cell sizes.</li> </ol> <p>The results from each configuration are saved, allowing us to compare how different receptive field settings impact the model\u2019s performance. This process enables a thorough search of the parameter space, optimizing the model for accurate segmentation.</p>"},{"location":"notebooks/segger_tutorial/#interpreting-output-metrics","title":"Interpreting Output Metrics\u00b6","text":"<p>The key output metrics include:</p> <ul> <li><code>frac_assigned</code>: The fraction of transcripts that were successfully assigned to a cell. A higher value indicates that the model is doing a good job associating transcripts with nuclei, which is a strong indicator of successful segmentation.</li> <li><code>cell_size_assigned</code>: The average size of cells that have assigned transcripts. This helps assess how well the model is predicting cell boundaries, with unusually large or small values indicating potential issues with segmentation accuracy.</li> <li><code>cell_size_cc</code>: The average size of connected components that were not assigned to a cell (i.e., nucleus-less regions). Large values here may suggest that transcripts are being incorrectly grouped together in the absence of a nucleus, which could indicate problems with the receptive field parameters or the segmentation process.</li> </ul> <p>These metrics illuminate the effectiveness of the model by highlighting both the success in associating transcripts with cells and potential areas where the model may need further tuning.</p>"},{"location":"source/conf/","title":"Conf","text":"<p>Configuration file for the Sphinx documentation builder.</p> <p>For the full list of built-in configuration values, see the documentation: https://www.sphinx-doc.org/en/master/usage/configuration.html</p> <p>-- Project information ----------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information</p> In\u00a0[\u00a0]: Copied! <pre>project = \"segger\"\ncopyright = \"2024, Elyas Heidari\"\nauthor = \"Elyas Heidari\"\nrelease = \"0.01\"\n</pre> project = \"segger\" copyright = \"2024, Elyas Heidari\" author = \"Elyas Heidari\" release = \"0.01\" <p>-- General configuration --------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration</p> In\u00a0[\u00a0]: Copied! <pre>extensions = []\n</pre> extensions = [] In\u00a0[\u00a0]: Copied! <pre>templates_path = [\"_templates\"]\nexclude_patterns = []\n</pre> templates_path = [\"_templates\"] exclude_patterns = [] <p>-- Options for HTML output ------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output</p> In\u00a0[\u00a0]: Copied! <pre>html_theme = \"alabaster\"\nhtml_static_path = [\"_static\"]\n</pre> html_theme = \"alabaster\" html_static_path = [\"_static\"]"},{"location":"user_guide/","title":"API Reference","text":""},{"location":"user_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Dataset Creation</li> <li>Training</li> <li>Validation</li> </ul>"},{"location":"user_guide/data_creation/","title":"Data Preparation for <code>segger</code>","text":"<p>The <code>segger</code> package provides a comprehensive data preparation module for cell segmentation and subsequent graph-based deep learning tasks by leveraging scalable and efficient processing tools. </p> <p>Note</p> <p>Currently, <code>segger</code> supports Xenium and Merscope datasets. </p>"},{"location":"user_guide/data_creation/#steps","title":"Steps","text":"<p>The data preparation module offers the following key functionalities:</p> <ol> <li>Lazy Loading of Large Datasets: Utilizes Dask to handle large-scale transcriptomics and boundary datasets efficiently, avoiding memory bottlenecks.</li> <li>Initial Filtering: Filters transcripts based on quality metrics and dataset-specific criteria to ensure data integrity and relevance.</li> <li>Tiling: Divides datasets into spatial tiles, essential for localized graph-based models and parallel processing.</li> <li>Graph Construction: Converts spatial data into graph formats using PyTorch Geometric (PyG), enabling the application of graph neural networks (GNNs).</li> <li>Boundary Processing: Handles polygons, performs spatial geometrical calculations, and checks transcript overlaps with boundaries.</li> </ol> <p>Key Technologies</p> <ul> <li>Dask: Facilitates parallel and lazy data processing, enabling scalable handling of large datasets.</li> <li>PyTorch Geometric (PyG): Enables the construction of graph-based data representations suitable for GNNs.</li> <li>Shapely &amp; Geopandas: Utilized for spatial operations such as polygon creation, scaling, and spatial relationship computations.</li> <li>Dask-Geopandas: Extends Geopandas for parallel processing of geospatial data, enhancing scalability.</li> </ul>"},{"location":"user_guide/data_creation/#core-components","title":"Core Components","text":""},{"location":"user_guide/data_creation/#1-spatialtranscriptomicssample-abstract-base-class","title":"1. <code>SpatialTranscriptomicsSample</code> (Abstract Base Class)","text":"<p>This abstract class defines the foundational structure for managing spatial transcriptomics datasets. It provides essential methods for:</p> <ul> <li>Loading Data: Scalable loading of transcript and boundary data using Dask.</li> <li>Filtering Transcripts: Applying quality-based or dataset-specific filtering criteria.</li> <li>Spatial Relationships: Computing overlaps and spatial relationships between transcripts and boundaries.</li> <li>Tiling: Dividing datasets into smaller spatial tiles for localized processing.</li> <li>Graph Preparation: Converting data tiles into <code>PyTorch Geometric</code> graph structures.</li> </ul>"},{"location":"user_guide/data_creation/#key-methods","title":"Key Methods:","text":"<ul> <li><code>load_transcripts()</code>: Loads transcriptomic data from Parquet files, applies quality filtering, and incorporates additional gene embeddings.</li> <li><code>load_boundaries()</code>: Loads boundary data (e.g., cell or nucleus boundaries) from Parquet files.</li> <li><code>get_tile_data()</code>: Retrieves transcriptomic and boundary data within specified spatial bounds.</li> <li><code>generate_and_scale_polygons()</code>: Creates and scales polygon representations of boundaries for spatial computations.</li> <li><code>compute_transcript_overlap_with_boundaries()</code>: Determines the association of transcripts with boundary polygons.</li> <li><code>build_pyg_data_from_tile()</code>: Converts tile-specific data into <code>HeteroData</code> objects suitable for PyG models.</li> </ul>"},{"location":"user_guide/data_creation/#2-xeniumsample-and-merscopesample-child-classes","title":"2. <code>XeniumSample</code> and <code>MerscopeSample</code> (Child Classes)","text":"<p>These classes inherit from <code>SpatialTranscriptomicsSample</code> and implement dataset-specific processing logic:</p> <ul> <li><code>XeniumSample</code>: Tailored for Xenium datasets, it includes specific filtering rules to exclude unwanted transcripts based on naming patterns (e.g., <code>NegControlProbe_</code>, <code>BLANK_</code>).</li> <li><code>MerscopeSample</code>: Designed for Merscope datasets, allowing for custom filtering and processing logic as needed.</li> </ul>"},{"location":"user_guide/data_creation/#workflow","title":"Workflow","text":"<p>The dataset creation and processing workflow involves several key steps, each ensuring that the spatial transcriptomics data is appropriately prepared for downstream machine learning tasks.</p>"},{"location":"user_guide/data_creation/#step-1-data-loading-and-filtering","title":"Step 1: Data Loading and Filtering","text":"<ul> <li>Transcriptomic Data: Loaded lazily using Dask to handle large datasets efficiently. Custom filtering rules specific to the dataset (Xenium or Merscope) are applied to ensure data quality.</li> <li>Boundary Data: Loaded similarly using Dask, representing spatial structures such as cell or nucleus boundaries.</li> </ul>"},{"location":"user_guide/data_creation/#step-2-tiling","title":"Step 2: Tiling","text":"<ul> <li>Spatial Segmentation: The dataset is divided into smaller, manageable tiles of size \\(x_{\\text{size}} \\times y_{\\text{size}}\\), defined by their top-left corner coordinates \\((x_i, y_j)\\).</li> </ul> \\[ n_x = \\left\\lfloor \\frac{x_{\\text{max}} - x_{\\text{min}}}{d_x} \\right\\rfloor, \\quad n_y = \\left\\lfloor \\frac{y_{\\text{max}} - y_{\\text{min}}}{d_y} \\right\\rfloor \\] <p>Where:   - \\(x_{\\text{min}}, y_{\\text{min}}\\): Minimum spatial coordinates.   - \\(x_{\\text{max}}, y_{\\text{max}}\\): Maximum spatial coordinates.   - \\(d_x, d_y\\): Step sizes along the \\(x\\)- and \\(y\\)-axes, respectively.</p> <ul> <li>Transcript and Boundary Inclusion: For each tile, transcripts and boundaries within the spatial bounds (with optional margins) are included:</li> </ul> \\[  x_i - \\text{margin}_x \\leq x_t &lt; x_i + x_{\\text{size}} + \\text{margin}_x, \\quad y_j - \\text{margin}_y \\leq y_t &lt; y_j + y_{\\text{size}} + \\text{margin}_y  \\] <p>Where:   - \\(x_t, y_t\\): Transcript coordinates.   - \\(\\text{margin}_x, \\text{margin}_y\\): Optional margins to include contextual data.</p>"},{"location":"user_guide/data_creation/#step-3-graph-construction","title":"Step 3: Graph Construction","text":"<p>For each tile, a graph \\(G\\) is constructed with:</p> <ul> <li>Nodes (\\(V\\)):</li> <li>Transcripts: Represented by their spatial coordinates \\((x_t, y_t)\\) and feature vectors \\(\\mathbf{f}_t\\).</li> <li> <p>Boundaries: Represented by centroid coordinates \\((x_b, y_b)\\) and associated properties (e.g., area).</p> </li> <li> <p>Edges (\\(E\\)):</p> </li> <li>Created based on spatial proximity using methods like KD-Tree or FAISS.</li> <li>Defined by a distance threshold \\(d\\) and the number of nearest neighbors \\(k\\):</li> </ul> \\[  E = \\{ (v_i, v_j) \\mid \\text{dist}(v_i, v_j) &lt; d, \\, v_i \\in V, \\, v_j \\in V \\} \\]"},{"location":"user_guide/data_creation/#step-4-label-computation","title":"Step 4: Label Computation","text":"<p>If enabled, edges can be labeled based on relationships, such as whether a transcript belongs to a boundary:</p> \\[ \\text{label}(t, b) =  \\begin{cases} 1 &amp; \\text{if } t \\text{ belongs to } b \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"user_guide/data_creation/#step-5-train-test-validation-splitting","title":"Step 5: Train, Test, Validation Splitting","text":"<p>The dataset is partitioned into training, validation, and test sets based on predefined probabilities \\(p_{\\text{train}}, p_{\\text{val}}, p_{\\text{test}}\\):</p> \\[ p_{\\text{train}} + p_{\\text{val}} + p_{\\text{test}} = 1 \\] <p>Each tile is randomly assigned to one of these sets according to the specified probabilities.</p>"},{"location":"user_guide/data_creation/#output","title":"Output","text":"<p>The final output consists of a set of tiles, each containing a graph representation of the spatial transcriptomics data. These tiles are stored in designated directories (<code>train_tiles</code>, <code>val_tiles</code>, <code>test_tiles</code>) and are ready for integration into machine learning pipelines.</p>"},{"location":"user_guide/data_creation/#example-usage","title":"Example Usage","text":"<p>Below are examples demonstrating how to utilize the <code>segger</code> data preparation module for both Xenium and Merscope datasets.</p>"},{"location":"user_guide/data_creation/#xenium-data","title":"Xenium Data","text":"<pre><code>from segger.data import XeniumSample\nfrom pathlib import Path\nimport scanpy as sc\n\n# Set up the file paths\nraw_data_dir = Path(\"/path/to/xenium_output\")\nprocessed_data_dir = Path(\"path/to/processed_files\")\nsample_tag = \"sample/tag\"\n\n# Load scRNA-seq data using Scanpy and subsample for efficiency\nscRNAseq_path = \"path/to/scRNAseq.h5ad\"\nscRNAseq = sc.read(scRNAseq_path)\nsc.pp.subsample(scRNAseq, fraction=0.1)\n\n# Calculate gene cell type abundance embedding from scRNA-seq data\nfrom segger.utils import calculate_gene_celltype_abundance_embedding\n\ncelltype_column = \"celltype_column\"\ngene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n    scRNAseq, celltype_column\n)\n\n# Create a XeniumSample instance for spatial transcriptomics processing\nxenium_sample = XeniumSample()\n\n# Load transcripts and include the calculated cell type abundance embedding\nxenium_sample.load_transcripts(\n    base_path=raw_data_dir,\n    sample=sample_tag,\n    transcripts_filename=\"transcripts.parquet\",\n    file_format=\"parquet\",\n    additional_embeddings={\"cell_type_abundance\": gene_celltype_abundance_embedding},\n)\n\n# Set the embedding to \"cell_type_abundance\" to use it in further processing\nxenium_sample.set_embedding(\"cell_type_abundance\")\n\n# Load nuclei data to define boundaries\nnuclei_path = raw_data_dir / sample_tag / \"nucleus_boundaries.parquet\"\nxenium_sample.load_boundaries(path=nuclei_path, file_format=\"parquet\")\n\n# Build PyTorch Geometric (PyG) data from a tile of the dataset\ntile_pyg_data = xenium_sample.build_pyg_data_from_tile(\n    boundaries_df=xenium_sample.boundaries_df,\n    transcripts_df=xenium_sample.transcripts_df,\n    r_tx=20,\n    k_tx=20,\n    use_precomputed=False,\n    workers=1,\n)\n\n# Save dataset in processed format for segmentation\nxenium_sample.save_dataset_for_segger(\n    processed_dir=processed_data_dir,\n    x_size=360,\n    y_size=360,\n    d_x=180,\n    d_y=180,\n    margin_x=10,\n    margin_y=10,\n    compute_labels=False,\n    r_tx=5,\n    k_tx=5,\n    val_prob=0.1,\n    test_prob=0.2,\n    neg_sampling_ratio_approx=5,\n    sampling_rate=1,\n    num_workers=1,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#merscope-data","title":"Merscope Data","text":"<pre><code>from segger.data import MerscopeSample\nfrom pathlib import Path\n\n# Set up the file paths\nraw_data_dir = Path(\"path/to/merscope_outputs\")\nprocessed_data_dir = Path(\"path/to/processed_files\")\nsample_tag = \"sample_tag\"\n\n# Create a MerscopeSample instance for spatial transcriptomics processing\nmerscope_sample = MerscopeSample()\n\n# Load transcripts from a CSV file\nmerscope_sample.load_transcripts(\n    base_path=raw_data_dir,\n    sample=sample_tag,\n    transcripts_filename=\"transcripts.csv\",\n    file_format=\"csv\",\n)\n\n# Optionally load cell boundaries\ncell_boundaries_path = raw_data_dir / sample_tag / \"cell_boundaries.parquet\"\nmerscope_sample.load_boundaries(path=cell_boundaries_path, file_format=\"parquet\")\n\n# Filter transcripts based on specific criteria\nfiltered_transcripts = merscope_sample.filter_transcripts(\n    merscope_sample.transcripts_df\n)\n\n# Build PyTorch Geometric (PyG) data from a tile of the dataset\ntile_pyg_data = merscope_sample.build_pyg_data_from_tile(\n    boundaries_df=merscope_sample.boundaries_df,\n    transcripts_df=filtered_transcripts,\n    r_tx=15,\n    k_tx=15,\n    use_precomputed=True,\n    workers=2,\n)\n\n# Save dataset in processed format for segmentation\nmerscope_sample.save_dataset_for_segger(\n    processed_dir=processed_data_dir,\n    x_size=360,\n    y_size=360,\n    d_x=180,\n    d_y=180,\n    margin_x=10,\n    margin_y=10,\n    compute_labels=True,\n    r_tx=5,\n    k_tx=5,\n    val_prob=0.1,\n    test_prob=0.2,\n    neg_sampling_ratio_approx=3,\n    sampling_rate=1,\n    num_workers=2,\n)\n</code></pre>"},{"location":"user_guide/training/","title":"Training the <code>segger</code> Model","text":""},{"location":"user_guide/training/#the-model","title":"The Model","text":"<p>The <code>segger</code> model is a graph neural network designed to handle heterogeneous graphs with two primary node types: transcripts and nuclei or cell boundaries. It leverages attention-based graph convolutional layers to compute node embeddings and relationships in spatial transcriptomics data. The architecture includes an initial embedding layer for node feature transformation, multiple graph attention layers (GATv2Conv), and residual linear connections.</p>"},{"location":"user_guide/training/#model-architecture","title":"Model Architecture","text":"<ul> <li> <p>Input Node Features:    For input node features \\(\\mathbf{x}\\), the model distinguishes between transcript nodes and boundary (or nucleus) nodes.</p> </li> <li> <p>Transcript Nodes: If \\(\\mathbf{x}\\) is 1-dimensional (e.g., for tokenized transcript data), the model applies an embedding layer:</p> </li> </ul> \\[ \\mathbf{h}_{i}^{(0)} = \\text{nn.Embedding}(i) \\] <p>where \\(i\\) is the transcript token index.</p> <ul> <li>Nuclei or Cell Boundary Nodes: If \\(\\mathbf{x}\\) has multiple dimensions, the model applies a linear transformation:</li> </ul> \\[ \\mathbf{h}_{i}^{(0)} = \\mathbf{W} \\mathbf{x}_{i} \\] <p>where \\(\\mathbf{W}\\) is a learnable weight matrix.</p> <ul> <li>Graph Attention Layers (GATv2Conv):    The node embeddings are updated through multiple attention-based layers. The update for a node \\(i\\) at layer \\(l+1\\) is given by:</li> </ul> \\[ \\mathbf{h}_{i}^{(l+1)} = \\text{ReLU}\\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W}^{(l)} \\mathbf{h}_{j}^{(l)} \\right) \\] <p>where:    - \\(\\alpha_{ij}\\) is the attention coefficient between node \\(i\\) and node \\(j\\), computed as:</p> \\[ \\alpha_{ij} = \\frac{\\exp\\left( \\text{LeakyReLU}\\left( \\mathbf{a}^{\\top} \\left[\\mathbf{W}^{(l)} \\mathbf{h}_{i}^{(l)} || \\mathbf{W}^{(l)} \\mathbf{h}_{j}^{(l)}\\right] \\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left( \\text{LeakyReLU}\\left( \\mathbf{a}^{\\top} \\left[\\mathbf{W}^{(l)} \\mathbf{h}_{i}^{(l)} || \\mathbf{W}^{(l)} \\mathbf{h}_{k}^{(l)}\\right] \\right)\\right)} \\] <ul> <li> <p>\\(\\mathbf{a}\\) is a learnable attention vector.</p> </li> <li> <p>Residual Linear Connections:    After each attention layer, a residual connection is added via a linear transformation to stabilize learning:</p> </li> </ul> \\[ \\mathbf{h}_{i}^{(l+1)} = \\text{ReLU}\\left( \\mathbf{h}_{i}^{(l+1)} + \\mathbf{W}_{res} \\mathbf{h}_{i}^{(l)} \\right) \\] <p>where \\(\\mathbf{W}_{res}\\) is a residual weight matrix.</p> <ul> <li>L2 Normalization:    Finally, the embeddings are normalized using L2 normalization:</li> </ul> \\[ \\mathbf{h}_{i} = \\frac{\\mathbf{h}_{i}}{\\|\\mathbf{h}_{i}\\|} \\] <p>ensuring the final node embeddings have unit norm.</p>"},{"location":"user_guide/training/#heterogeneous-graph-transformation","title":"Heterogeneous Graph Transformation","text":"<p>In the next step, the <code>segger</code> model is transformed into a heterogeneous graph neural network using PyTorch Geometric's <code>to_hetero</code> function. This transformation enables the model to handle distinct node and edge types (transcripts and nuclei or cell boundaries) with separate mechanisms for modeling their relationships.</p>"},{"location":"user_guide/training/#usage","title":"Usage","text":"<p>To instantiate and run the <code>segger</code> model:</p> <pre><code>model = segger(\n    num_tx_tokens=5000,  # Number of unique 'tx' tokens\n    init_emb=32,  # Initial embedding dimension\n    hidden_channels=64,  # Number of hidden channels\n    num_mid_layers=2,  # Number of middle layers\n    out_channels=128,  # Number of output channels\n    heads=4,  # Number of attention heads\n)\n\noutput = model(x, edge_index)\n</code></pre> <p>Once transformed to a heterogeneous model and trained using PyTorch Lightning, the model can efficiently learn relationships between transcripts and nuclei or cell boundaries.</p>"},{"location":"user_guide/training/#training-the-heterogeneous-gnn-with-pytorch-lightning","title":"Training the heterogeneous GNN with <code>pytorch-lightning</code>","text":"<p>The training module makes use of PyTorch Lightning for efficient and scalable training, alongside PyTorch Geometric for processing the graph-based data. The module is built to handle multi-GPU setups and allows the flexibility to adjust hyperparameters, aggregation methods, and embedding sizes.</p> <p>The <code>SpatialTranscriptomicsDataset</code> class is used to load and manage spatial transcriptomics data stored in the format of PyTorch Geometric <code>Data</code> objects. It inherits from <code>InMemoryDataset</code> to load preprocessed datasets, ensuring efficient in-memory data handling for training and validation phases.</p>"},{"location":"user_guide/training/#example-training-command","title":"Example Training Command","text":"<pre><code>python scripts/train_model.py \\ \n  --train_dir path/to/train/tiles \\\n  --val_dir path/to/val/tiles \\\n  --batch_size_train 4 \\\n  --batch_size_val 4 \\\n  --num_tx_tokens 500 \\\n  --init_emb 8 \\\n  --hidden_channels 64 \\\n  --out_channels 16 \\\n  --heads 4 \\\n  --mid_layers 1 \\\n  --aggr sum \\\n  --accelerator cuda \\\n  --strategy auto \\\n  --precision 16-mixed \\\n  --devices 4 \\\n  --epochs 100 \\\n  --default_root_dir ./models/clean2\n</code></pre> <p>The <code>scripts/train_model.py</code> file can be found on the github repo. This example submits a job to train the <code>segger</code> model on 4 GPUs with a batch size of 4 for both training and validation, utilizing 16-bit mixed precision.</p>"},{"location":"user_guide/validation/","title":"Validating the segmentations","text":"<p>This module provides utilities for validating segmentation methods in single-cell transcriptomics, focusing on evaluating performance across metrics such as sensitivity, specificity, and spatial localization.</p>"},{"location":"user_guide/validation/#benchmarking-and-validation-of-segmentation-methods","title":"Benchmarking and Validation of Segmentation Methods","text":"<p>To rigorously evaluate segmentation performance, we use a suite of metrics grouped into four categories: General Statistics, Sensitivity, Spatial Localization, and Specificity and Contamination. These metrics provide a comprehensive framework for assessing the accuracy and precision of segmentation methods.</p>"},{"location":"user_guide/validation/#general-statistics","title":"General Statistics","text":"<ul> <li>Percent Assigned Transcripts: Measures the proportion of transcripts correctly assigned to cells.</li> </ul> \\[ \\text{Percent Assigned} = \\frac{N_{\\text{assigned}}}{N_{\\text{total}}} \\times 100 \\] <ul> <li>Transcript Density: Assesses transcript counts relative to cell size.</li> </ul> \\[ D = \\frac{\\text{transcript counts}}{\\text{cell area}} \\]"},{"location":"user_guide/validation/#sensitivity","title":"Sensitivity","text":"<ul> <li>F1 Score for Cell Type Purity: Evaluates how well a segmentation method can identify cells based on marker genes.</li> </ul> \\[ \\text{F1}_{\\text{purity}} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\] <ul> <li>Gene-specific Assignment Metrics: Measures the proportion of correctly assigned transcripts for a specific gene.</li> </ul> \\[ \\text{Percent Assigned}_g = \\frac{N_{\\text{assigned}}^g}{N_{\\text{total}}^g} \\times 100 \\]"},{"location":"user_guide/validation/#spatial-localization","title":"Spatial Localization","text":"<ul> <li>Percent Cytoplasmic and Percent Nucleus Transcripts: Evaluates the spatial distribution of transcripts within cells.</li> </ul> \\[ \\text{Percent Cytoplasmic} = \\frac{N_{\\text{cytoplasmic}}}{N_{\\text{assigned}}} \\times 100 \\] \\[ \\text{Percent Nucleus} = \\frac{N_{\\text{nucleus}}}{N_{\\text{assigned}}} \\times 100 \\] <ul> <li>Neighborhood Entropy: Measures the diversity of neighboring cell types.</li> </ul> \\[ E = -\\sum_{c} p(c) \\log(p(c)) \\]"},{"location":"user_guide/validation/#specificity-and-contamination","title":"Specificity and Contamination","text":"<ul> <li>Mutually Exclusive Co-expression Rate (MECR): Quantifies how mutually exclusive gene expression is across cells.</li> </ul> \\[ \\text{MECR}(g_1, g_2) = \\frac{P(g_1 \\cap g_2)}{P(g_1 \\cup g_2)} \\] <ul> <li>Contamination from Neighboring Cells: Assesses transcript contamination from adjacent cells.</li> </ul> \\[ C_{ij} = \\frac{\\sum_{k \\in \\text{neighbors}} m_{ik} \\cdot w_{kj}}{\\sum_{k \\in \\text{neighbors}} m_{ik}} \\]"},{"location":"user_guide/validation/#comparison-across-segmentation-methods","title":"Comparison Across Segmentation Methods","text":"<p>A correlation analysis is used to compare different segmentation methods based on metrics such as transcript count and cell area. The Comparison Metric is defined as:</p> \\[ \\text{Comparison Metric}(m_1, m_2) = \\frac{\\sum_{i=1}^{n} (M_1(i) - \\bar{M_1}) (M_2(i) - \\bar{M_2})}{\\sqrt{\\sum_{i=1}^{n} (M_1(i) - \\bar{M_1})^2 \\sum_{i=1}^{n} (M_2(i) - \\bar{M_2})^2}} \\]"}]}