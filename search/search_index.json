{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to segger","text":"<p>segger is a cutting-edge tool for cell segmentation in single-molecule spatial omics datasets. By leveraging graph neural networks (GNNs) and heterogeneous graphs, segger offers unmatched accuracy and scalability.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p> Installation Guide   Get started with installing segger on your machine.</p> </li> <li> <p> User Guide   Learn how to use segger for cell segmentation tasks.</p> </li> <li> <p> Command-Line Interface (CLI)   Explore the CLI options for working with segger.</p> </li> <li> <p> API Reference   Dive into the detailed API documentation for advanced usage.</p> </li> </ul>"},{"location":"#why-segger","title":"Why segger?","text":"<ul> <li> Highly parallelizable  \u2013 Optimized for multi-GPU environments</li> <li> Fast and efficient \u2013 Trains in a fraction of the time compared to alternatives</li> <li> Transfer learning \u2013 Easily adaptable to new datasets and technologies</li> </ul>"},{"location":"#challenges-in-segmentation","title":"Challenges in Segmentation","text":"<p>Spatial omics segmentation faces issues like:</p> <ul> <li>Over/Under-segmentation</li> <li>Transcript contamination</li> <li>Scalability limitations</li> </ul> <p>segger tackles these with a graph-based approach, achieving superior segmentation accuracy.</p>"},{"location":"#how-segger-works","title":"How segger Works","text":""},{"location":"#_1","title":"Home","text":""},{"location":"#powered-by","title":"Powered by","text":"<ul> <li> PyTorch Lightning &amp; PyTorch Geometric: Enables fast, efficient graph neural network (GNN) implementation for heterogeneous graphs.</li> <li> Dask: Scalable parallel processing and distributed task scheduling, ideal for handling large transcriptomic datasets.</li> <li> Shapely &amp; Geopandas: Utilized for spatial operations such as polygon creation, scaling, and spatial relationship computations.</li> <li> RAPIDS: Provides GPU-accelerated computation for tasks like k-nearest neighbors (KNN) graph construction.</li> <li> AnnData &amp; Scanpy: Efficient processing for single-cell datasets.</li> <li> SciPy: Facilitates spatial graph construction, including distance metrics and convex hull calculations for transcript clustering.</li> </ul>"},{"location":"#contributions","title":"Contributions","text":"<p>segger is open-source and welcomes contributions. Join us in advancing spatial omics segmentation!</p> <ul> <li> <p> Source Code GitHub</p> </li> <li> <p> Bug Tracker Report Issues</p> </li> <li> <p> Full Documentation API Reference</p> </li> </ul>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#segger-command-line-interface","title":"Segger Command Line Interface","text":""},{"location":"cli/#1-creating-a-dataset","title":"1. Creating a Dataset","text":"<p>The <code>create_dataset</code> command helps you to build a dataset for spatial transcriptomics. Here\u2019s a breakdown of the options available:</p> // Example: Creating a dataset for spatial transcriptomicspython3 src/segger/cli/create_dataset_fast.py \\    --base_dir /path/to/raw_data \\    --data_dir /path/to/save/processed_data \\    --sample_type xenium \\    --scrnaseq_file /path/to/scrnaseq_file \\    --celltype_column celltype_column_name \\    --k_bd 3 \\    --dist_bd 15.0 \\    --k_tx 3 \\    --dist_tx 5.0 \\    --tile_width 200 \\    --tile_height 200 \\    --neg_sampling_ratio 5.0 \\    --frac 1.0 \\    --val_prob 0.1 \\    --test_prob 0.2 \\    --n_workers 16"},{"location":"cli/#parameters","title":"Parameters","text":"Parameter Description Default Value <code>base_dir</code> Directory containing the raw dataset (e.g., transcripts, boundaries). - <code>data_dir</code> Directory to save the processed Segger dataset (in PyTorch Geometric format). - <code>sample_type</code> The sample type of the raw data, e.g., \"xenium\" or \"merscope\". None <code>scrnaseq_file</code> Path to the scRNAseq file. None <code>celltype_column</code> Column name for cell type annotations in the scRNAseq file. None <code>k_bd</code> Number of nearest neighbors for boundary nodes. <code>3</code> <code>dist_bd</code> Maximum distance for boundary neighbors. <code>15.0</code> <code>k_tx</code> Number of nearest neighbors for transcript nodes. <code>3</code> <code>dist_tx</code> Maximum distance for transcript neighbors. <code>5.0</code> <code>tile_width</code> Width of the tiles in pixels (ignored if <code>tile_size</code> is provided). None <code>tile_height</code> Height of the tiles in pixels (ignored if <code>tile_size</code> is provided). None <code>neg_sampling_ratio</code> Ratio of negative samples. <code>5.0</code> <code>frac</code> Fraction of the dataset to process. Useful for subsampling large datasets. <code>1.0</code> <code>val_prob</code> Proportion of the dataset used for validation split. <code>0.1</code> <code>test_prob</code> Proportion of the dataset used for testing split. <code>0.2</code> <code>n_workers</code> Number of workers for parallel processing. <code>1</code>"},{"location":"cli/#key-updates","title":"Key Updates","text":"<ul> <li>Faster Dataset Creation This method is way faster due to the use of ND-tree-based partitioning and parallel processing.</li> </ul> <p>Customizing Your Dataset</p> <ul> <li>dataset_type: Defines the type of spatial transcriptomics data. Currently, xenium and merscope are supported and have been tested.</li> <li>val_prob, test_prob: Control the dataset portions for validation and testing. Adjust based on your dataset size and evaluation needs.</li> <li>frac: Specifies the fraction of the dataset to process. Reducing <code>frac</code> can be useful when working with very large datasets, allowing for faster dataset creation by only processing a subset of the data.</li> </ul> <p>Faster Dataset Creation</p> <p>Increasing the number of workers (<code>n_workers</code>) can significantly accelerate the dataset creation process, especially for large datasets, by taking advantage of parallel processing across multiple CPU cores.</p> <p>Enhancing Segmentation Accuracy with scRNA-seq</p> <p>Incorporating single cell RNA sequencing (scRNA-seq) data as features can provide additional biological context, improving the accuracy of the segger model.</p>"},{"location":"cli/#2-training-a-model","title":"2. Training a Model","text":"<p>The <code>train</code> command initializes and trains a model using the dataset created. Here are the key parameters:</p> // Example: Training a segger modelpython3 src/segger/cli/train_model.py \\     --dataset_dir /path/to/saved/processed_data \\     --models_dir /path/to/save/model/checkpoints \\     --sample_tag first_training \\     --init_emb 8 \\     --hidden_channels 32 \\     --num_tx_tokens 500 \\     --out_channels 8 \\     --heads 2 \\     --num_mid_layers 2 \\     --batch_size 4 \\     --num_workers 2 \\     --accelerator cuda \\     --max_epochs 200 \\     --devices 4 \\     --strategy auto \\     --precision 16-mixed"},{"location":"cli/#parameters_1","title":"Parameters","text":"Parameter Description Default Value <code>dataset_dir</code> Directory containing the processed Segger dataset (in PyTorch Geometric format). - <code>models_dir</code> Directory to save the trained model and training logs. - <code>sample_tag</code> Tag used to identify the dataset during training. - <code>init_emb</code> Size of the embedding layer for input data. <code>8</code> <code>hidden_channels</code> Number of hidden units in each layer of the neural network. <code>32</code> <code>num_tx_tokens</code> Number of transcript tokens used during training. <code>500</code> <code>out_channels</code> Number of output channels from the model. <code>8</code> <code>heads</code> Number of attention heads used in graph attention layers. <code>2</code> <code>num_mid_layers</code> Number of mid layers in the model. <code>2</code> <code>batch_size</code> Number of samples to process per training batch. <code>4</code> <code>num_workers</code> Number of workers to use for parallel data loading. <code>2</code> <code>accelerator</code> Device used for training (e.g., <code>cuda</code> for GPU or <code>cpu</code>). <code>cuda</code> <code>max_epochs</code> Number of training epochs. <code>200</code> <code>devices</code> Number of devices (GPUs) to use during training. <code>4</code> <code>strategy</code> Strategy used for training (e.g., <code>ddp</code> for distributed training or <code>auto</code>). <code>auto</code> <code>precision</code> Precision used for training (e.g., <code>16-mixed</code> for mixed precision training). <code>16-mixed</code> <p>Optimizing training time</p> <ul> <li>devices: Use multiple GPUs by increasing the <code>devices</code> parameter to further accelerate training.</li> <li>batch_size: A larger batch size can speed up training, but requires more memory. Adjust based on your hardware capabilities.</li> <li>epochs: Increasing the number of epochs can improve model performance by allowing more learning cycles, but it will also extend the overall training time. Balance this based on your time constraints and hardware capacity.</li> </ul> <p>Ensure Correct CUDA and PyTorch Setup</p> <p>Before using the <code>--accelerator cuda</code> flag, ensure your system has CUDA installed and configured correctly. Also, check that the installed CUDA version is compatible with your PyTorch and PyTorch Geometric versions.</p>"},{"location":"cli/#3-making-predictions","title":"3. Making Predictions","text":"<p>After training the model, use the <code>predict</code> command to make predictions on new data:</p> // Example: Make predictions using a trained modelpython3 src/segger/cli/predict_fast.py \\     --segger_data_dir /path/to/saved/processed_data \\     --models_dir /path/to/saved/model/checkpoints \\     --benchmarks_dir /path/to/save/segmentation/results \\     --transcripts_file /path/to/raw_data/transcripts.parquet \\     --batch_size 1 \\     --num_workers 1 \\     --model_version 0 \\     --save_tag segger_embedding_1001 \\     --min_transcripts 5 \\     --cell_id_col segger_cell_id \\     --use_cc false \\     --knn_method cuda \\     --file_format anndata \\     --k_bd 4 \\     --dist_bd 12.0 \\     --k_tx 5 \\     --dist_tx 5.0"},{"location":"cli/#parameters_2","title":"Parameters","text":"Parameter Description Default Value <code>segger_data_dir</code> Directory containing the processed Segger dataset (in PyTorch Geometric format). - <code>models_dir</code> Directory containing the trained models. - <code>benchmarks_dir</code> Directory to save the segmentation results, including cell boundaries and associations. - <code>transcripts_file</code> Path to the transcripts.parquet file. - <code>batch_size</code> Number of samples to process per batch during prediction. <code>1</code> <code>num_workers</code> Number of workers for parallel data loading. <code>1</code> <code>model_version</code> Model version number to load for predictions, corresponding to the version from training logs. <code>0</code> <code>save_tag</code> Tag used to name and organize the segmentation results. <code>segger_embedding_1001</code> <code>min_transcripts</code> Minimum number of transcripts required for segmentation. <code>5</code> <code>cell_id_col</code> Column name for cell IDs in the output data. <code>segger_cell_id</code> <code>use_cc</code> Whether to use connected components for grouping transcripts without direct nucleus association. <code>False</code> <code>knn_method</code> Method for KNN computation (e.g., <code>cuda</code> for GPU-based computation). <code>cuda</code> <code>file_format</code> Format for the output segmentation data (e.g., <code>anndata</code>). <code>anndata</code> <code>k_bd</code> Number of nearest neighbors for boundary nodes. <code>4</code> <code>dist_bd</code> Maximum distance for boundary nodes. <code>12.0</code> <code>k_tx</code> Number of nearest neighbors for transcript nodes. <code>5</code> <code>dist_tx</code> Maximum distance for transcript nodes. <code>5.0</code> <p>Improving Prediction Pipeline</p> <ul> <li>batch_size: A larger batch size can speed up training, but requires more memory. Adjust based on your hardware capabilities.</li> <li>use_cc: Enabling connected component analysis can improve the accuracy of transcript assignments.</li> </ul> <p>Ensure Correct CUDA, cuVS, and PyTorch Setup</p> <p>Before using the <code>knn_method cuda</code> flag, ensure your system has CUDA installed and configured properly. Also, verify that the installed CUDA version is compatible with your cuPy, cuVS, PyTorch, and PyTorch Geometric versions.</p>"},{"location":"cli/#4-running-the-entire-pipeline","title":"4. Running the Entire Pipeline","text":"<p>The <code>submit_job.py</code> script allows you to run the complete Segger pipeline or specific stages like dataset creation, training, or prediction. The pipeline execution is determined by the configuration provided in a YAML file, supporting various environments like Docker, Singularity, and HPC systems (with LSF, Slurm support is planned).</p>"},{"location":"cli/#selecting-pipelines","title":"Selecting Pipelines","text":"<p>You can run the three stages\u2014dataset creation, training, and prediction\u2014sequentially or independently by specifying the pipelines in the YAML configuration file:</p> <pre><code>- `1` for dataset creation\n- `2` for model training\n- `3` for prediction\n</code></pre> <p>This allows you to run the full pipeline or just specific steps. Set the desired stages under the pipelines field in your YAML file.</p>"},{"location":"cli/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Use the following command to run the pipeline:</p> <pre><code>python3 submit_job.py --config_file=filename.yaml\n</code></pre> <ul> <li>If no <code>--config_file</code> is provided, the default <code>config.yaml</code> file will be used.</li> </ul>"},{"location":"cli/#5-containerization","title":"5. Containerization","text":"<p>For users who want a portable, containerized environment, segger supports both Docker and Singularity containers. These containers provide a consistent runtime environment with all dependencies pre-installed.</p>"},{"location":"cli/#using-docker","title":"Using Docker","text":"<p>You can pull the segger Docker image from Docker Hub with this command:</p> <pre><code>docker pull danielunyi42/segger_dev:cuda121\n</code></pre> <p>To run the pipeline in Docker, make sure your YAML configuration includes the following settings:</p> <ul> <li><code>use_singularity</code>: false</li> <li><code>use_lsf</code>: false</li> </ul> <p>Afterwards, run the pipeline inside the Docker container with the same <code>submit_job.py</code> command.</p>"},{"location":"cli/#using-singularity","title":"Using Singularity","text":"<p>For a Singularity environment, pull the image with:</p> <pre><code>singularity pull docker://danielunyi42/segger_dev:cuda121\n</code></pre> <p>Ensure <code>use_singularity: true</code> in the YAML file and specify the Singularity image file (e.g., <code>segger_dev_latest.sif</code>) in the <code>singularity_image</code> field.</p> <p>Containerization</p> <ul> <li>The segger Docker image currently supports CUDA 11.8 and CUDA 12.1.</li> </ul>"},{"location":"cli/#6-hpc-environments","title":"6. HPC Environments","text":"<p>Segger also supports HPC environments with LSF job scheduling. To run the pipeline on an HPC cluster using LSF, set <code>use_lsf: true</code> in your YAML configuration.</p> <p>If your HPC system supports Slurm, a similar setup is planned and will be introduced soon.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#segger-installation-guide","title":"segger Installation Guide","text":"<p>segger provides multiple installation options to suit your requirements. You can install it using:</p> <ul> <li>Virtual environments (recommended for most users)</li> <li>Containerized environments (Docker or Singularity)</li> <li>Editable mode from GitHub (for developers or users who want to modify the source code)</li> </ul> <p>Recommendation</p> <p>To avoid dependency conflicts, we recommend installing segger in a virtual environment or a container environment.</p> <p>segger requires CUDA 11 or CUDA 12 for GPU acceleration.</p>"},{"location":"installation/#installation-in-virtual-environment","title":"Installation in Virtual Environment","text":""},{"location":"installation/#using-venv","title":"Using <code>venv</code>","text":"<pre><code># Step 1: Create and activate the virtual environment.\npython3.10 -m venv segger-venv\nsource segger-venv/bin/activate\n\n# Step 2: Install segger with CUDA support.\npip install --upgrade pip\npip install .[cuda12]\n\n# Step 3: Verify the installation.\npython --version\npip show segger\n\n# step 4 [Optional]: If your system doesn't have a universally installed CUDA toolkit, you can link CuPy to PyTorch's CUDA runtime library.\nexport LD_LIBRARY_PATH=$(pwd)/segger-venv/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"installation/#using-conda","title":"Using <code>conda</code>","text":"<pre><code># Step 1: Create and activate the conda environment.\nconda create -n segger-env python=3.10\nconda activate segger-env\n\n# Step 2: Install segger with CUDA support.\npip install --upgrade pip\npip install .[cuda12]\n\n# Step 3: Verify the installation.\npython --version\npip show segger\n\n# Step 4 [Optional]: If your system doesn't have a universally installed CUDA toolkit, you can link CuPy to PyTorch's CUDA runtime library.\nexport LD_LIBRARY_PATH=$(conda info --base)/envs/segger-env/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"installation/#how-to-choose-between-cuda11-and-cuda12","title":"How to Choose Between <code>[cuda11]</code> and <code>[cuda12]</code>","text":"<ol> <li>Check Your NVIDIA Driver Version: Run <code>nvidia-smi</code>. Use <code>[cuda11]</code> for driver version \u2265 450.80.02 or <code>[cuda12]</code> for version \u2265 525.60.13.</li> <li>Check for a CUDA Toolkit: Run <code>nvcc --version</code>. If it outputs a CUDA version (11.x or 12.x), choose the corresponding <code>[cuda11]</code> or <code>[cuda12]</code>.</li> <li>Default to PyTorch CUDA Runtime: If CUDA toolkit is not installed, segger can use PyTorch's bundled CUDA runtime. You can link CuPy as shown in Step 4 of the venv/conda installation.</li> </ol>"},{"location":"installation/#installation-in-container-environment","title":"Installation in Container Environment","text":""},{"location":"installation/#using-docker","title":"Using <code>docker</code>","text":"<pre><code># Step 1: Pull the official Docker image.\ndocker pull danielunyi42/segger_dev:cuda121\n\n# Step 2: Run the Docker container with GPU support.\ndocker run --gpus all -it danielunyi42/segger_dev:cuda121\n</code></pre> <p>The official Docker image comes with all dependencies pre-installed, including the CUDA toolkit, PyTorch, and CuPy. The current images support CUDA 11.8 and CUDA 12.1, which can be specified in the image tag.</p>"},{"location":"installation/#using-singularity","title":"Using <code>singularity</code>","text":"<pre><code># Step 1: Pull the official Docker image.\nsingularity pull docker://danielunyi42/segger_dev:cuda121\n\n# Step 2: Run the Singularity container with GPU support.\nsingularity exec --nv segger_dev_cuda121.sif\n</code></pre> <p>The Singularity image is derived from the official Docker image and includes all pre-installed dependencies.</p>"},{"location":"installation/#directory-mapping-for-input-and-output-data","title":"Directory Mapping for Input and Output Data","text":"<p>Directory mapping allows:</p> <ul> <li>Access to input data (spatial transcriptomics datasets) from your local machine inside the container.</li> <li>Saving output data (segmentation results and logs) generated by segger to your local machine.</li> </ul> <p>Setting up directory mapping is really easy:</p> <ul> <li> <p>For Docker: <pre><code>docker run --gpus all -it -v /path/to/local/data:/workspace/data danielunyi42/segger_dev:cuda121\n</code></pre></p> </li> <li> <p>For Singularity: <pre><code>singularity exec --nv -B /path/to/local/data:/workspace/data segger_dev_cuda121.sif\n</code></pre></p> </li> <li>Place your input datasets in <code>/path/to/local/data</code> on your host machine.</li> <li>Inside the container, access these datasets from <code>/workspace/data</code>.</li> <li>Save results to <code>/workspace/data</code>, which will be available in <code>/path/to/local/data</code> on the host machine.</li> </ul>"},{"location":"installation/#editable-github-installation","title":"Editable GitHub installation","text":"<p>For developers or users who want to modify the source code:</p> <pre><code>git clone https://github.com/EliHei2/segger_dev.git\ncd segger_dev\npip install -e \".[cuda12]\"\n</code></pre> <p>Common Installation Issues</p> <ul> <li> <p>Python Version: Ensure you are using Python &gt;= 3.10. Check your Python version by running:   <pre><code>python --version\n</code></pre>   If your version is lower than 3.10, please upgrade Python.</p> </li> <li> <p>CUDA Compatibility (GPU): For GPU installations, verify that your system has the correct NVIDIA drivers installed. Run:   <pre><code>nvidia-smi\n</code></pre>   Ensure that the displayed CUDA version is compatible with your selected <code>[cuda11]</code> or <code>[cuda12]</code> extra.</p> <ul> <li>Minimum driver version for CUDA 11.x: <code>450.80.02</code></li> <li>Minimum driver version for CUDA 12.x: <code>525.60.13</code></li> </ul> </li> <li> <p>Permissions: If you encounter permission errors during installation, use the --user flag to install the package without requiring administrative privileges:   <pre><code>pip install --user .[cuda12]\n</code></pre>   Alternatively, consider using a virtual environment (venv or conda) to isolate the installation.</p> </li> <li> <p>Environment Configuration: Ensure that all required dependencies are installed in your environment.</p> </li> </ul>"},{"location":"_build/html/_sources/cli/","title":"Command-Line Interface (CLI)","text":"<p>Documentation for the Segger CLI.</p> <p>```python import click</p> <p>@click.group() def cli():     pass</p> <p>@cli.command() def create_dataset():     \"\"\"Create a new dataset.\"\"\"     pass</p> <p>if name == 'main':     cli()</p>"},{"location":"_build/html/_sources/installation/","title":"Installation Guide","text":"<p>This guide provides detailed instructions for installing the <code>segger</code> package. Whether you are installing for CPU or GPU, the instructions below will guide you through the process.</p> <pre><code>Ensure you are using `Python &gt;= 3.10` before starting the installation process.\n</code></pre>"},{"location":"_build/html/_sources/installation/#install-the-package","title":"Install the Package","text":""},{"location":"_build/html/_sources/installation/#from-source","title":"From Source","text":"<p>To install <code>segger</code> from the source code:</p> <ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/EliHei2/segger_dev.git\n</code></pre></p> </li> <li> <p>Navigate to the project directory:     <pre><code>cd segger_dev\n</code></pre></p> </li> <li> <p>Install the package:     <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"_build/html/_sources/installation/#cpu-and-gpu-installation-from-pypi","title":"CPU and GPU Installation from PyPI","text":"<p><pre><code>```{tab-item} CPU Installation\nIf you only need CPU support, use the following command:\n\n```bash\npip install segger\n</code></pre> This will install the package without any GPU-related dependencies.</p> <pre><code>This is ideal for environments where GPU support is not required or available.\n</code></pre> <p>```{tab-item} GPU Installation For installations with GPU support, use the following command:</p> <pre><code>pip install segger[gpu]\n</code></pre> <p>This includes the necessary dependencies for CUDA-enabled GPUs.</p> <pre><code>Ensure your machine has the appropriate CUDA drivers and NVIDIA libraries installed.\n</code></pre> <pre><code>\n</code></pre> <pre><code>## Optional Dependencies\n\nThe following sections describe optional dependencies you can install for specific features.\n\n```{tab-set}\n```{tab-item} Torch Geometric\n\nTo install `torch-geometric` related dependencies, run:\n\n```bash\npip install segger[torch-geometric]\n</code></pre> <p>Follow the additional steps on the PyTorch Geometric installation page to ensure proper setup.</p> <pre><code>Ensure you install `torch-geometric` with the correct CUDA version for GPU support.\n</code></pre> <p>```{tab-item} Multiprocessing</p> <p>To install <code>segger</code> with multiprocessing support, use the following command:</p> <pre><code>pip install segger[multiprocessing]\n</code></pre> <p>This will enable multi-core parallel processing features.</p> <pre><code>## Platform-Specific Installations\n\nBelow are instructions for installing `segger` on different operating systems.\n\n```{tab-set}\n```{tab-item} Linux\n\nOn Linux, use the following command to install the package:\n\n```bash\npip install segger\n</code></pre> <p>For GPU support on Linux, ensure you have the necessary CUDA drivers installed:</p> <pre><code>pip install segger[gpu]\n</code></pre> <p>```{tab-item} macOS</p> <p>To install on macOS, use the following command:</p> <pre><code>pip install segger\n</code></pre> <p>Note that macOS does not natively support CUDA, so GPU support is not available.</p> <pre><code>If you require GPU support, we recommend using Linux or Windows.\n</code></pre> <p>```{tab-item} Windows</p> <p>To install on Windows, use the following command:</p> <pre><code>pip install segger\n</code></pre> <p>For GPU support on Windows:</p> <pre><code>pip install segger[gpu]\n</code></pre> <pre><code>Ensure your CUDA drivers are installed on Windows by using `nvidia-smi`.\n</code></pre> <pre><code>## Installation for Developers\n\nFor developers looking to contribute or work with `segger` in a development environment, you can install the package with development dependencies.\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/EliHei2/segger_dev.git\n    ```\n\n2. Navigate to the project directory:\n    ```bash\n    cd segger_dev\n    ```\n\n3. Install the package with development dependencies:\n    ```bash\n    pip install -e .[dev]\n    ```\n\nThis will install additional dependencies such as `pytest`, `black`, `flake8`, and more.\n\n## Common Installation Issues\n\n```{tip}\nIf you encounter installation issues, ensure that you are using the correct Python version (`&gt;= 3.10`) and that you have the necessary permissions to install packages on your system.\n</code></pre> <p>Some common errors include:</p> <ul> <li>Missing Python version: Ensure you are using <code>Python &gt;= 3.10</code>.</li> <li>Insufficient permissions: Use <code>pip install --user</code> if you do not have admin permissions.</li> <li>Conflicting CUDA drivers: Ensure you have compatible CUDA versions installed if using GPU support.</li> </ul> <p>For further troubleshooting, please refer to the official documentation.</p> <p>For more information, visit the official GitHub repository.</p> <p>```</p>"},{"location":"api/","title":"API Reference","text":"<p>This page contains auto-generated API reference documentation [^1].</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>CLI</li> <li>Data</li> <li>Validation</li> <li>Predict</li> <li>Train Model</li> </ul>"},{"location":"api/data/","title":"segger.data","text":"<p>The <code>segger.data</code> module in Segger is designed to facilitate data preparation for spatial transcriptomics datasets, focusing on Xenium and Merscope. It provides a unified, scalable interface for handling large datasets, performing boundary and transcript filtering, and preparing data for graph-based deep learning models.</p>"},{"location":"api/data/#submodules","title":"Submodules","text":"<ul> <li>Constants: Contains predefined constants for spatial data processing.</li> <li>IO: Provides utilities for input/output operations, supporting various data formats.</li> <li>Utils: Miscellaneous utility functions for processing and analysis.</li> </ul>"},{"location":"api/data/#key-features","title":"Key Features","text":"<ul> <li>Lazy Loading: Efficiently handles large datasets using Dask, avoiding memory bottlenecks.</li> <li>Flexible Tiling: Spatially segments datasets into tiles, optimizing for parallel processing.</li> <li>Graph Construction: Converts spatial data into PyTorch Geometric (PyG) graph formats for GNN-based models.</li> <li>Boundary Handling: Processes spatial polygons and computes transcript overlaps.</li> <li>Dataset-agnostic API: Unified API for multiple spatial transcriptomics technologies, such as Xenium and Merscope.</li> </ul>"},{"location":"api/data/#api-documentation","title":"API Documentation","text":"<p>Data module for Segger.</p> <p>Contains utilities for handling and processing spatial transcriptomics data.</p>"},{"location":"api/data/#segger.data.MerscopeSample","title":"MerscopeSample","text":"<pre><code>MerscopeSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(transcripts_df, transcripts_radius, boundaries_graph, embedding_df, MerscopeKeys)\n</code></pre>"},{"location":"api/data/#segger.data.MerscopeSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on specific criteria for Merscope using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <p>dd.DataFrame The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <p>float, optional The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame The filtered Dask DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def filter_transcripts(self, transcripts_df: dd.DataFrame, min_qv: float = 20.0) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on specific criteria for Merscope using Dask.\n\n    Parameters:\n        transcripts_df : dd.DataFrame\n            The Dask DataFrame containing transcript data.\n        min_qv : float, optional\n            The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame\n            The filtered Dask DataFrame.\n    \"\"\"\n    # Add custom Merscope-specific filtering logic if needed\n    # For now, apply only the quality value filter\n    return transcripts_df[transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv]\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/utils.py</code> <pre><code>def __init__(\n    self, root: str, transform: Callable = None, pre_transform: Callable = None, pre_filter: Callable = None\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample","title":"SpatialTranscriptomicsSample","text":"<pre><code>SpatialTranscriptomicsSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, keys=None, verbose=True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Initialize the SpatialTranscriptomicsSample class.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>A DataFrame containing transcript data.</p> <code>None</code> <code>transcripts_radius</code> <code>int</code> <p>Radius for transcripts in the analysis.</p> <code>10</code> <code>boundaries_graph</code> <code>bool</code> <p>Whether to include boundaries (e.g., nucleus, cell) graph information.</p> <code>False</code> <code>keys</code> <code>Dict</code> <p>The enum class containing key mappings specific to the dataset.</p> <code>None</code> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: pd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    keys: Dict = None,\n    verbose: bool = True,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsSample class.\n\n    Args:\n        transcripts_df (pd.DataFrame, optional): A DataFrame containing transcript data.\n        transcripts_radius (int, optional): Radius for transcripts in the analysis.\n        boundaries_graph (bool, optional): Whether to include boundaries (e.g., nucleus, cell) graph information.\n        keys (Dict, optional): The enum class containing key mappings specific to the dataset.\n    \"\"\"\n    self.transcripts_df = transcripts_df\n    self.transcripts_radius = transcripts_radius\n    self.boundaries_graph = boundaries_graph\n    self.keys = keys\n    self.embedding_df = embedding_df\n    self.current_embedding = \"token\"\n    self.verbose = verbose\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.build_pyg_data_from_tile","title":"build_pyg_data_from_tile","text":"<pre><code>build_pyg_data_from_tile(boundaries_df, transcripts_df, r_tx=5.0, k_tx=3, method='kd_tree', gpu=False, workers=1, scale_boundaries=1.0)\n</code></pre> <p>Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundaries data (e.g., nucleus, cell).</p> required <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcripts data.</p> required <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>HeteroData</code> <code>HeteroData</code> <p>PyG Heterogeneous Data object.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def build_pyg_data_from_tile(\n    self,\n    boundaries_df: dd.DataFrame,\n    transcripts_df: dd.DataFrame,\n    r_tx: float = 5.0,\n    k_tx: int = 3,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n    scale_boundaries: float = 1.0,\n) -&gt; HeteroData:\n    \"\"\"\n    Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): Dask DataFrame containing boundaries data (e.g., nucleus, cell).\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcripts data.\n        r_tx (float): Radius for building the transcript-to-transcript graph.\n        k_tx (int): Number of nearest neighbors for the tx-tx graph.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        HeteroData: PyG Heterogeneous Data object.\n    \"\"\"\n    # Initialize the PyG HeteroData object\n    data = HeteroData()\n\n    # Lazily compute boundaries geometries using Dask\n    if self.verbose:\n        print(\"Computing boundaries geometries...\")\n    bd_gdf = self.compute_boundaries_geometries(boundaries_df, scale_factor=scale_boundaries)\n    bd_gdf = bd_gdf[bd_gdf[\"geometry\"].notnull()]\n\n    # Add boundary node data to PyG HeteroData lazily\n    data[\"bd\"].id = bd_gdf[self.keys.CELL_ID.value].values\n    data[\"bd\"].pos = torch.as_tensor(bd_gdf[[\"centroid_x\", \"centroid_y\"]].values.astype(float))\n\n    if data[\"bd\"].pos.isnan().any():\n        raise ValueError(data[\"bd\"].id[data[\"bd\"].pos.isnan().any(1)])\n\n    bd_x = bd_gdf.iloc[:, 4:]\n    data[\"bd\"].x = torch.as_tensor(bd_x.to_numpy(), dtype=torch.float32)\n\n    # Extract the transcript coordinates lazily\n    if self.verbose:\n        print(\"Preparing transcript features and positions...\")\n    x_xyz = transcripts_df[[self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]].to_numpy()\n    data[\"tx\"].id = torch.as_tensor(transcripts_df[self.keys.TRANSCRIPTS_ID.value].values.astype(int))\n    data[\"tx\"].pos = torch.tensor(x_xyz, dtype=torch.float32)\n\n    # Lazily prepare transcript embeddings (if available)\n    if self.verbose:\n        print(\"Preparing transcript embeddings..\")\n    token_encoding = self.tx_encoder.transform(transcripts_df[self.keys.FEATURE_NAME.value])\n    transcripts_df[\"token\"] = token_encoding  # Store the integer tokens in the 'token' column\n    data[\"tx\"].token = torch.as_tensor(token_encoding).int()\n    # Handle additional embeddings lazily as well\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        embeddings = delayed(lambda df: self.embedding_df.loc[df[self.keys.FEATURE_NAME.value].values].values)(\n            transcripts_df\n        )\n    else:\n        embeddings = token_encoding\n    if hasattr(embeddings, \"compute\"):\n        embeddings = embeddings.compute()\n    x_features = torch.as_tensor(embeddings).int()\n    data[\"tx\"].x = x_features\n\n    # Check if the overlap column exists, if not, compute it lazily using Dask\n    if self.keys.OVERLAPS_BOUNDARY.value not in transcripts_df.columns:\n        if self.verbose:\n            print(f\"Computing overlaps for transcripts...\")\n        transcripts_df = self.compute_transcript_overlap_with_boundaries(\n            transcripts_df, polygons_gdf=bd_gdf, scale_factor=1.0\n        )\n\n    # Connect transcripts with their corresponding boundaries (e.g., nuclei, cells)\n    if self.verbose:\n        print(\"Connecting transcripts with boundaries...\")\n    overlaps = transcripts_df[self.keys.OVERLAPS_BOUNDARY.value].values\n    valid_cell_ids = bd_gdf[self.keys.CELL_ID.value].values\n    ind = np.where(overlaps &amp; transcripts_df[self.keys.CELL_ID.value].isin(valid_cell_ids))[0]\n    tx_bd_edge_index = np.column_stack(\n        (ind, np.searchsorted(valid_cell_ids, transcripts_df.iloc[ind][self.keys.CELL_ID.value]))\n    )\n\n    # Add transcript-boundary edge index to PyG HeteroData\n    data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.as_tensor(tx_bd_edge_index.T, dtype=torch.long)\n\n    # Compute transcript-to-transcript (tx-tx) edges using Dask (lazy computation)\n    if self.verbose:\n        print(\"Computing tx-tx edges...\")\n    tx_positions = transcripts_df[[self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]].values\n    delayed_tx_edge_index = delayed(get_edge_index)(\n        tx_positions, tx_positions, k=k_tx, dist=r_tx, method=method, gpu=gpu, workers=workers\n    )\n    tx_edge_index = delayed_tx_edge_index.compute()\n\n    # Add the tx-tx edge index to the PyG HeteroData object\n    data[\"tx\", \"neighbors\", \"tx\"].edge_index = torch.as_tensor(tx_edge_index.T, dtype=torch.long)\n\n    if self.verbose:\n        print(\"Finished building PyG data for the tile.\")\n    return data\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.compute_boundaries_geometries","title":"compute_boundaries_geometries","text":"<pre><code>compute_boundaries_geometries(boundaries_df=None, polygons_gdf=None, scale_factor=1.0, area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>The dataframe containing boundaries data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <code>area</code> <code>bool</code> <p>Whether to compute area.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>Whether to compute convexity.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>Whether to compute elongation.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>Whether to compute circularity.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def compute_boundaries_geometries(\n    self,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.\n\n    Parameters:\n        boundaries_df (dd.DataFrame, optional): The dataframe containing boundaries data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n        area (bool, optional): Whether to compute area.\n        convexity (bool, optional): Whether to compute convexity.\n        elongation (bool, optional): Whether to compute elongation.\n        circularity (bool, optional): Whether to compute circularity.\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\")\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        if self.verbose:\n            print(\n                f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\"\n            )\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    # Check if the generated polygons_gdf is empty\n    if polygons_gdf.shape[0] == 0:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(f\"Polygons are available. Proceeding with geometrical computations.\")\n\n    # Compute additional geometrical properties\n    polygons = polygons_gdf.geometry\n\n    # Compute additional geometrical properties\n    if area:\n        if self.verbose:\n            print(\"Computing area...\")\n        polygons_gdf[\"area\"] = polygons.area\n    if convexity:\n        if self.verbose:\n            print(\"Computing convexity...\")\n        polygons_gdf[\"convexity\"] = polygons.convex_hull.area / polygons.area\n    if elongation:\n        if self.verbose:\n            print(\"Computing elongation...\")\n        r = polygons.minimum_rotated_rectangle()\n        polygons_gdf[\"elongation\"] = (r.length * r.length) / r.area\n    if circularity:\n        if self.verbose:\n            print(\"Computing circularity...\")\n        r = polygons_gdf.minimum_bounding_radius()\n        polygons_gdf[\"circularity\"] = polygons.area / (r * r)\n\n    if self.verbose:\n        print(\"Geometrical computations completed.\")\n\n    return polygons_gdf.reset_index(drop=True)\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.compute_transcript_overlap_with_boundaries","title":"compute_transcript_overlap_with_boundaries","text":"<pre><code>compute_transcript_overlap_with_boundaries(transcripts_df, boundaries_df=None, polygons_gdf=None, scale_factor=1.0)\n</code></pre> <p>Computes the overlap of transcript locations with scaled boundary polygons and assigns corresponding cell IDs to the transcripts using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcript data.</p> required <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def compute_transcript_overlap_with_boundaries(\n    self,\n    transcripts_df: dd.DataFrame,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Computes the overlap of transcript locations with scaled boundary polygons\n    and assigns corresponding cell IDs to the transcripts using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcript data.\n        boundaries_df (dd.DataFrame, optional): Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\")\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    if polygons_gdf.empty:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(f\"Polygons are available. Proceeding with overlap computation.\")\n\n    # Create a delayed function to check if a point is within any polygon\n    def check_overlap(transcript, polygons_gdf):\n        x = transcript[self.keys.TRANSCRIPTS_X.value]\n        y = transcript[self.keys.TRANSCRIPTS_Y.value]\n        point = Point(x, y)\n\n        overlap = False\n        cell_id = None\n\n        # Check for point containment lazily within polygons\n        for _, polygon in polygons_gdf.iterrows():\n            if polygon.geometry.contains(point):\n                overlap = True\n                cell_id = polygon[self.keys.CELL_ID.value]\n                break\n\n        return overlap, cell_id\n\n    # Apply the check_overlap function in parallel to each row using Dask's map_partitions\n    if self.verbose:\n        print(f\"Starting overlap computation for transcripts with the boundary polygons.\")\n    if isinstance(transcripts_df, pd.DataFrame):\n        # luca: I found this bug here\n        warnings.warn(\"BUG! This function expects Dask DataFrames, not Pandas DataFrames.\")\n        # if we want to really have the below working in parallel, we need to add n_partitions&gt;1 here\n        transcripts_df = dd.from_pandas(transcripts_df, npartitions=1)\n        transcripts_df.compute().columns\n    transcripts_df = transcripts_df.map_partitions(\n        lambda df: df.assign(\n            **{\n                self.keys.OVERLAPS_BOUNDARY.value: df.apply(\n                    lambda row: delayed(check_overlap)(row, polygons_gdf)[0], axis=1\n                ),\n                self.keys.CELL_ID.value: df.apply(lambda row: delayed(check_overlap)(row, polygons_gdf)[1], axis=1),\n            }\n        )\n    )\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.create_scaled_polygon","title":"create_scaled_polygon  <code>staticmethod</code>","text":"<pre><code>create_scaled_polygon(group, scale_factor, keys)\n</code></pre> <p>Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>Group of boundary coordinates (for a specific cell).</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons.</p> required <code>keys</code> <code>Dict or Enum</code> <p>A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>@staticmethod\ndef create_scaled_polygon(group: pd.DataFrame, scale_factor: float, keys) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.\n\n    Parameters:\n        group (pd.DataFrame): Group of boundary coordinates (for a specific cell).\n        scale_factor (float): The factor by which to scale the polygons.\n        keys (Dict or Enum): A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.\n    \"\"\"\n    # Extract coordinates and cell ID from the group using keys\n    x_coords = group[keys[\"vertex_x\"]]\n    y_coords = group[keys[\"vertex_y\"]]\n    cell_id = group[keys[\"cell_id\"]].iloc[0]\n\n    # Ensure there are at least 3 points to form a polygon\n    if len(x_coords) &gt;= 3:\n\n        polygon = Polygon(zip(x_coords, y_coords))\n        if polygon.is_valid and not polygon.is_empty:\n            # Scale the polygon by the provided factor\n            scaled_polygon = polygon.buffer(scale_factor)\n            if scaled_polygon.is_valid and not scaled_polygon.is_empty:\n                return gpd.GeoDataFrame(\n                    {\"geometry\": [scaled_polygon], keys[\"cell_id\"]: [cell_id]}, geometry=\"geometry\", crs=\"EPSG:4326\"\n                )\n    # Return an empty GeoDataFrame if no valid polygon is created\n    return gpd.GeoDataFrame({\"geometry\": [None], keys[\"cell_id\"]: [cell_id]}, geometry=\"geometry\", crs=\"EPSG:4326\")\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.filter_transcripts","title":"filter_transcripts  <code>abstractmethod</code>","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Abstract method to filter transcripts based on dataset-specific criteria.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>@abstractmethod\ndef filter_transcripts(self, transcripts_df: pd.DataFrame, min_qv: float = 20.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Abstract method to filter transcripts based on dataset-specific criteria.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.generate_and_scale_polygons","title":"generate_and_scale_polygons","text":"<pre><code>generate_and_scale_polygons(boundaries_df, scale_factor=1.0)\n</code></pre> <p>Generate and scale polygons from boundary coordinates using Dask. Keeps class structure intact by using static method for the core polygon generation.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>DataFrame containing boundary coordinates.</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def generate_and_scale_polygons(self, boundaries_df: dd.DataFrame, scale_factor: float = 1.0) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Generate and scale polygons from boundary coordinates using Dask.\n    Keeps class structure intact by using static method for the core polygon generation.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): DataFrame containing boundary coordinates.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.\n    \"\"\"\n    # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n\n    # Extract required columns from self.keys\n    cell_id_column = self.keys.CELL_ID.value\n    vertex_x_column = self.keys.BOUNDARIES_VERTEX_X.value\n    vertex_y_column = self.keys.BOUNDARIES_VERTEX_Y.value\n\n    create_polygon = self.create_scaled_polygon\n    # Use a lambda to wrap the static method call and avoid passing the function object directly to Dask\n    polygons_ddf = boundaries_df.groupby(cell_id_column).apply(\n        lambda group: create_polygon(\n            group=group,\n            scale_factor=scale_factor,\n            keys={  # Pass keys as a dict for the lambda function\n                \"vertex_x\": vertex_x_column,\n                \"vertex_y\": vertex_y_column,\n                \"cell_id\": cell_id_column,\n            },\n        )\n    )\n\n    # Lazily compute centroids for each polygon\n    if self.verbose:\n        print(\"Adding centroids to the polygons...\")\n    polygons_ddf[\"centroid_x\"] = polygons_ddf.geometry.centroid.x\n    polygons_ddf[\"centroid_y\"] = polygons_ddf.geometry.centroid.y\n\n    polygons_ddf = polygons_ddf.drop_duplicates()\n    # polygons_ddf = polygons_ddf.to_crs(\"EPSG:3857\")\n\n    return polygons_ddf\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.load_boundaries","title":"load_boundaries","text":"<pre><code>load_boundaries(path, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load boundaries data lazily using Dask, filtering by the specified bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the boundaries file.</p> required <code>file_format</code> <code>str</code> <p>Format of the file to load. Only 'parquet' is supported in this refactor.</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered boundaries DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def load_boundaries(\n    self,\n    path: Path,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load boundaries data lazily using Dask, filtering by the specified bounding box.\n\n    Parameters:\n        path (Path): Path to the boundaries file.\n        file_format (str, optional): Format of the file to load. Only 'parquet' is supported in this refactor.\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered boundaries DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(f\"Unsupported file format: {file_format}\")\n\n    self.boundaries_path = path\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.BOUNDARIES_VERTEX_X.value,\n        self.keys.BOUNDARIES_VERTEX_Y.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&gt;=\", x_min),\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&lt;=\", x_max),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&gt;=\", y_min),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(path).columns)\n    if \"geometry\" in columns:\n        bbox = (x_min, y_min, x_max, y_max)\n        # TODO: check that SpatialData objects write the \"bbox covering metadata\" to the parquet file\n        gdf = dgpd.read_parquet(path, bbox=bbox)\n        id_col, x_col, y_col = (\n            self.keys.CELL_ID.value,\n            self.keys.BOUNDARIES_VERTEX_X.value,\n            self.keys.BOUNDARIES_VERTEX_Y.value,\n        )\n\n        # Function to expand each polygon into a list of vertices\n        def expand_polygon(row):\n            expanded_data = []\n            polygon = row[\"geometry\"]\n            if polygon.geom_type == \"Polygon\":\n                exterior_coords = polygon.exterior.coords\n                for x, y in exterior_coords:\n                    expanded_data.append({id_col: row.name, x_col: x, y_col: y})\n            else:\n                # Instead of expanding the gdf and then having code later to recreate it (when computing the pyg graph)\n                # we could directly have this function returning a Dask GeoDataFrame. This means that we don't need\n                # to implement this else black\n                raise ValueError(f\"Unsupported geometry type: {polygon.geom_type}\")\n            return expanded_data\n\n        # Apply the function to each partition and collect results\n        def process_partition(df):\n            expanded_data = [expand_polygon(row) for _, row in df.iterrows()]\n            # Flatten the list of lists\n            flattened_data = [item for sublist in expanded_data for item in sublist]\n            return pd.DataFrame(flattened_data)\n\n        # Use map_partitions to apply the function and convert it into a Dask DataFrame\n        boundaries_df = gdf.map_partitions(process_partition, meta={id_col: str, x_col: float, y_col: float})\n    else:\n        boundaries_df = dd.read_parquet(path, columns=columns_to_read, filters=filters)\n\n        # Convert the cell IDs to strings lazily\n        boundaries_df[self.keys.CELL_ID.value] = boundaries_df[self.keys.CELL_ID.value].apply(\n            lambda x: str(x) if pd.notnull(x) else None, meta=(\"cell_id\", \"object\")\n        )\n\n    if self.verbose:\n        print(f\"Loaded boundaries from '{path}' within bounding box ({x_min}, {x_max}, {y_min}, {y_max}).\")\n\n    return boundaries_df\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.load_transcripts","title":"load_transcripts","text":"<pre><code>load_transcripts(base_path=None, sample=None, transcripts_filename=None, path=None, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load transcripts from a Parquet file using Dask for efficient chunked processing, only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>The base directory path where samples are stored.</p> <code>None</code> <code>sample</code> <code>str</code> <p>The sample name or identifier.</p> <code>None</code> <code>transcripts_filename</code> <code>str</code> <p>The filename of the transcripts file (default is derived from the dataset keys).</p> <code>None</code> <code>path</code> <code>Path</code> <p>Specific path to the transcripts file.</p> <code>None</code> <code>file_format</code> <code>str</code> <p>Format of the file to load (default is 'parquet').</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered transcripts DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def load_transcripts(\n    self,\n    base_path: Path = None,\n    sample: str = None,\n    transcripts_filename: str = None,\n    path: Path = None,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n    # additional_embeddings: Optional[Dict[str, pd.DataFrame]] = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load transcripts from a Parquet file using Dask for efficient chunked processing,\n    only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.\n\n    Parameters:\n        base_path (Path, optional): The base directory path where samples are stored.\n        sample (str, optional): The sample name or identifier.\n        transcripts_filename (str, optional): The filename of the transcripts file (default is derived from the dataset keys).\n        path (Path, optional): Specific path to the transcripts file.\n        file_format (str, optional): Format of the file to load (default is 'parquet').\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered transcripts DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(\"This version only supports parquet files with Dask.\")\n\n    # Set the file path for transcripts\n    transcripts_filename = transcripts_filename or self.keys.TRANSCRIPTS_FILE.value\n    file_path = path or (base_path / sample / transcripts_filename)\n    self.transcripts_path = file_path\n\n    # Set metadata\n    # self.set_metadata()\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Check for available columns in the file's metadata (without loading the data)\n    parquet_metadata = dd.read_parquet(file_path, meta_only=True)\n    available_columns = parquet_metadata.columns\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.TRANSCRIPTS_ID.value,\n        self.keys.TRANSCRIPTS_X.value,\n        self.keys.TRANSCRIPTS_Y.value,\n        self.keys.FEATURE_NAME.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Check if the QUALITY_VALUE key exists in the dataset, and add it to the columns list if present\n    if self.keys.QUALITY_VALUE.value in available_columns:\n        columns_to_read.append(self.keys.QUALITY_VALUE.value)\n\n    if self.keys.OVERLAPS_BOUNDARY.value in available_columns:\n        columns_to_read.append(self.keys.OVERLAPS_BOUNDARY.value)\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.TRANSCRIPTS_X.value, \"&gt;=\", x_min),\n        (self.keys.TRANSCRIPTS_X.value, \"&lt;=\", x_max),\n        (self.keys.TRANSCRIPTS_Y.value, \"&gt;=\", y_min),\n        (self.keys.TRANSCRIPTS_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(file_path).columns)\n    transcripts_df = dd.read_parquet(file_path, columns=columns_to_read, filters=filters).compute()\n\n    # Convert transcript and cell IDs to strings lazily\n    transcripts_df[self.keys.TRANSCRIPTS_ID.value] = transcripts_df[self.keys.TRANSCRIPTS_ID.value].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n    transcripts_df[self.keys.CELL_ID.value] = transcripts_df[self.keys.CELL_ID.value].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n\n    # Convert feature names from bytes to strings if necessary\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[self.keys.FEATURE_NAME.value].astype(str)\n\n    # Apply dataset-specific filtering (e.g., quality filtering for Xenium)\n    transcripts_df = self.filter_transcripts(transcripts_df)\n\n    # Handle additional embeddings if provided\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        valid_genes = self.embedding_df.index\n        # Lazily count the number of rows in the DataFrame before filtering\n        initial_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        # Filter the DataFrame lazily based on valid genes from embeddings\n        transcripts_df = transcripts_df[transcripts_df[self.keys.FEATURE_NAME.value].isin(valid_genes)]\n        final_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        if self.verbose:\n            print(f\"Dropped {initial_count - final_count} transcripts not found in embedding.\")\n\n    # Ensure that the 'OVERLAPS_BOUNDARY' column is boolean if it exists\n    if self.keys.OVERLAPS_BOUNDARY.value in transcripts_df.columns:\n        transcripts_df[self.keys.OVERLAPS_BOUNDARY.value] = transcripts_df[\n            self.keys.OVERLAPS_BOUNDARY.value\n        ].astype(bool)\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.save_dataset_for_segger","title":"save_dataset_for_segger","text":"<pre><code>save_dataset_for_segger(processed_dir, x_size=1000, y_size=1000, d_x=900, d_y=900, margin_x=None, margin_y=None, compute_labels=True, r_tx=5, k_tx=3, val_prob=0.1, test_prob=0.2, neg_sampling_ratio_approx=5, sampling_rate=1, num_workers=1, scale_boundaries=1.0, method='kd_tree', gpu=False, workers=1)\n</code></pre> <p>Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.</p> <p>Parameters:</p> Name Type Description Default <code>processed_dir</code> <code>Path</code> <p>Directory to save the processed dataset.</p> required <code>x_size</code> <code>float</code> <p>Width of each tile.</p> <code>1000</code> <code>y_size</code> <code>float</code> <p>Height of each tile.</p> <code>1000</code> <code>d_x</code> <code>float</code> <p>Step size in the x direction for tiles.</p> <code>900</code> <code>d_y</code> <code>float</code> <p>Step size in the y direction for tiles.</p> <code>900</code> <code>margin_x</code> <code>float</code> <p>Margin in the x direction to include transcripts.</p> <code>None</code> <code>margin_y</code> <code>float</code> <p>Margin in the y direction to include transcripts.</p> <code>None</code> <code>compute_labels</code> <code>bool</code> <p>Whether to compute edge labels for tx_belongs_bd edges.</p> <code>True</code> <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>val_prob</code> <code>float</code> <p>Probability of assigning a tile to the validation set.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Probability of assigning a tile to the test set.</p> <code>0.2</code> <code>neg_sampling_ratio_approx</code> <code>float</code> <p>Approximate ratio of negative samples.</p> <code>5</code> <code>sampling_rate</code> <code>float</code> <p>Rate of sampling tiles.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use to compute the neighborhood graph (per tile).</p> <code>1</code> Source code in <code>src/segger/data/io.py</code> <pre><code>def save_dataset_for_segger(\n    self,\n    processed_dir: Path,\n    x_size: float = 1000,\n    y_size: float = 1000,\n    d_x: float = 900,\n    d_y: float = 900,\n    margin_x: float = None,\n    margin_y: float = None,\n    compute_labels: bool = True,\n    r_tx: float = 5,\n    k_tx: int = 3,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n    neg_sampling_ratio_approx: float = 5,\n    sampling_rate: float = 1,\n    num_workers: int = 1,\n    scale_boundaries: float = 1.0,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n) -&gt; None:\n    \"\"\"\n    Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.\n\n    Parameters:\n        processed_dir (Path): Directory to save the processed dataset.\n        x_size (float, optional): Width of each tile.\n        y_size (float, optional): Height of each tile.\n        d_x (float, optional): Step size in the x direction for tiles.\n        d_y (float, optional): Step size in the y direction for tiles.\n        margin_x (float, optional): Margin in the x direction to include transcripts.\n        margin_y (float, optional): Margin in the y direction to include transcripts.\n        compute_labels (bool, optional): Whether to compute edge labels for tx_belongs_bd edges.\n        r_tx (float, optional): Radius for building the transcript-to-transcript graph.\n        k_tx (int, optional): Number of nearest neighbors for the tx-tx graph.\n        val_prob (float, optional): Probability of assigning a tile to the validation set.\n        test_prob (float, optional): Probability of assigning a tile to the test set.\n        neg_sampling_ratio_approx (float, optional): Approximate ratio of negative samples.\n        sampling_rate (float, optional): Rate of sampling tiles.\n        num_workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use to compute the neighborhood graph (per tile).\n\n    \"\"\"\n    # Prepare directories for storing processed tiles\n    self._prepare_directories(processed_dir)\n\n    # Get x and y coordinate ranges for tiling\n    x_range, y_range = self._get_ranges(d_x, d_y)\n\n    # Generate parameters for each tile\n    tile_params = self._generate_tile_params(\n        x_range,\n        y_range,\n        x_size,\n        y_size,\n        margin_x,\n        margin_y,\n        compute_labels,\n        r_tx,\n        k_tx,\n        val_prob,\n        test_prob,\n        neg_sampling_ratio_approx,\n        sampling_rate,\n        processed_dir,\n        scale_boundaries,\n        method,\n        gpu,\n        workers,\n    )\n\n    # Process each tile using Dask to parallelize the task\n    if self.verbose:\n        print(\"Starting tile processing...\")\n    tasks = [delayed(self._process_tile)(params) for params in tile_params]\n\n    with ProgressBar():\n        # Use Dask to process all tiles in parallel\n        dask.compute(*tasks, num_workers=num_workers)\n    if self.verbose:\n        print(\"Tile processing completed.\")\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.set_embedding","title":"set_embedding","text":"<pre><code>set_embedding(embedding_name)\n</code></pre> <p>Set the current embedding type for the transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_name</code> <p>str The name of the embedding to use.</p> required Source code in <code>src/segger/data/io.py</code> <pre><code>def set_embedding(self, embedding_name: str) -&gt; None:\n    \"\"\"\n    Set the current embedding type for the transcripts.\n\n    Parameters:\n        embedding_name : str\n            The name of the embedding to use.\n\n    \"\"\"\n    if embedding_name in self.embeddings_dict:\n        self.current_embedding = embedding_name\n    else:\n        raise ValueError(f\"Embedding {embedding_name} not found in embeddings_dict.\")\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.set_file_paths","title":"set_file_paths","text":"<pre><code>set_file_paths(transcripts_path, boundaries_path)\n</code></pre> <p>Set the paths for the transcript and boundary files.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_path</code> <code>Path</code> <p>Path to the Parquet file containing transcripts data.</p> required <code>boundaries_path</code> <code>Path</code> <p>Path to the Parquet file containing boundaries data.</p> required Source code in <code>src/segger/data/io.py</code> <pre><code>def set_file_paths(self, transcripts_path: Path, boundaries_path: Path) -&gt; None:\n    \"\"\"\n    Set the paths for the transcript and boundary files.\n\n    Parameters:\n        transcripts_path (Path): Path to the Parquet file containing transcripts data.\n        boundaries_path (Path): Path to the Parquet file containing boundaries data.\n    \"\"\"\n    self.transcripts_path = transcripts_path\n    self.boundaries_path = boundaries_path\n\n    if self.verbose:\n        print(f\"Set transcripts file path to {transcripts_path}\")\n    if self.verbose:\n        print(f\"Set boundaries file path to {boundaries_path}\")\n</code></pre>"},{"location":"api/data/#segger.data.SpatialTranscriptomicsSample.set_metadata","title":"set_metadata","text":"<pre><code>set_metadata()\n</code></pre> <p>Set metadata for the transcript dataset, including bounding box limits and unique gene names, without reading the entire Parquet file. Additionally, return integer tokens for unique gene names instead of one-hot encodings and store the lookup table for later mapping.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def set_metadata(self) -&gt; None:\n    \"\"\"\n    Set metadata for the transcript dataset, including bounding box limits and unique gene names,\n    without reading the entire Parquet file. Additionally, return integer tokens for unique gene names\n    instead of one-hot encodings and store the lookup table for later mapping.\n    \"\"\"\n    # Load the Parquet file metadata\n    parquet_file = pq.read_table(self.transcripts_path)\n\n    # Get the column names for X, Y, and feature names from the class's keys\n    x_col = self.keys.TRANSCRIPTS_X.value\n    y_col = self.keys.TRANSCRIPTS_Y.value\n    feature_col = self.keys.FEATURE_NAME.value\n\n    # Initialize variables to track min/max values for X and Y\n    x_min, x_max, y_min, y_max = float(\"inf\"), float(\"-inf\"), float(\"inf\"), float(\"-inf\")\n\n    # Extract unique gene names and ensure they're strings\n    gene_set = set()\n\n    # Define the filter for unwanted codewords\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    row_group_size = 4_000_000\n    start = 0\n    n = len(parquet_file)\n    while start &lt; n:\n        chunk = parquet_file.slice(start, start + row_group_size)\n        start += row_group_size\n\n        # Update the bounding box values (min/max)\n        x_values = chunk[x_col].to_pandas()\n        y_values = chunk[y_col].to_pandas()\n\n        x_min = min(x_min, x_values.min())\n        x_max = max(x_max, x_values.max())\n        y_min = min(y_min, y_values.min())\n        y_max = max(y_max, y_values.max())\n\n        # Convert feature values (gene names) to strings and filter out unwanted codewords\n        feature_values = (\n            chunk[feature_col]\n            .to_pandas()\n            .apply(\n                lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else str(x),\n            )\n        )\n\n        # Filter out unwanted codewords\n        filtered_genes = feature_values[~feature_values.str.startswith(filter_codewords)]\n\n        # Update the unique gene set\n        gene_set.update(filtered_genes.unique())\n\n    # Set bounding box limits\n    self.x_min = x_min\n    self.x_max = x_max\n    self.y_min = y_min\n    self.y_max = y_max\n\n    if self.verbose:\n        print(\n            f\"Bounding box limits set: x_min={self.x_min}, x_max={self.x_max}, y_min={self.y_min}, y_max={self.y_max}\"\n        )\n\n    # Convert the set of unique genes into a sorted list for consistent ordering\n    self.unique_genes = sorted(gene_set)\n    if self.verbose:\n        print(f\"Extracted {len(self.unique_genes)} unique gene names for integer tokenization.\")\n\n    # Initialize a LabelEncoder to convert unique genes into integer tokens\n    self.tx_encoder = LabelEncoder()\n\n    # Fit the LabelEncoder on the unique genes\n    self.tx_encoder.fit(self.unique_genes)\n\n    # Store the integer tokens mapping to gene names\n    self.gene_to_token_map = dict(\n        zip(self.tx_encoder.classes_, self.tx_encoder.transform(self.tx_encoder.classes_))\n    )\n\n    if self.verbose:\n        print(\"Integer tokens have been computed and stored based on unique gene names.\")\n\n    # Optional: Create a reverse mapping for lookup purposes (token to gene)\n    self.token_to_gene_map = {v: k for k, v in self.gene_to_token_map.items()}\n\n    if self.verbose:\n        print(\"Lookup tables (gene_to_token_map and token_to_gene_map) have been created.\")\n</code></pre>"},{"location":"api/data/#segger.data.XeniumSample","title":"XeniumSample","text":"<pre><code>XeniumSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(\n        transcripts_df, transcripts_radius, boundaries_graph, embedding_df, XeniumKeys, verbose=verbose\n    )\n</code></pre>"},{"location":"api/data/#segger.data.XeniumSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def filter_transcripts(self, transcripts_df: dd.DataFrame, min_qv: float = 20.0) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[self.keys.FEATURE_NAME.value].apply(\n            lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n        )\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/#segger.data.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(adata: ad.AnnData, celltype_column: str) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(gene_expression_binary, index=adata.obs_names, columns=adata.var_names)\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/#segger.data.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[assigned_transcripts[\"overlaps_nucleus\"] != 1]\n    percent_cytoplasmic = len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[non_assigned_transcripts[\"overlaps_nucleus\"] != 1]\n    percent_non_assigned_cytoplasmic = len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (gene_group_assigned.size() / (gene_group_all.size() + 1) * 100).reset_index(\n        names=\"percent_assigned\"\n    )\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100).reset_index(\n        name=\"percent_cytoplasmic\"\n    )\n    gene_metrics = pd.merge(gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\").fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/#segger.data.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"})[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\")\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = df[df[cell_id_col].astype(str) == \"UNASSIGNED\"].groupby(\"feature_name\")[\"feature_name\"].count()\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = var_df.index.map(unassigned_counts).fillna(0).astype(int)\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(adata.obs, cell_summary.loc[adata.obs_names, :], left_index=True, right_index=True)\n\n    return adata\n</code></pre>"},{"location":"api/data/#segger.data.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/#segger.data.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(coords_1, coords_2, k=k, dist=dist, workers=workers)\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/constants/","title":"segger.data.constants","text":""},{"location":"api/data/constants/#segger.data.constants.MerscopeKeys","title":"MerscopeKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/constants/#segger.data.constants.SpatialDataKeys","title":"SpatialDataKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/constants/#segger.data.constants.SpatialTranscriptomicsKeys","title":"SpatialTranscriptomicsKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Unified keys for spatial transcriptomics data, supporting multiple platforms.</p>"},{"location":"api/data/constants/#segger.data.constants.XeniumKeys","title":"XeniumKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for 10X Genomics Xenium formatted dataset.</p>"},{"location":"api/data/io/","title":"segger.data.io","text":""},{"location":"api/data/io/#segger.data.io.MerscopeKeys","title":"MerscopeKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/io/#segger.data.io.MerscopeSample","title":"MerscopeSample","text":"<pre><code>MerscopeSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(transcripts_df, transcripts_radius, boundaries_graph, embedding_df, MerscopeKeys)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.MerscopeSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on specific criteria for Merscope using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <p>dd.DataFrame The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <p>float, optional The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame The filtered Dask DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def filter_transcripts(self, transcripts_df: dd.DataFrame, min_qv: float = 20.0) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on specific criteria for Merscope using Dask.\n\n    Parameters:\n        transcripts_df : dd.DataFrame\n            The Dask DataFrame containing transcript data.\n        min_qv : float, optional\n            The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame\n            The filtered Dask DataFrame.\n    \"\"\"\n    # Add custom Merscope-specific filtering logic if needed\n    # For now, apply only the quality value filter\n    return transcripts_df[transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialDataKeys","title":"SpatialDataKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/io/#segger.data.io.SpatialDataSample","title":"SpatialDataSample","text":"<pre><code>SpatialDataSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, feature_name=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    feature_name: str | None = None,\n    verbose: bool = True,\n):\n    if feature_name is not None:\n        # luca: just a quick hack for now, I propose to use dataclasses instead of enums to address this\n        SpatialDataKeys.FEATURE_NAME._value_ = feature_name\n    else:\n        raise ValueError(\n            \"the automatic determination of a feature_name from a SpatialData object is not enabled yet\"\n        )\n\n    super().__init__(\n        transcripts_df, transcripts_radius, boundaries_graph, embedding_df, SpatialDataKeys, verbose=verbose\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialDataSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def filter_transcripts(self, transcripts_df: dd.DataFrame, min_qv: float = 20.0) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[self.keys.FEATURE_NAME.value].apply(\n            lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n        )\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/utils.py</code> <pre><code>def __init__(\n    self, root: str, transform: Callable = None, pre_transform: Callable = None, pre_filter: Callable = None\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsKeys","title":"SpatialTranscriptomicsKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Unified keys for spatial transcriptomics data, supporting multiple platforms.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample","title":"SpatialTranscriptomicsSample","text":"<pre><code>SpatialTranscriptomicsSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, keys=None, verbose=True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Initialize the SpatialTranscriptomicsSample class.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>A DataFrame containing transcript data.</p> <code>None</code> <code>transcripts_radius</code> <code>int</code> <p>Radius for transcripts in the analysis.</p> <code>10</code> <code>boundaries_graph</code> <code>bool</code> <p>Whether to include boundaries (e.g., nucleus, cell) graph information.</p> <code>False</code> <code>keys</code> <code>Dict</code> <p>The enum class containing key mappings specific to the dataset.</p> <code>None</code> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: pd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    keys: Dict = None,\n    verbose: bool = True,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsSample class.\n\n    Args:\n        transcripts_df (pd.DataFrame, optional): A DataFrame containing transcript data.\n        transcripts_radius (int, optional): Radius for transcripts in the analysis.\n        boundaries_graph (bool, optional): Whether to include boundaries (e.g., nucleus, cell) graph information.\n        keys (Dict, optional): The enum class containing key mappings specific to the dataset.\n    \"\"\"\n    self.transcripts_df = transcripts_df\n    self.transcripts_radius = transcripts_radius\n    self.boundaries_graph = boundaries_graph\n    self.keys = keys\n    self.embedding_df = embedding_df\n    self.current_embedding = \"token\"\n    self.verbose = verbose\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.build_pyg_data_from_tile","title":"build_pyg_data_from_tile","text":"<pre><code>build_pyg_data_from_tile(boundaries_df, transcripts_df, r_tx=5.0, k_tx=3, method='kd_tree', gpu=False, workers=1, scale_boundaries=1.0)\n</code></pre> <p>Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundaries data (e.g., nucleus, cell).</p> required <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcripts data.</p> required <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>HeteroData</code> <code>HeteroData</code> <p>PyG Heterogeneous Data object.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def build_pyg_data_from_tile(\n    self,\n    boundaries_df: dd.DataFrame,\n    transcripts_df: dd.DataFrame,\n    r_tx: float = 5.0,\n    k_tx: int = 3,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n    scale_boundaries: float = 1.0,\n) -&gt; HeteroData:\n    \"\"\"\n    Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): Dask DataFrame containing boundaries data (e.g., nucleus, cell).\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcripts data.\n        r_tx (float): Radius for building the transcript-to-transcript graph.\n        k_tx (int): Number of nearest neighbors for the tx-tx graph.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        HeteroData: PyG Heterogeneous Data object.\n    \"\"\"\n    # Initialize the PyG HeteroData object\n    data = HeteroData()\n\n    # Lazily compute boundaries geometries using Dask\n    if self.verbose:\n        print(\"Computing boundaries geometries...\")\n    bd_gdf = self.compute_boundaries_geometries(boundaries_df, scale_factor=scale_boundaries)\n    bd_gdf = bd_gdf[bd_gdf[\"geometry\"].notnull()]\n\n    # Add boundary node data to PyG HeteroData lazily\n    data[\"bd\"].id = bd_gdf[self.keys.CELL_ID.value].values\n    data[\"bd\"].pos = torch.as_tensor(bd_gdf[[\"centroid_x\", \"centroid_y\"]].values.astype(float))\n\n    if data[\"bd\"].pos.isnan().any():\n        raise ValueError(data[\"bd\"].id[data[\"bd\"].pos.isnan().any(1)])\n\n    bd_x = bd_gdf.iloc[:, 4:]\n    data[\"bd\"].x = torch.as_tensor(bd_x.to_numpy(), dtype=torch.float32)\n\n    # Extract the transcript coordinates lazily\n    if self.verbose:\n        print(\"Preparing transcript features and positions...\")\n    x_xyz = transcripts_df[[self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]].to_numpy()\n    data[\"tx\"].id = torch.as_tensor(transcripts_df[self.keys.TRANSCRIPTS_ID.value].values.astype(int))\n    data[\"tx\"].pos = torch.tensor(x_xyz, dtype=torch.float32)\n\n    # Lazily prepare transcript embeddings (if available)\n    if self.verbose:\n        print(\"Preparing transcript embeddings..\")\n    token_encoding = self.tx_encoder.transform(transcripts_df[self.keys.FEATURE_NAME.value])\n    transcripts_df[\"token\"] = token_encoding  # Store the integer tokens in the 'token' column\n    data[\"tx\"].token = torch.as_tensor(token_encoding).int()\n    # Handle additional embeddings lazily as well\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        embeddings = delayed(lambda df: self.embedding_df.loc[df[self.keys.FEATURE_NAME.value].values].values)(\n            transcripts_df\n        )\n    else:\n        embeddings = token_encoding\n    if hasattr(embeddings, \"compute\"):\n        embeddings = embeddings.compute()\n    x_features = torch.as_tensor(embeddings).int()\n    data[\"tx\"].x = x_features\n\n    # Check if the overlap column exists, if not, compute it lazily using Dask\n    if self.keys.OVERLAPS_BOUNDARY.value not in transcripts_df.columns:\n        if self.verbose:\n            print(f\"Computing overlaps for transcripts...\")\n        transcripts_df = self.compute_transcript_overlap_with_boundaries(\n            transcripts_df, polygons_gdf=bd_gdf, scale_factor=1.0\n        )\n\n    # Connect transcripts with their corresponding boundaries (e.g., nuclei, cells)\n    if self.verbose:\n        print(\"Connecting transcripts with boundaries...\")\n    overlaps = transcripts_df[self.keys.OVERLAPS_BOUNDARY.value].values\n    valid_cell_ids = bd_gdf[self.keys.CELL_ID.value].values\n    ind = np.where(overlaps &amp; transcripts_df[self.keys.CELL_ID.value].isin(valid_cell_ids))[0]\n    tx_bd_edge_index = np.column_stack(\n        (ind, np.searchsorted(valid_cell_ids, transcripts_df.iloc[ind][self.keys.CELL_ID.value]))\n    )\n\n    # Add transcript-boundary edge index to PyG HeteroData\n    data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.as_tensor(tx_bd_edge_index.T, dtype=torch.long)\n\n    # Compute transcript-to-transcript (tx-tx) edges using Dask (lazy computation)\n    if self.verbose:\n        print(\"Computing tx-tx edges...\")\n    tx_positions = transcripts_df[[self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]].values\n    delayed_tx_edge_index = delayed(get_edge_index)(\n        tx_positions, tx_positions, k=k_tx, dist=r_tx, method=method, gpu=gpu, workers=workers\n    )\n    tx_edge_index = delayed_tx_edge_index.compute()\n\n    # Add the tx-tx edge index to the PyG HeteroData object\n    data[\"tx\", \"neighbors\", \"tx\"].edge_index = torch.as_tensor(tx_edge_index.T, dtype=torch.long)\n\n    if self.verbose:\n        print(\"Finished building PyG data for the tile.\")\n    return data\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.compute_boundaries_geometries","title":"compute_boundaries_geometries","text":"<pre><code>compute_boundaries_geometries(boundaries_df=None, polygons_gdf=None, scale_factor=1.0, area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>The dataframe containing boundaries data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <code>area</code> <code>bool</code> <p>Whether to compute area.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>Whether to compute convexity.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>Whether to compute elongation.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>Whether to compute circularity.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def compute_boundaries_geometries(\n    self,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.\n\n    Parameters:\n        boundaries_df (dd.DataFrame, optional): The dataframe containing boundaries data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n        area (bool, optional): Whether to compute area.\n        convexity (bool, optional): Whether to compute convexity.\n        elongation (bool, optional): Whether to compute elongation.\n        circularity (bool, optional): Whether to compute circularity.\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\")\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        if self.verbose:\n            print(\n                f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\"\n            )\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    # Check if the generated polygons_gdf is empty\n    if polygons_gdf.shape[0] == 0:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(f\"Polygons are available. Proceeding with geometrical computations.\")\n\n    # Compute additional geometrical properties\n    polygons = polygons_gdf.geometry\n\n    # Compute additional geometrical properties\n    if area:\n        if self.verbose:\n            print(\"Computing area...\")\n        polygons_gdf[\"area\"] = polygons.area\n    if convexity:\n        if self.verbose:\n            print(\"Computing convexity...\")\n        polygons_gdf[\"convexity\"] = polygons.convex_hull.area / polygons.area\n    if elongation:\n        if self.verbose:\n            print(\"Computing elongation...\")\n        r = polygons.minimum_rotated_rectangle()\n        polygons_gdf[\"elongation\"] = (r.length * r.length) / r.area\n    if circularity:\n        if self.verbose:\n            print(\"Computing circularity...\")\n        r = polygons_gdf.minimum_bounding_radius()\n        polygons_gdf[\"circularity\"] = polygons.area / (r * r)\n\n    if self.verbose:\n        print(\"Geometrical computations completed.\")\n\n    return polygons_gdf.reset_index(drop=True)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.compute_transcript_overlap_with_boundaries","title":"compute_transcript_overlap_with_boundaries","text":"<pre><code>compute_transcript_overlap_with_boundaries(transcripts_df, boundaries_df=None, polygons_gdf=None, scale_factor=1.0)\n</code></pre> <p>Computes the overlap of transcript locations with scaled boundary polygons and assigns corresponding cell IDs to the transcripts using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcript data.</p> required <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def compute_transcript_overlap_with_boundaries(\n    self,\n    transcripts_df: dd.DataFrame,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Computes the overlap of transcript locations with scaled boundary polygons\n    and assigns corresponding cell IDs to the transcripts using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcript data.\n        boundaries_df (dd.DataFrame, optional): Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\")\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    if polygons_gdf.empty:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(f\"Polygons are available. Proceeding with overlap computation.\")\n\n    # Create a delayed function to check if a point is within any polygon\n    def check_overlap(transcript, polygons_gdf):\n        x = transcript[self.keys.TRANSCRIPTS_X.value]\n        y = transcript[self.keys.TRANSCRIPTS_Y.value]\n        point = Point(x, y)\n\n        overlap = False\n        cell_id = None\n\n        # Check for point containment lazily within polygons\n        for _, polygon in polygons_gdf.iterrows():\n            if polygon.geometry.contains(point):\n                overlap = True\n                cell_id = polygon[self.keys.CELL_ID.value]\n                break\n\n        return overlap, cell_id\n\n    # Apply the check_overlap function in parallel to each row using Dask's map_partitions\n    if self.verbose:\n        print(f\"Starting overlap computation for transcripts with the boundary polygons.\")\n    if isinstance(transcripts_df, pd.DataFrame):\n        # luca: I found this bug here\n        warnings.warn(\"BUG! This function expects Dask DataFrames, not Pandas DataFrames.\")\n        # if we want to really have the below working in parallel, we need to add n_partitions&gt;1 here\n        transcripts_df = dd.from_pandas(transcripts_df, npartitions=1)\n        transcripts_df.compute().columns\n    transcripts_df = transcripts_df.map_partitions(\n        lambda df: df.assign(\n            **{\n                self.keys.OVERLAPS_BOUNDARY.value: df.apply(\n                    lambda row: delayed(check_overlap)(row, polygons_gdf)[0], axis=1\n                ),\n                self.keys.CELL_ID.value: df.apply(lambda row: delayed(check_overlap)(row, polygons_gdf)[1], axis=1),\n            }\n        )\n    )\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.create_scaled_polygon","title":"create_scaled_polygon  <code>staticmethod</code>","text":"<pre><code>create_scaled_polygon(group, scale_factor, keys)\n</code></pre> <p>Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>Group of boundary coordinates (for a specific cell).</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons.</p> required <code>keys</code> <code>Dict or Enum</code> <p>A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>@staticmethod\ndef create_scaled_polygon(group: pd.DataFrame, scale_factor: float, keys) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.\n\n    Parameters:\n        group (pd.DataFrame): Group of boundary coordinates (for a specific cell).\n        scale_factor (float): The factor by which to scale the polygons.\n        keys (Dict or Enum): A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.\n    \"\"\"\n    # Extract coordinates and cell ID from the group using keys\n    x_coords = group[keys[\"vertex_x\"]]\n    y_coords = group[keys[\"vertex_y\"]]\n    cell_id = group[keys[\"cell_id\"]].iloc[0]\n\n    # Ensure there are at least 3 points to form a polygon\n    if len(x_coords) &gt;= 3:\n\n        polygon = Polygon(zip(x_coords, y_coords))\n        if polygon.is_valid and not polygon.is_empty:\n            # Scale the polygon by the provided factor\n            scaled_polygon = polygon.buffer(scale_factor)\n            if scaled_polygon.is_valid and not scaled_polygon.is_empty:\n                return gpd.GeoDataFrame(\n                    {\"geometry\": [scaled_polygon], keys[\"cell_id\"]: [cell_id]}, geometry=\"geometry\", crs=\"EPSG:4326\"\n                )\n    # Return an empty GeoDataFrame if no valid polygon is created\n    return gpd.GeoDataFrame({\"geometry\": [None], keys[\"cell_id\"]: [cell_id]}, geometry=\"geometry\", crs=\"EPSG:4326\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.filter_transcripts","title":"filter_transcripts  <code>abstractmethod</code>","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Abstract method to filter transcripts based on dataset-specific criteria.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>@abstractmethod\ndef filter_transcripts(self, transcripts_df: pd.DataFrame, min_qv: float = 20.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Abstract method to filter transcripts based on dataset-specific criteria.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.generate_and_scale_polygons","title":"generate_and_scale_polygons","text":"<pre><code>generate_and_scale_polygons(boundaries_df, scale_factor=1.0)\n</code></pre> <p>Generate and scale polygons from boundary coordinates using Dask. Keeps class structure intact by using static method for the core polygon generation.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>DataFrame containing boundary coordinates.</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def generate_and_scale_polygons(self, boundaries_df: dd.DataFrame, scale_factor: float = 1.0) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Generate and scale polygons from boundary coordinates using Dask.\n    Keeps class structure intact by using static method for the core polygon generation.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): DataFrame containing boundary coordinates.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.\n    \"\"\"\n    # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n\n    # Extract required columns from self.keys\n    cell_id_column = self.keys.CELL_ID.value\n    vertex_x_column = self.keys.BOUNDARIES_VERTEX_X.value\n    vertex_y_column = self.keys.BOUNDARIES_VERTEX_Y.value\n\n    create_polygon = self.create_scaled_polygon\n    # Use a lambda to wrap the static method call and avoid passing the function object directly to Dask\n    polygons_ddf = boundaries_df.groupby(cell_id_column).apply(\n        lambda group: create_polygon(\n            group=group,\n            scale_factor=scale_factor,\n            keys={  # Pass keys as a dict for the lambda function\n                \"vertex_x\": vertex_x_column,\n                \"vertex_y\": vertex_y_column,\n                \"cell_id\": cell_id_column,\n            },\n        )\n    )\n\n    # Lazily compute centroids for each polygon\n    if self.verbose:\n        print(\"Adding centroids to the polygons...\")\n    polygons_ddf[\"centroid_x\"] = polygons_ddf.geometry.centroid.x\n    polygons_ddf[\"centroid_y\"] = polygons_ddf.geometry.centroid.y\n\n    polygons_ddf = polygons_ddf.drop_duplicates()\n    # polygons_ddf = polygons_ddf.to_crs(\"EPSG:3857\")\n\n    return polygons_ddf\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.load_boundaries","title":"load_boundaries","text":"<pre><code>load_boundaries(path, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load boundaries data lazily using Dask, filtering by the specified bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the boundaries file.</p> required <code>file_format</code> <code>str</code> <p>Format of the file to load. Only 'parquet' is supported in this refactor.</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered boundaries DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def load_boundaries(\n    self,\n    path: Path,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load boundaries data lazily using Dask, filtering by the specified bounding box.\n\n    Parameters:\n        path (Path): Path to the boundaries file.\n        file_format (str, optional): Format of the file to load. Only 'parquet' is supported in this refactor.\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered boundaries DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(f\"Unsupported file format: {file_format}\")\n\n    self.boundaries_path = path\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.BOUNDARIES_VERTEX_X.value,\n        self.keys.BOUNDARIES_VERTEX_Y.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&gt;=\", x_min),\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&lt;=\", x_max),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&gt;=\", y_min),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(path).columns)\n    if \"geometry\" in columns:\n        bbox = (x_min, y_min, x_max, y_max)\n        # TODO: check that SpatialData objects write the \"bbox covering metadata\" to the parquet file\n        gdf = dgpd.read_parquet(path, bbox=bbox)\n        id_col, x_col, y_col = (\n            self.keys.CELL_ID.value,\n            self.keys.BOUNDARIES_VERTEX_X.value,\n            self.keys.BOUNDARIES_VERTEX_Y.value,\n        )\n\n        # Function to expand each polygon into a list of vertices\n        def expand_polygon(row):\n            expanded_data = []\n            polygon = row[\"geometry\"]\n            if polygon.geom_type == \"Polygon\":\n                exterior_coords = polygon.exterior.coords\n                for x, y in exterior_coords:\n                    expanded_data.append({id_col: row.name, x_col: x, y_col: y})\n            else:\n                # Instead of expanding the gdf and then having code later to recreate it (when computing the pyg graph)\n                # we could directly have this function returning a Dask GeoDataFrame. This means that we don't need\n                # to implement this else black\n                raise ValueError(f\"Unsupported geometry type: {polygon.geom_type}\")\n            return expanded_data\n\n        # Apply the function to each partition and collect results\n        def process_partition(df):\n            expanded_data = [expand_polygon(row) for _, row in df.iterrows()]\n            # Flatten the list of lists\n            flattened_data = [item for sublist in expanded_data for item in sublist]\n            return pd.DataFrame(flattened_data)\n\n        # Use map_partitions to apply the function and convert it into a Dask DataFrame\n        boundaries_df = gdf.map_partitions(process_partition, meta={id_col: str, x_col: float, y_col: float})\n    else:\n        boundaries_df = dd.read_parquet(path, columns=columns_to_read, filters=filters)\n\n        # Convert the cell IDs to strings lazily\n        boundaries_df[self.keys.CELL_ID.value] = boundaries_df[self.keys.CELL_ID.value].apply(\n            lambda x: str(x) if pd.notnull(x) else None, meta=(\"cell_id\", \"object\")\n        )\n\n    if self.verbose:\n        print(f\"Loaded boundaries from '{path}' within bounding box ({x_min}, {x_max}, {y_min}, {y_max}).\")\n\n    return boundaries_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.load_transcripts","title":"load_transcripts","text":"<pre><code>load_transcripts(base_path=None, sample=None, transcripts_filename=None, path=None, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load transcripts from a Parquet file using Dask for efficient chunked processing, only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>The base directory path where samples are stored.</p> <code>None</code> <code>sample</code> <code>str</code> <p>The sample name or identifier.</p> <code>None</code> <code>transcripts_filename</code> <code>str</code> <p>The filename of the transcripts file (default is derived from the dataset keys).</p> <code>None</code> <code>path</code> <code>Path</code> <p>Specific path to the transcripts file.</p> <code>None</code> <code>file_format</code> <code>str</code> <p>Format of the file to load (default is 'parquet').</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered transcripts DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def load_transcripts(\n    self,\n    base_path: Path = None,\n    sample: str = None,\n    transcripts_filename: str = None,\n    path: Path = None,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n    # additional_embeddings: Optional[Dict[str, pd.DataFrame]] = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load transcripts from a Parquet file using Dask for efficient chunked processing,\n    only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.\n\n    Parameters:\n        base_path (Path, optional): The base directory path where samples are stored.\n        sample (str, optional): The sample name or identifier.\n        transcripts_filename (str, optional): The filename of the transcripts file (default is derived from the dataset keys).\n        path (Path, optional): Specific path to the transcripts file.\n        file_format (str, optional): Format of the file to load (default is 'parquet').\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered transcripts DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(\"This version only supports parquet files with Dask.\")\n\n    # Set the file path for transcripts\n    transcripts_filename = transcripts_filename or self.keys.TRANSCRIPTS_FILE.value\n    file_path = path or (base_path / sample / transcripts_filename)\n    self.transcripts_path = file_path\n\n    # Set metadata\n    # self.set_metadata()\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Check for available columns in the file's metadata (without loading the data)\n    parquet_metadata = dd.read_parquet(file_path, meta_only=True)\n    available_columns = parquet_metadata.columns\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.TRANSCRIPTS_ID.value,\n        self.keys.TRANSCRIPTS_X.value,\n        self.keys.TRANSCRIPTS_Y.value,\n        self.keys.FEATURE_NAME.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Check if the QUALITY_VALUE key exists in the dataset, and add it to the columns list if present\n    if self.keys.QUALITY_VALUE.value in available_columns:\n        columns_to_read.append(self.keys.QUALITY_VALUE.value)\n\n    if self.keys.OVERLAPS_BOUNDARY.value in available_columns:\n        columns_to_read.append(self.keys.OVERLAPS_BOUNDARY.value)\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.TRANSCRIPTS_X.value, \"&gt;=\", x_min),\n        (self.keys.TRANSCRIPTS_X.value, \"&lt;=\", x_max),\n        (self.keys.TRANSCRIPTS_Y.value, \"&gt;=\", y_min),\n        (self.keys.TRANSCRIPTS_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(file_path).columns)\n    transcripts_df = dd.read_parquet(file_path, columns=columns_to_read, filters=filters).compute()\n\n    # Convert transcript and cell IDs to strings lazily\n    transcripts_df[self.keys.TRANSCRIPTS_ID.value] = transcripts_df[self.keys.TRANSCRIPTS_ID.value].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n    transcripts_df[self.keys.CELL_ID.value] = transcripts_df[self.keys.CELL_ID.value].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n\n    # Convert feature names from bytes to strings if necessary\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[self.keys.FEATURE_NAME.value].astype(str)\n\n    # Apply dataset-specific filtering (e.g., quality filtering for Xenium)\n    transcripts_df = self.filter_transcripts(transcripts_df)\n\n    # Handle additional embeddings if provided\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        valid_genes = self.embedding_df.index\n        # Lazily count the number of rows in the DataFrame before filtering\n        initial_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        # Filter the DataFrame lazily based on valid genes from embeddings\n        transcripts_df = transcripts_df[transcripts_df[self.keys.FEATURE_NAME.value].isin(valid_genes)]\n        final_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        if self.verbose:\n            print(f\"Dropped {initial_count - final_count} transcripts not found in embedding.\")\n\n    # Ensure that the 'OVERLAPS_BOUNDARY' column is boolean if it exists\n    if self.keys.OVERLAPS_BOUNDARY.value in transcripts_df.columns:\n        transcripts_df[self.keys.OVERLAPS_BOUNDARY.value] = transcripts_df[\n            self.keys.OVERLAPS_BOUNDARY.value\n        ].astype(bool)\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.save_dataset_for_segger","title":"save_dataset_for_segger","text":"<pre><code>save_dataset_for_segger(processed_dir, x_size=1000, y_size=1000, d_x=900, d_y=900, margin_x=None, margin_y=None, compute_labels=True, r_tx=5, k_tx=3, val_prob=0.1, test_prob=0.2, neg_sampling_ratio_approx=5, sampling_rate=1, num_workers=1, scale_boundaries=1.0, method='kd_tree', gpu=False, workers=1)\n</code></pre> <p>Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.</p> <p>Parameters:</p> Name Type Description Default <code>processed_dir</code> <code>Path</code> <p>Directory to save the processed dataset.</p> required <code>x_size</code> <code>float</code> <p>Width of each tile.</p> <code>1000</code> <code>y_size</code> <code>float</code> <p>Height of each tile.</p> <code>1000</code> <code>d_x</code> <code>float</code> <p>Step size in the x direction for tiles.</p> <code>900</code> <code>d_y</code> <code>float</code> <p>Step size in the y direction for tiles.</p> <code>900</code> <code>margin_x</code> <code>float</code> <p>Margin in the x direction to include transcripts.</p> <code>None</code> <code>margin_y</code> <code>float</code> <p>Margin in the y direction to include transcripts.</p> <code>None</code> <code>compute_labels</code> <code>bool</code> <p>Whether to compute edge labels for tx_belongs_bd edges.</p> <code>True</code> <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>val_prob</code> <code>float</code> <p>Probability of assigning a tile to the validation set.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Probability of assigning a tile to the test set.</p> <code>0.2</code> <code>neg_sampling_ratio_approx</code> <code>float</code> <p>Approximate ratio of negative samples.</p> <code>5</code> <code>sampling_rate</code> <code>float</code> <p>Rate of sampling tiles.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use to compute the neighborhood graph (per tile).</p> <code>1</code> Source code in <code>src/segger/data/io.py</code> <pre><code>def save_dataset_for_segger(\n    self,\n    processed_dir: Path,\n    x_size: float = 1000,\n    y_size: float = 1000,\n    d_x: float = 900,\n    d_y: float = 900,\n    margin_x: float = None,\n    margin_y: float = None,\n    compute_labels: bool = True,\n    r_tx: float = 5,\n    k_tx: int = 3,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n    neg_sampling_ratio_approx: float = 5,\n    sampling_rate: float = 1,\n    num_workers: int = 1,\n    scale_boundaries: float = 1.0,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n) -&gt; None:\n    \"\"\"\n    Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.\n\n    Parameters:\n        processed_dir (Path): Directory to save the processed dataset.\n        x_size (float, optional): Width of each tile.\n        y_size (float, optional): Height of each tile.\n        d_x (float, optional): Step size in the x direction for tiles.\n        d_y (float, optional): Step size in the y direction for tiles.\n        margin_x (float, optional): Margin in the x direction to include transcripts.\n        margin_y (float, optional): Margin in the y direction to include transcripts.\n        compute_labels (bool, optional): Whether to compute edge labels for tx_belongs_bd edges.\n        r_tx (float, optional): Radius for building the transcript-to-transcript graph.\n        k_tx (int, optional): Number of nearest neighbors for the tx-tx graph.\n        val_prob (float, optional): Probability of assigning a tile to the validation set.\n        test_prob (float, optional): Probability of assigning a tile to the test set.\n        neg_sampling_ratio_approx (float, optional): Approximate ratio of negative samples.\n        sampling_rate (float, optional): Rate of sampling tiles.\n        num_workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use to compute the neighborhood graph (per tile).\n\n    \"\"\"\n    # Prepare directories for storing processed tiles\n    self._prepare_directories(processed_dir)\n\n    # Get x and y coordinate ranges for tiling\n    x_range, y_range = self._get_ranges(d_x, d_y)\n\n    # Generate parameters for each tile\n    tile_params = self._generate_tile_params(\n        x_range,\n        y_range,\n        x_size,\n        y_size,\n        margin_x,\n        margin_y,\n        compute_labels,\n        r_tx,\n        k_tx,\n        val_prob,\n        test_prob,\n        neg_sampling_ratio_approx,\n        sampling_rate,\n        processed_dir,\n        scale_boundaries,\n        method,\n        gpu,\n        workers,\n    )\n\n    # Process each tile using Dask to parallelize the task\n    if self.verbose:\n        print(\"Starting tile processing...\")\n    tasks = [delayed(self._process_tile)(params) for params in tile_params]\n\n    with ProgressBar():\n        # Use Dask to process all tiles in parallel\n        dask.compute(*tasks, num_workers=num_workers)\n    if self.verbose:\n        print(\"Tile processing completed.\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_embedding","title":"set_embedding","text":"<pre><code>set_embedding(embedding_name)\n</code></pre> <p>Set the current embedding type for the transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_name</code> <p>str The name of the embedding to use.</p> required Source code in <code>src/segger/data/io.py</code> <pre><code>def set_embedding(self, embedding_name: str) -&gt; None:\n    \"\"\"\n    Set the current embedding type for the transcripts.\n\n    Parameters:\n        embedding_name : str\n            The name of the embedding to use.\n\n    \"\"\"\n    if embedding_name in self.embeddings_dict:\n        self.current_embedding = embedding_name\n    else:\n        raise ValueError(f\"Embedding {embedding_name} not found in embeddings_dict.\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_file_paths","title":"set_file_paths","text":"<pre><code>set_file_paths(transcripts_path, boundaries_path)\n</code></pre> <p>Set the paths for the transcript and boundary files.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_path</code> <code>Path</code> <p>Path to the Parquet file containing transcripts data.</p> required <code>boundaries_path</code> <code>Path</code> <p>Path to the Parquet file containing boundaries data.</p> required Source code in <code>src/segger/data/io.py</code> <pre><code>def set_file_paths(self, transcripts_path: Path, boundaries_path: Path) -&gt; None:\n    \"\"\"\n    Set the paths for the transcript and boundary files.\n\n    Parameters:\n        transcripts_path (Path): Path to the Parquet file containing transcripts data.\n        boundaries_path (Path): Path to the Parquet file containing boundaries data.\n    \"\"\"\n    self.transcripts_path = transcripts_path\n    self.boundaries_path = boundaries_path\n\n    if self.verbose:\n        print(f\"Set transcripts file path to {transcripts_path}\")\n    if self.verbose:\n        print(f\"Set boundaries file path to {boundaries_path}\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_metadata","title":"set_metadata","text":"<pre><code>set_metadata()\n</code></pre> <p>Set metadata for the transcript dataset, including bounding box limits and unique gene names, without reading the entire Parquet file. Additionally, return integer tokens for unique gene names instead of one-hot encodings and store the lookup table for later mapping.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def set_metadata(self) -&gt; None:\n    \"\"\"\n    Set metadata for the transcript dataset, including bounding box limits and unique gene names,\n    without reading the entire Parquet file. Additionally, return integer tokens for unique gene names\n    instead of one-hot encodings and store the lookup table for later mapping.\n    \"\"\"\n    # Load the Parquet file metadata\n    parquet_file = pq.read_table(self.transcripts_path)\n\n    # Get the column names for X, Y, and feature names from the class's keys\n    x_col = self.keys.TRANSCRIPTS_X.value\n    y_col = self.keys.TRANSCRIPTS_Y.value\n    feature_col = self.keys.FEATURE_NAME.value\n\n    # Initialize variables to track min/max values for X and Y\n    x_min, x_max, y_min, y_max = float(\"inf\"), float(\"-inf\"), float(\"inf\"), float(\"-inf\")\n\n    # Extract unique gene names and ensure they're strings\n    gene_set = set()\n\n    # Define the filter for unwanted codewords\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    row_group_size = 4_000_000\n    start = 0\n    n = len(parquet_file)\n    while start &lt; n:\n        chunk = parquet_file.slice(start, start + row_group_size)\n        start += row_group_size\n\n        # Update the bounding box values (min/max)\n        x_values = chunk[x_col].to_pandas()\n        y_values = chunk[y_col].to_pandas()\n\n        x_min = min(x_min, x_values.min())\n        x_max = max(x_max, x_values.max())\n        y_min = min(y_min, y_values.min())\n        y_max = max(y_max, y_values.max())\n\n        # Convert feature values (gene names) to strings and filter out unwanted codewords\n        feature_values = (\n            chunk[feature_col]\n            .to_pandas()\n            .apply(\n                lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else str(x),\n            )\n        )\n\n        # Filter out unwanted codewords\n        filtered_genes = feature_values[~feature_values.str.startswith(filter_codewords)]\n\n        # Update the unique gene set\n        gene_set.update(filtered_genes.unique())\n\n    # Set bounding box limits\n    self.x_min = x_min\n    self.x_max = x_max\n    self.y_min = y_min\n    self.y_max = y_max\n\n    if self.verbose:\n        print(\n            f\"Bounding box limits set: x_min={self.x_min}, x_max={self.x_max}, y_min={self.y_min}, y_max={self.y_max}\"\n        )\n\n    # Convert the set of unique genes into a sorted list for consistent ordering\n    self.unique_genes = sorted(gene_set)\n    if self.verbose:\n        print(f\"Extracted {len(self.unique_genes)} unique gene names for integer tokenization.\")\n\n    # Initialize a LabelEncoder to convert unique genes into integer tokens\n    self.tx_encoder = LabelEncoder()\n\n    # Fit the LabelEncoder on the unique genes\n    self.tx_encoder.fit(self.unique_genes)\n\n    # Store the integer tokens mapping to gene names\n    self.gene_to_token_map = dict(\n        zip(self.tx_encoder.classes_, self.tx_encoder.transform(self.tx_encoder.classes_))\n    )\n\n    if self.verbose:\n        print(\"Integer tokens have been computed and stored based on unique gene names.\")\n\n    # Optional: Create a reverse mapping for lookup purposes (token to gene)\n    self.token_to_gene_map = {v: k for k, v in self.gene_to_token_map.items()}\n\n    if self.verbose:\n        print(\"Lookup tables (gene_to_token_map and token_to_gene_map) have been created.\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.XeniumKeys","title":"XeniumKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for 10X Genomics Xenium formatted dataset.</p>"},{"location":"api/data/io/#segger.data.io.XeniumSample","title":"XeniumSample","text":"<pre><code>XeniumSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>src/segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(\n        transcripts_df, transcripts_radius, boundaries_graph, embedding_df, XeniumKeys, verbose=verbose\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.XeniumSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>src/segger/data/io.py</code> <pre><code>def filter_transcripts(self, transcripts_df: dd.DataFrame, min_qv: float = 20.0) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[self.keys.FEATURE_NAME.value].apply(\n            lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n        )\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(adata: ad.AnnData, celltype_column: str) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(gene_expression_binary, index=adata.obs_names, columns=adata.var_names)\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[assigned_transcripts[\"overlaps_nucleus\"] != 1]\n    percent_cytoplasmic = len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[non_assigned_transcripts[\"overlaps_nucleus\"] != 1]\n    percent_non_assigned_cytoplasmic = len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (gene_group_assigned.size() / (gene_group_all.size() + 1) * 100).reset_index(\n        names=\"percent_assigned\"\n    )\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100).reset_index(\n        name=\"percent_cytoplasmic\"\n    )\n    gene_metrics = pd.merge(gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\").fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/io/#segger.data.io.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"})[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\")\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = df[df[cell_id_col].astype(str) == \"UNASSIGNED\"].groupby(\"feature_name\")[\"feature_name\"].count()\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = var_df.index.map(unassigned_counts).fillna(0).astype(int)\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(adata.obs, cell_summary.loc[adata.obs_names, :], left_index=True, right_index=True)\n\n    return adata\n</code></pre>"},{"location":"api/data/io/#segger.data.io.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/io/#segger.data.io.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/io/#segger.data.io.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(coords_1, coords_2, k=k, dist=dist, workers=workers)\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray, coords_2: np.ndarray, k: int = 5, dist: int = 10, workers: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(coords_2, k=k, distance_upper_bound=dist, workers=workers)\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T)\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/io/#segger.data.io.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/io/#segger.data.io.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/data/utils/","title":"segger.data.utils","text":""},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/utils.py</code> <pre><code>def __init__(\n    self, root: str, transform: Callable = None, pre_transform: Callable = None, pre_filter: Callable = None\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(adata: ad.AnnData, celltype_column: str) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(gene_expression_binary, index=adata.obs_names, columns=adata.var_names)\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[assigned_transcripts[\"overlaps_nucleus\"] != 1]\n    percent_cytoplasmic = len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[non_assigned_transcripts[\"overlaps_nucleus\"] != 1]\n    percent_non_assigned_cytoplasmic = len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (gene_group_assigned.size() / (gene_group_all.size() + 1) * 100).reset_index(\n        names=\"percent_assigned\"\n    )\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100).reset_index(\n        name=\"percent_cytoplasmic\"\n    )\n    gene_metrics = pd.merge(gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\").fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"})[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\")\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = df[df[cell_id_col].astype(str) == \"UNASSIGNED\"].groupby(\"feature_name\")[\"feature_name\"].count()\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = var_df.index.map(unassigned_counts).fillna(0).astype(int)\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(adata.obs, cell_summary.loc[adata.obs_names, :], left_index=True, right_index=True)\n\n    return adata\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/utils/#segger.data.utils.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/utils/#segger.data.utils.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(coords_1, coords_2, k=k, dist=dist, workers=workers)\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray, coords_2: np.ndarray, k: int = 5, dist: int = 10, workers: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(coords_2, k=k, distance_upper_bound=dist, workers=workers)\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T)\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/models/","title":"segger.models","text":"<p>Models module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p> <p>Models module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/models/#segger.models.Segger","title":"Segger","text":"<pre><code>Segger(num_node_features, init_emb=16, hidden_channels=32, num_mid_layers=3, out_channels=32, heads=3, is_token_based=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initializes the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>num_node_features</code> <code>dict[str, int]</code> <p>Number of node features for each node type.</p> required <code>init_emb</code> <code>int)         </code> <p>Initial embedding size for both 'tx' and boundary (non-token) nodes.</p> <code>16</code> <code>hidden_channels</code> <code>int)  </code> <p>Number of hidden channels.</p> <code>32</code> <code>num_mid_layers</code> <code>int)   </code> <p>Number of hidden layers (excluding first and last layers).</p> <code>3</code> <code>out_channels</code> <code>int)     </code> <p>Number of output channels.</p> <code>32</code> <code>heads</code> <code>int)            </code> <p>Number of attention heads.</p> <code>3</code> <code>is_token_based</code> <code>bool)  </code> <p>Whether the model is using token-based embeddings or scRNAseq embeddings.</p> <code>True</code> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def __init__(\n    self,\n    num_node_features: dict[str, int],\n    init_emb: int = 16,\n    hidden_channels: int = 32,\n    num_mid_layers: int = 3,\n    out_channels: int = 32,\n    heads: int = 3,\n    is_token_based: bool = True,\n):\n    \"\"\"\n    Initializes the Segger model.\n\n    Args:\n        num_node_features (dict[str, int]): Number of node features for each node type.\n        init_emb (int)         : Initial embedding size for both 'tx' and boundary (non-token) nodes.\n        hidden_channels (int)  : Number of hidden channels.\n        num_mid_layers (int)   : Number of hidden layers (excluding first and last layers).\n        out_channels (int)     : Number of output channels.\n        heads (int)            : Number of attention heads.\n        is_token_based (bool)  : Whether the model is using token-based embeddings or scRNAseq embeddings.\n    \"\"\"\n    super().__init__()\n\n    # Initialize node embeddings\n    if is_token_based:\n        # Using token-based embeddings for transcript ('tx') nodes\n        self.node_init = nn.ModuleDict(\n            {\n                \"tx\": nn.Embedding(num_node_features[\"tx\"], init_emb),\n                \"bd\": nn.Linear(num_node_features[\"bd\"], init_emb),\n            }\n        )\n    else:\n        # Using scRNAseq embeddings (i.e. prior biological knowledge) for transcript ('tx') nodes\n        self.node_init = nn.ModuleDict(\n            {\n                \"tx\": nn.Linear(num_node_features[\"tx\"], init_emb),\n                \"bd\": nn.Linear(num_node_features[\"bd\"], init_emb),\n            }\n        )\n\n    # First GATv2Conv layer\n    self.conv1 = SkipGAT(init_emb, hidden_channels, heads)\n\n    # Middle GATv2Conv layers\n    self.num_mid_layers = num_mid_layers\n    if num_mid_layers &gt; 0:\n        self.conv_mid_layers = nn.ModuleList()\n        for _ in range(num_mid_layers):\n            self.conv_mid_layers.append(SkipGAT(heads * hidden_channels, hidden_channels, heads))\n\n    # Last GATv2Conv layer\n    self.conv_last = SkipGAT(heads * hidden_channels, out_channels, heads)\n\n    # Finalize node embeddings\n    self.node_final = HeteroDictLinear(heads * out_channels, out_channels, types=(\"tx\", \"bd\"))\n</code></pre>"},{"location":"api/models/#segger.models.Segger.decode","title":"decode","text":"<pre><code>decode(z_dict, edge_index)\n</code></pre> <p>Decode the node embeddings to predict edge values.</p> <p>Parameters:</p> Name Type Description Default <code>z_dict</code> <code>dict[str, Tensor]</code> <p>Node embeddings for each node type.</p> required <code>edge_index</code> <code>EdgeIndex</code> <p>Edge label indices.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Predicted edge values.</p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def decode(\n    self,\n    z_dict: dict[str, Tensor],\n    edge_index: Union[Tensor],\n) -&gt; Tensor:\n    \"\"\"\n    Decode the node embeddings to predict edge values.\n\n    Args:\n        z_dict (dict[str, Tensor]): Node embeddings for each node type.\n        edge_index (EdgeIndex): Edge label indices.\n\n    Returns:\n        Tensor: Predicted edge values.\n    \"\"\"\n    z_left = z_dict[\"tx\"][edge_index[0]]\n    z_right = z_dict[\"bd\"][edge_index[1]]\n    return (z_left * z_right).sum(dim=-1)\n</code></pre>"},{"location":"api/models/#segger.models.Segger.forward","title":"forward","text":"<pre><code>forward(x_dict, edge_index_dict)\n</code></pre> <p>Forward pass for the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>x_dict</code> <code>dict[str, Tensor]</code> <p>Node features for each node type.</p> required <code>edge_index_dict</code> <code>dict[str, Tensor]</code> <p>Edge indices for each edge type.</p> required Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def forward(\n    self,\n    x_dict: dict[str, Tensor],\n    edge_index_dict: dict[str, Tensor],\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Forward pass for the Segger model.\n\n    Args:\n        x_dict (dict[str, Tensor]): Node features for each node type.\n        edge_index_dict (dict[str, Tensor]): Edge indices for each edge type.\n    \"\"\"\n\n    x_dict = {key: self.node_init[key](x) for key, x in x_dict.items()}\n\n    x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n\n    x_dict = self.conv1(x_dict, edge_index_dict)\n\n    if self.num_mid_layers &gt; 0:\n        for i in range(self.num_mid_layers):\n            x_dict = self.conv_mid_layers[i](x_dict, edge_index_dict)\n\n    x_dict = self.conv_last(x_dict, edge_index_dict)\n\n    x_dict = self.node_final(x_dict)\n\n    return x_dict\n</code></pre>"},{"location":"api/prediction/","title":"segger.prediction","text":"<p>prediction module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p> <p>Prediction module for Segger.</p> <p>Contains prediction scripts and utilities for the Segger model.</p>"},{"location":"api/prediction/#segger.prediction.load_model","title":"load_model","text":"<pre><code>load_model(checkpoint_path)\n</code></pre> <p>Load a LitSegger model from a checkpoint.</p>"},{"location":"api/prediction/#segger.prediction.load_model--parameters","title":"Parameters","text":"<p>checkpoint_path : str     Specific checkpoint file to load, or directory where the model checkpoints are stored.     If directory, the latest checkpoint is loaded.</p>"},{"location":"api/prediction/#segger.prediction.load_model--returns","title":"Returns","text":"<p>LitSegger     The loaded LitSegger model.</p>"},{"location":"api/prediction/#segger.prediction.load_model--raises","title":"Raises","text":"<p>FileNotFoundError     If the specified checkpoint file does not exist.</p> Source code in <code>src/segger/prediction/predict_parquet.py</code> <pre><code>def load_model(checkpoint_path: str) -&gt; LitSegger:\n    \"\"\"\n    Load a LitSegger model from a checkpoint.\n\n    Parameters\n    ----------\n    checkpoint_path : str\n        Specific checkpoint file to load, or directory where the model checkpoints are stored.\n        If directory, the latest checkpoint is loaded.\n\n    Returns\n    -------\n    LitSegger\n        The loaded LitSegger model.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified checkpoint file does not exist.\n    \"\"\"\n    checkpoint_path = Path(checkpoint_path)\n    msg = f\"No checkpoint found at {checkpoint_path}. Please make sure you've provided the correct path.\"\n\n    # Get last checkpoint if directory is provided\n    if os.path.isdir(checkpoint_path):\n        checkpoints = glob.glob(str(checkpoint_path / \"*.ckpt\"))\n        if len(checkpoints) == 0:\n            raise FileNotFoundError(msg)\n\n        # Sort checkpoints by epoch and step\n        def sort_order(c):\n            match = re.match(r\".*epoch=(\\d+)-step=(\\d+).ckpt\", c)\n            return int(match[1]), int(match[2])\n\n        checkpoint_path = Path(sorted(checkpoints, key=sort_order)[-1])\n    elif not checkpoint_path.exists():\n        raise FileExistsError(msg)\n\n    # Load model from checkpoint\n    lit_segger = LitSegger.load_from_checkpoint(\n        checkpoint_path=checkpoint_path,\n    )\n\n    return lit_segger\n</code></pre>"},{"location":"api/prediction/#segger.prediction.segment","title":"segment","text":"<pre><code>segment(model, dm, save_dir, seg_tag, transcript_file, score_cut=0.5, use_cc=True, file_format='', save_transcripts=True, save_anndata=True, save_cell_masks=False, receptive_field={'k_bd': 4, 'dist_bd': 10, 'k_tx': 5, 'dist_tx': 3}, knn_method='cuda', verbose=False, gpu_ids=['0'], **anndata_kwargs)\n</code></pre> <p>Perform segmentation using the model, save transcripts, AnnData, and cell masks as needed, and log the parameters used during segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LitSegger</code> <p>The trained segmentation model.</p> required <code>dm</code> <code>SeggerDataModule</code> <p>The SeggerDataModule instance for data loading.</p> required <code>save_dir</code> <code>Union[str, Path]</code> <p>Directory to save the final segmentation results.</p> required <code>seg_tag</code> <code>str</code> <p>Tag to include in the saved filename.</p> required <code>transcript_file</code> <code>Union[str, Path]</code> <p>Path to the transcripts Parquet file.</p> required <code>score_cut</code> <code>float</code> <p>The threshold for assigning transcripts to cells based on                          similarity scores. Defaults to 0.5.</p> <code>0.5</code> <code>use_cc</code> <code>bool</code> <p>If True, perform connected components analysis for unassigned                      transcripts. Defaults to True.</p> <code>True</code> <code>save_transcripts</code> <code>bool</code> <p>Whether to save the transcripts as Parquet. Defaults to True.</p> <code>True</code> <code>save_anndata</code> <code>bool</code> <p>Whether to save the results in AnnData format. Defaults to True.</p> <code>True</code> <code>save_cell_masks</code> <code>bool</code> <p>Save cell masks as Dask Geopandas Parquet. Defaults to False.</p> <code>False</code> <code>receptive_field</code> <code>dict</code> <p>Defines the receptive field for transcript-cell and                               transcript-transcript relations. Defaults to                               {'k_bd': 4, 'dist_bd': 10, 'k_tx': 5, 'dist_tx': 3}.</p> <code>{'k_bd': 4, 'dist_bd': 10, 'k_tx': 5, 'dist_tx': 3}</code> <code>knn_method</code> <code>str</code> <p>The method to use for nearest neighbors ('cuda' or 'kd_tree').                         Defaults to 'cuda'.</p> <code>'cuda'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose status updates. Defaults to False.</p> <code>False</code> <code>**anndata_kwargs</code> <p>Additional keyword arguments passed to the <code>create_anndata</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Saves the result to disk in various formats and logs the parameter choices.</p> Source code in <code>src/segger/prediction/predict_parquet.py</code> <pre><code>def segment(\n    model: LitSegger,\n    dm: SeggerDataModule,\n    save_dir: Union[str, Path],\n    seg_tag: str,\n    transcript_file: Union[str, Path],\n    score_cut: float = 0.5,\n    use_cc: bool = True,\n    file_format: str = \"\",\n    save_transcripts: bool = True,\n    save_anndata: bool = True,\n    save_cell_masks: bool = False,  # Placeholder for future implementation\n    receptive_field: dict = {\"k_bd\": 4, \"dist_bd\": 10, \"k_tx\": 5, \"dist_tx\": 3},\n    knn_method: str = \"cuda\",\n    verbose: bool = False,\n    gpu_ids: list = [\"0\"],\n    **anndata_kwargs,\n) -&gt; None:\n    \"\"\"\n    Perform segmentation using the model, save transcripts, AnnData, and cell masks as needed,\n    and log the parameters used during segmentation.\n\n    Args:\n        model (LitSegger): The trained segmentation model.\n        dm (SeggerDataModule): The SeggerDataModule instance for data loading.\n        save_dir (Union[str, Path]): Directory to save the final segmentation results.\n        seg_tag (str): Tag to include in the saved filename.\n        transcript_file (Union[str, Path]): Path to the transcripts Parquet file.\n        score_cut (float, optional): The threshold for assigning transcripts to cells based on\n                                     similarity scores. Defaults to 0.5.\n        use_cc (bool, optional): If True, perform connected components analysis for unassigned\n                                 transcripts. Defaults to True.\n        save_transcripts (bool, optional): Whether to save the transcripts as Parquet. Defaults to True.\n        save_anndata (bool, optional): Whether to save the results in AnnData format. Defaults to True.\n        save_cell_masks (bool, optional): Save cell masks as Dask Geopandas Parquet. Defaults to False.\n        receptive_field (dict, optional): Defines the receptive field for transcript-cell and\n                                          transcript-transcript relations. Defaults to\n                                          {'k_bd': 4, 'dist_bd': 10, 'k_tx': 5, 'dist_tx': 3}.\n        knn_method (str, optional): The method to use for nearest neighbors ('cuda' or 'kd_tree').\n                                    Defaults to 'cuda'.\n        verbose (bool, optional): Whether to print verbose status updates. Defaults to False.\n        **anndata_kwargs: Additional keyword arguments passed to the `create_anndata` function.\n\n    Returns:\n        None. Saves the result to disk in various formats and logs the parameter choices.\n    \"\"\"\n\n    start_time = time()\n\n    # Create a subdirectory with important parameter info (receptive field values)\n    sub_dir_name = f\"{seg_tag}_{score_cut}_{use_cc}_{receptive_field['k_bd']}_{receptive_field['dist_bd']}_{receptive_field['k_tx']}_{receptive_field['dist_tx']}_{datetime.now().strftime('%Y%m%d')}\"\n    save_dir = Path(save_dir) / sub_dir_name\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Paths for saving the output_ddf and edge_index Parquet files\n    output_ddf_save_path = save_dir / \"transcripts_df.parquet\"\n    edge_index_save_path = save_dir / \"edge_index.parquet\"\n\n    if output_ddf_save_path.exists():\n        warnings.warn(f\"Removing existing file: {output_ddf_save_path}\")\n        shutil.rmtree(output_ddf_save_path)\n\n    if use_cc:\n        if edge_index_save_path.exists():\n            warnings.warn(f\"Removing existing file: {edge_index_save_path}\")\n            shutil.rmtree(edge_index_save_path)\n\n    if verbose:\n        print(f\"Starting segmentation for {seg_tag}...\")\n\n    # Step 1: Load the data loaders from the SeggerDataModule\n    step_start_time = time()\n    train_dataloader = dm.train_dataloader()\n    val_dataloader = dm.val_dataloader()\n    test_dataloader = dm.test_dataloader()\n\n    # Loop through the data loaders (train, val, and test)\n    for loader_name, loader in zip(\n        [\"Train\", \"Validation\", \"Test\"], [train_dataloader, val_dataloader, test_dataloader]\n    ):\n        # for loader_name, loader in zip(['Test'], [test_dataloader]):\n        if verbose:\n            print(f\"Processing {loader_name} data...\")\n\n        for batch in tqdm(loader, desc=f\"Processing {loader_name} batches\"):\n            gpu_id = random.choice(gpu_ids)\n            # Call predict_batch for each batch\n            predict_batch(\n                model,\n                batch,\n                score_cut,\n                receptive_field,\n                use_cc=use_cc,\n                knn_method=knn_method,\n                edge_index_save_path=edge_index_save_path,\n                output_ddf_save_path=output_ddf_save_path,\n                gpu_id=gpu_id,\n            )\n\n    if verbose:\n        elapsed_time = time() - step_start_time\n        print(f\"Batch processing completed in {elapsed_time:.2f} seconds.\")\n\n    seg_final_dd = pd.read_parquet(output_ddf_save_path)\n\n    step_start_time = time()\n    if verbose:\n        print(f\"Applying max score selection logic...\")\n    output_ddf_save_path = save_dir / \"transcripts_df.parquet\"\n\n    seg_final_dd = pd.read_parquet(output_ddf_save_path)\n\n    seg_final_filtered = seg_final_dd.sort_values(\"score\", ascending=False).drop_duplicates(\n        subset=\"transcript_id\", keep=\"first\"\n    )\n\n    if verbose:\n        elapsed_time = time() - step_start_time\n        print(f\"Max score selection completed in {elapsed_time:.2f} seconds.\")\n\n    # Step 3: Load the transcripts DataFrame and merge results\n\n    if verbose:\n        print(f\"Loading transcripts from {transcript_file}...\")\n\n    transcripts_df = pd.read_parquet(transcript_file)\n    transcripts_df[\"transcript_id\"] = transcripts_df[\"transcript_id\"].astype(str)\n\n    step_start_time = time()\n    if verbose:\n        print(f\"Merging segmentation results with transcripts...\")\n\n    # Outer merge to include all transcripts, even those without assigned cell ids\n    transcripts_df_filtered = transcripts_df.merge(seg_final_filtered, on=\"transcript_id\", how=\"outer\")\n\n    if verbose:\n        elapsed_time = time() - step_start_time\n        print(f\"Merged segmentation results with transcripts in {elapsed_time:.2f} seconds.\")\n\n    if use_cc:\n\n        step_start_time = time()\n        if verbose:\n            print(f\"Computing connected components for unassigned transcripts...\")\n        # Load edge indices from saved Parquet\n        edge_index_dd = pd.read_parquet(edge_index_save_path)\n\n        # Step 2: Get unique transcript_ids from edge_index_dd and their positional indices\n        transcript_ids_in_edges = pd.concat([edge_index_dd[\"source\"], edge_index_dd[\"target\"]]).unique()\n\n        # Create a lookup table with unique indices\n        lookup_table = pd.Series(data=range(len(transcript_ids_in_edges)), index=transcript_ids_in_edges).to_dict()\n\n        # Map source and target to positional indices\n        edge_index_dd[\"index_source\"] = edge_index_dd[\"source\"].map(lookup_table)\n        edge_index_dd[\"index_target\"] = edge_index_dd[\"target\"].map(lookup_table)\n        # Step 3: Compute connected components for transcripts involved in edges\n        source_indices = np.asarray(edge_index_dd[\"index_source\"])\n        target_indices = np.asarray(edge_index_dd[\"index_target\"])\n        data_cp = np.ones(len(source_indices), dtype=np.float32)\n\n        # Create the sparse COO matrix\n        coo_cp_matrix = scipy_coo_matrix(\n            (data_cp, (source_indices, target_indices)),\n            shape=(len(transcript_ids_in_edges), len(transcript_ids_in_edges)),\n        )\n\n        # Use CuPy's connected components algorithm to compute components\n        n, comps = cc(coo_cp_matrix, directed=True, connection=\"strong\")\n        if verbose:\n            elapsed_time = time() - step_start_time\n            print(f\"Computed connected components for unassigned transcripts in {elapsed_time:.2f} seconds.\")\n\n        step_start_time = time()\n        if verbose:\n            print(f\"The rest...\")\n        # # Step 4: Map back the component labels to the original transcript_ids\n\n        def _get_id():\n            \"\"\"Generate a random Xenium-style ID.\"\"\"\n            return \"\".join(np.random.choice(list(\"abcdefghijklmnopqrstuvwxyz\"), 8)) + \"-nx\"\n\n        new_ids = np.array([_get_id() for _ in range(n)])\n        comp_labels = new_ids[comps]\n        comp_labels = pd.Series(comp_labels, index=transcript_ids_in_edges)\n        # Step 5: Handle only unassigned transcripts in transcripts_df_filtered\n        unassigned_mask = transcripts_df_filtered[\"segger_cell_id\"].isna()\n\n        unassigned_transcripts_df = transcripts_df_filtered.loc[unassigned_mask, [\"transcript_id\"]]\n\n        # Step 6: Map component labels only to unassigned transcript_ids\n        new_segger_cell_ids = unassigned_transcripts_df[\"transcript_id\"].map(comp_labels)\n\n        # Step 7: Create a DataFrame with updated 'segger_cell_id' for unassigned transcripts\n        unassigned_transcripts_df = unassigned_transcripts_df.assign(segger_cell_id=new_segger_cell_ids)\n\n        # Step 8: Merge this DataFrame back into the original to update only the unassigned segger_cell_id\n\n        # Merging the updates back to the original DataFrame\n        transcripts_df_filtered = transcripts_df_filtered.merge(\n            unassigned_transcripts_df[[\"transcript_id\", \"segger_cell_id\"]],\n            on=\"transcript_id\",\n            how=\"left\",  # Perform a left join to only update the unassigned rows\n            suffixes=(\"\", \"_new\"),  # Suffix for new column to avoid overwriting\n        )\n\n        # Step 9: Fill missing segger_cell_id values with the updated values from the merge\n        transcripts_df_filtered[\"segger_cell_id\"] = transcripts_df_filtered[\"segger_cell_id\"].fillna(\n            transcripts_df_filtered[\"segger_cell_id_new\"]\n        )\n\n        transcripts_df_filtered = transcripts_df_filtered.drop(columns=[\"segger_cell_id_new\"])\n\n        if verbose:\n            elapsed_time = time() - step_start_time\n            print(f\"The rest computed in {elapsed_time:.2f} seconds.\")\n\n    # Step 5: Save the merged results based on options\n    transcripts_df_filtered[\"segger_cell_id\"] = transcripts_df_filtered[\"segger_cell_id\"].fillna(\"UNASSIGNED\")\n    # transcripts_df_filtered = filter_transcripts(transcripts_df_filtered, qv=qv)\n\n    if save_transcripts:\n        if verbose:\n            step_start_time = time()\n            print(f\"Saving transcripts.parquet...\")\n        transcripts_save_path = save_dir / \"segger_transcripts.parquet\"\n        # transcripts_df_filtered = transcripts_df_filtered.repartition(npartitions=100)\n        transcripts_df_filtered.to_parquet(\n            transcripts_save_path,\n            engine=\"pyarrow\",  # PyArrow is faster and recommended\n            compression=\"snappy\",  # Use snappy compression for speed\n            # write_index=False,  # Skip writing index if not needed\n            # append=False,  # Set to True if you're appending to an existing Parquet file\n            # overwrite=True,\n        )  # Dask handles Parquet well\n        if verbose:\n            elapsed_time = time() - step_start_time\n            print(f\"Saved trasncripts.parquet in {elapsed_time:.2f} seconds.\")\n\n    if save_anndata:\n        if verbose:\n            step_start_time = time()\n            print(f\"Saving anndata object...\")\n        anndata_save_path = save_dir / \"segger_adata.h5ad\"\n        segger_adata = create_anndata(transcripts_df_filtered, **anndata_kwargs)  # Compute for AnnData\n        segger_adata.write(anndata_save_path)\n        if verbose:\n            elapsed_time = time() - step_start_time\n            print(f\"Saved anndata object in {elapsed_time:.2f} seconds.\")\n\n    if save_cell_masks:\n        if verbose:\n            step_start_time = time()\n            print(f\"Computing and saving cell masks anndata object...\")\n        # Placeholder for future cell masks implementation as Dask Geopandas Parquet\n        boundaries_gdf = generate_boundaries(transcripts_df_filtered)\n        cell_masks_save_path = save_dir / \"segger_cell_boundaries.parquet\"\n\n        boundaries_gdf.to_parquet(cell_masks_save_path)\n        if verbose:\n            elapsed_time = time() - step_start_time\n            print(f\"Saved cell masks in {elapsed_time:.2f} seconds.\")\n\n    if verbose:\n        elapsed_time = time() - step_start_time\n        print(f\"Results saved in {elapsed_time:.2f} seconds at {save_dir}.\")\n\n    # Step 6: Save segmentation parameters as a JSON log\n    log_data = {\n        \"seg_tag\": seg_tag,\n        \"score_cut\": score_cut,\n        \"use_cc\": use_cc,\n        \"receptive_field\": receptive_field,\n        \"knn_method\": knn_method,\n        \"save_transcripts\": save_transcripts,\n        \"save_anndata\": save_anndata,\n        \"save_cell_masks\": save_cell_masks,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n\n    log_path = save_dir / \"segmentation_log.json\"\n    with open(log_path, \"w\") as log_file:\n        json.dump(log_data, log_file, indent=4)\n\n    # Step 7: Garbage collection and memory cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # Total time taken for the segmentation process\n    if verbose:\n        total_time = time() - start_time\n        print(f\"Total segmentation process completed in {total_time:.2f} seconds.\")\n</code></pre>"},{"location":"api/training/","title":"segger.training","text":"<p>training module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/training/#segger.training.LitSegger","title":"LitSegger","text":"<pre><code>LitSegger(learning_rate=0.001, **kwargs)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>LitSegger is a PyTorch Lightning module for training and validating the Segger model.</p>"},{"location":"api/training/#segger.training.LitSegger--attributes","title":"Attributes","text":"<p>model : Segger     The Segger model wrapped with PyTorch Geometric's to_hetero for heterogeneous graph support. validation_step_outputs : list     A list to store outputs from the validation steps. criterion : torch.nn.Module     The loss function used for training, specifically BCEWithLogitsLoss.</p> <p>Initializes the LitSegger module with the given parameters.</p>"},{"location":"api/training/#segger.training.LitSegger--parameters","title":"Parameters","text":"<p>learning_rate : float     The learning rate for the optimizer. **kwargs : dict     Keyword arguments for initializing the module. Specific parameters     depend on whether the module is initialized with new parameters or components.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def __init__(self, learning_rate: float = 1e-3, **kwargs):\n    \"\"\"\n    Initializes the LitSegger module with the given parameters.\n\n    Parameters\n    ----------\n    learning_rate : float\n        The learning rate for the optimizer.\n    **kwargs : dict\n        Keyword arguments for initializing the module. Specific parameters\n        depend on whether the module is initialized with new parameters or components.\n    \"\"\"\n    super().__init__()\n    new_args = inspect.getfullargspec(self.from_new)[0][1:]\n    cmp_args = inspect.getfullargspec(self.from_components)[0][1:]\n\n    # Initialize with new parameters (ensure num_tx_tokens is passed here)\n    if set(kwargs.keys()) == set(new_args):\n        self.from_new(**kwargs)\n\n    # Initialize with existing components\n    elif set(kwargs.keys()) == set(cmp_args):\n        self.from_components(**kwargs)\n\n    # Handle invalid arguments\n    else:\n        raise ValueError(\n            f\"Supplied kwargs do not match either constructor. Should be one of '{new_args}' or '{cmp_args}'.\"\n        )\n\n    self.validation_step_outputs = []\n    self.criterion = torch.nn.BCEWithLogitsLoss()\n    self.learning_rate = learning_rate\n</code></pre>"},{"location":"api/training/#segger.training.LitSegger.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configures the optimizer for training.</p>"},{"location":"api/training/#segger.training.LitSegger.configure_optimizers--returns","title":"Returns","text":"<p>torch.optim.Optimizer     The optimizer for training.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def configure_optimizers(self) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Configures the optimizer for training.\n\n    Returns\n    -------\n    torch.optim.Optimizer\n        The optimizer for training.\n    \"\"\"\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    return optimizer\n</code></pre>"},{"location":"api/training/#segger.training.LitSegger.forward","title":"forward","text":"<pre><code>forward(batch)\n</code></pre> <p>Forward pass for the batch of data.</p>"},{"location":"api/training/#segger.training.LitSegger.forward--parameters","title":"Parameters","text":"<p>batch : SpatialTranscriptomicsDataset     The batch of data, including node features and edge indices.</p>"},{"location":"api/training/#segger.training.LitSegger.forward--returns","title":"Returns","text":"<p>torch.Tensor     The output of the model.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def forward(self, batch: SpatialTranscriptomicsDataset) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for the batch of data.\n\n    Parameters\n    ----------\n    batch : SpatialTranscriptomicsDataset\n        The batch of data, including node features and edge indices.\n\n    Returns\n    -------\n    torch.Tensor\n        The output of the model.\n    \"\"\"\n    z = self.model(batch.x_dict, batch.edge_index_dict)\n    edge_label_index = batch[\"tx\", \"belongs\", \"bd\"].edge_label_index\n    output = self.model.decode(z, edge_label_index)\n    return output\n</code></pre>"},{"location":"api/training/#segger.training.LitSegger.from_components","title":"from_components","text":"<pre><code>from_components(model)\n</code></pre> <p>Initializes the LitSegger module with existing Segger components.</p>"},{"location":"api/training/#segger.training.LitSegger.from_components--parameters","title":"Parameters","text":"<p>model : Segger     The Segger model to be used.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def from_components(self, model: Segger):\n    \"\"\"\n    Initializes the LitSegger module with existing Segger components.\n\n    Parameters\n    ----------\n    model : Segger\n        The Segger model to be used.\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"api/training/#segger.training.LitSegger.from_new","title":"from_new","text":"<pre><code>from_new(num_node_features, init_emb, hidden_channels, out_channels, heads, num_mid_layers, aggr, is_token_based=True)\n</code></pre> <p>Initializes the LitSegger module with new parameters.</p>"},{"location":"api/training/#segger.training.LitSegger.from_new--parameters","title":"Parameters","text":"<p>num_node_features : dict[str, int]     Number of node features for each node type. init_emb : int     Initial embedding size. hidden_channels : int     Number of hidden channels. out_channels : int     Number of output channels. heads : int     Number of attention heads. num_mid_layers: int     Number of hidden layers (excluding first and last layers). aggr : str     Aggregation method for heterogeneous graph conversion. is_token_based : bool     Whether the model is using token-based embeddings or scRNAseq embeddings.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def from_new(\n    self,\n    num_node_features: dict[str, int],\n    init_emb: int,\n    hidden_channels: int,\n    out_channels: int,\n    heads: int,\n    num_mid_layers: int,\n    aggr: str,\n    is_token_based: bool = True,\n):\n    \"\"\"\n    Initializes the LitSegger module with new parameters.\n\n    Parameters\n    ----------\n    num_node_features : dict[str, int]\n        Number of node features for each node type.\n    init_emb : int\n        Initial embedding size.\n    hidden_channels : int\n        Number of hidden channels.\n    out_channels : int\n        Number of output channels.\n    heads : int\n        Number of attention heads.\n    num_mid_layers: int\n        Number of hidden layers (excluding first and last layers).\n    aggr : str\n        Aggregation method for heterogeneous graph conversion.\n    is_token_based : bool\n        Whether the model is using token-based embeddings or scRNAseq embeddings.\n    \"\"\"\n    # Create the Segger model (ensure num_tx_tokens is passed here)\n    self.model = Segger(\n        num_node_features=num_node_features,\n        init_emb=init_emb,\n        hidden_channels=hidden_channels,\n        out_channels=out_channels,\n        heads=heads,\n        num_mid_layers=num_mid_layers,\n        is_token_based=is_token_based,\n    )\n    # Save hyperparameters\n    self.save_hyperparameters()\n</code></pre>"},{"location":"api/training/#segger.training.LitSegger.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Defines the training step.</p>"},{"location":"api/training/#segger.training.LitSegger.training_step--parameters","title":"Parameters","text":"<p>batch : Any     The batch of data. batch_idx : int     The index of the batch.</p>"},{"location":"api/training/#segger.training.LitSegger.training_step--returns","title":"Returns","text":"<p>torch.Tensor     The loss value for the current training step.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Defines the training step.\n\n    Parameters\n    ----------\n    batch : Any\n        The batch of data.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss value for the current training step.\n    \"\"\"\n    # Get edge labels\n    edge_label_index = batch[\"tx\", \"belongs\", \"bd\"].edge_label_index\n    edge_label = batch[\"tx\", \"belongs\", \"bd\"].edge_label\n\n    # Forward pass to get the logits\n    z = self.model(batch.x_dict, batch.edge_index_dict)\n    output = self.model.decode(z, edge_label_index)\n\n    # Compute binary cross-entropy loss with logits (no sigmoid here)\n    loss = self.criterion(output, edge_label)\n\n    # Log the training loss\n    self.log(\"train_loss\", loss, prog_bar=True, batch_size=batch.num_graphs)\n    return loss\n</code></pre>"},{"location":"api/training/#segger.training.LitSegger.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Defines the validation step.</p>"},{"location":"api/training/#segger.training.LitSegger.validation_step--parameters","title":"Parameters","text":"<p>batch : Any     The batch of data. batch_idx : int     The index of the batch.</p>"},{"location":"api/training/#segger.training.LitSegger.validation_step--returns","title":"Returns","text":"<p>torch.Tensor     The loss value for the current validation step.</p> Source code in <code>src/segger/training/train.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Defines the validation step.\n\n    Parameters\n    ----------\n    batch : Any\n        The batch of data.\n    batch_idx : int\n        The index of the batch.\n\n    Returns\n    -------\n    torch.Tensor\n        The loss value for the current validation step.\n    \"\"\"\n    # Get edge labels\n    edge_label_index = batch[\"tx\", \"belongs\", \"bd\"].edge_label_index\n    edge_label = batch[\"tx\", \"belongs\", \"bd\"].edge_label\n\n    # Forward pass to get the logits\n    z = self.model(batch.x_dict, batch.edge_index_dict)\n    output = self.model.decode(z, edge_label_index)\n\n    # Compute binary cross-entropy loss with logits (no sigmoid here)\n    loss = self.criterion(output, edge_label)\n\n    # Apply sigmoid to logits for AUROC and F1 metrics\n    out_values_prob = torch.sigmoid(output)\n\n    # Compute metrics\n    auroc = torchmetrics.AUROC(task=\"binary\")\n    auroc_res = auroc(out_values_prob, edge_label)\n\n    f1 = F1Score(task=\"binary\").to(self.device)\n    f1_res = f1(out_values_prob, edge_label)\n\n    # Log validation metrics\n    self.log(\"validation_loss\", loss, batch_size=batch.num_graphs)\n    self.log(\"validation_auroc\", auroc_res, prog_bar=True, batch_size=batch.num_graphs)\n    self.log(\"validation_f1\", f1_res, prog_bar=True, batch_size=batch.num_graphs)\n\n    return loss\n</code></pre>"},{"location":"api/training/#segger.training.Segger","title":"Segger","text":"<pre><code>Segger(num_node_features, init_emb=16, hidden_channels=32, num_mid_layers=3, out_channels=32, heads=3, is_token_based=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Initializes the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>num_node_features</code> <code>dict[str, int]</code> <p>Number of node features for each node type.</p> required <code>init_emb</code> <code>int)         </code> <p>Initial embedding size for both 'tx' and boundary (non-token) nodes.</p> <code>16</code> <code>hidden_channels</code> <code>int)  </code> <p>Number of hidden channels.</p> <code>32</code> <code>num_mid_layers</code> <code>int)   </code> <p>Number of hidden layers (excluding first and last layers).</p> <code>3</code> <code>out_channels</code> <code>int)     </code> <p>Number of output channels.</p> <code>32</code> <code>heads</code> <code>int)            </code> <p>Number of attention heads.</p> <code>3</code> <code>is_token_based</code> <code>bool)  </code> <p>Whether the model is using token-based embeddings or scRNAseq embeddings.</p> <code>True</code> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def __init__(\n    self,\n    num_node_features: dict[str, int],\n    init_emb: int = 16,\n    hidden_channels: int = 32,\n    num_mid_layers: int = 3,\n    out_channels: int = 32,\n    heads: int = 3,\n    is_token_based: bool = True,\n):\n    \"\"\"\n    Initializes the Segger model.\n\n    Args:\n        num_node_features (dict[str, int]): Number of node features for each node type.\n        init_emb (int)         : Initial embedding size for both 'tx' and boundary (non-token) nodes.\n        hidden_channels (int)  : Number of hidden channels.\n        num_mid_layers (int)   : Number of hidden layers (excluding first and last layers).\n        out_channels (int)     : Number of output channels.\n        heads (int)            : Number of attention heads.\n        is_token_based (bool)  : Whether the model is using token-based embeddings or scRNAseq embeddings.\n    \"\"\"\n    super().__init__()\n\n    # Initialize node embeddings\n    if is_token_based:\n        # Using token-based embeddings for transcript ('tx') nodes\n        self.node_init = nn.ModuleDict(\n            {\n                \"tx\": nn.Embedding(num_node_features[\"tx\"], init_emb),\n                \"bd\": nn.Linear(num_node_features[\"bd\"], init_emb),\n            }\n        )\n    else:\n        # Using scRNAseq embeddings (i.e. prior biological knowledge) for transcript ('tx') nodes\n        self.node_init = nn.ModuleDict(\n            {\n                \"tx\": nn.Linear(num_node_features[\"tx\"], init_emb),\n                \"bd\": nn.Linear(num_node_features[\"bd\"], init_emb),\n            }\n        )\n\n    # First GATv2Conv layer\n    self.conv1 = SkipGAT(init_emb, hidden_channels, heads)\n\n    # Middle GATv2Conv layers\n    self.num_mid_layers = num_mid_layers\n    if num_mid_layers &gt; 0:\n        self.conv_mid_layers = nn.ModuleList()\n        for _ in range(num_mid_layers):\n            self.conv_mid_layers.append(SkipGAT(heads * hidden_channels, hidden_channels, heads))\n\n    # Last GATv2Conv layer\n    self.conv_last = SkipGAT(heads * hidden_channels, out_channels, heads)\n\n    # Finalize node embeddings\n    self.node_final = HeteroDictLinear(heads * out_channels, out_channels, types=(\"tx\", \"bd\"))\n</code></pre>"},{"location":"api/training/#segger.training.Segger.decode","title":"decode","text":"<pre><code>decode(z_dict, edge_index)\n</code></pre> <p>Decode the node embeddings to predict edge values.</p> <p>Parameters:</p> Name Type Description Default <code>z_dict</code> <code>dict[str, Tensor]</code> <p>Node embeddings for each node type.</p> required <code>edge_index</code> <code>EdgeIndex</code> <p>Edge label indices.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Predicted edge values.</p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def decode(\n    self,\n    z_dict: dict[str, Tensor],\n    edge_index: Union[Tensor],\n) -&gt; Tensor:\n    \"\"\"\n    Decode the node embeddings to predict edge values.\n\n    Args:\n        z_dict (dict[str, Tensor]): Node embeddings for each node type.\n        edge_index (EdgeIndex): Edge label indices.\n\n    Returns:\n        Tensor: Predicted edge values.\n    \"\"\"\n    z_left = z_dict[\"tx\"][edge_index[0]]\n    z_right = z_dict[\"bd\"][edge_index[1]]\n    return (z_left * z_right).sum(dim=-1)\n</code></pre>"},{"location":"api/training/#segger.training.Segger.forward","title":"forward","text":"<pre><code>forward(x_dict, edge_index_dict)\n</code></pre> <p>Forward pass for the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>x_dict</code> <code>dict[str, Tensor]</code> <p>Node features for each node type.</p> required <code>edge_index_dict</code> <code>dict[str, Tensor]</code> <p>Edge indices for each edge type.</p> required Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def forward(\n    self,\n    x_dict: dict[str, Tensor],\n    edge_index_dict: dict[str, Tensor],\n) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Forward pass for the Segger model.\n\n    Args:\n        x_dict (dict[str, Tensor]): Node features for each node type.\n        edge_index_dict (dict[str, Tensor]): Edge indices for each edge type.\n    \"\"\"\n\n    x_dict = {key: self.node_init[key](x) for key, x in x_dict.items()}\n\n    x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n\n    x_dict = self.conv1(x_dict, edge_index_dict)\n\n    if self.num_mid_layers &gt; 0:\n        for i in range(self.num_mid_layers):\n            x_dict = self.conv_mid_layers[i](x_dict, edge_index_dict)\n\n    x_dict = self.conv_last(x_dict, edge_index_dict)\n\n    x_dict = self.node_final(x_dict)\n\n    return x_dict\n</code></pre>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/utils.py</code> <pre><code>def __init__(\n    self, root: str, transform: Callable = None, pre_transform: Callable = None, pre_filter: Callable = None\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(os.path.join(self.processed_dir, self.processed_file_names[idx]))\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/training/#segger.training.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>src/segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/validation/","title":"segger.validation","text":"<p>This module handles validation utilities for the Segger tool.</p>"},{"location":"api/validation/#submodules","title":"Submodules","text":"<ul> <li>Utils</li> <li>Xenium Explorer</li> </ul>"},{"location":"api/validation/#api-documentation","title":"API Documentation","text":""},{"location":"api/validation/#segger.validation.annotate_query_with_reference","title":"annotate_query_with_reference","text":"<pre><code>annotate_query_with_reference(reference_adata, query_adata, transfer_column)\n</code></pre> <p>Annotate query AnnData object using a scRNA-seq reference atlas.</p> <ul> <li>reference_adata: ad.AnnData     Reference AnnData object containing the scRNA-seq atlas data.</li> <li>query_adata: ad.AnnData     Query AnnData object containing the data to be annotated.</li> <li>transfer_column: str     The name of the column in the reference atlas's <code>obs</code> to transfer to the query dataset.</li> </ul> <ul> <li>query_adata: ad.AnnData     Annotated query AnnData object with transferred labels and UMAP coordinates from the reference.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def annotate_query_with_reference(\n    reference_adata: ad.AnnData, query_adata: ad.AnnData, transfer_column: str\n) -&gt; ad.AnnData:\n    \"\"\"Annotate query AnnData object using a scRNA-seq reference atlas.\n\n    Args:\n    - reference_adata: ad.AnnData\n        Reference AnnData object containing the scRNA-seq atlas data.\n    - query_adata: ad.AnnData\n        Query AnnData object containing the data to be annotated.\n    - transfer_column: str\n        The name of the column in the reference atlas's `obs` to transfer to the query dataset.\n\n    Returns:\n    - query_adata: ad.AnnData\n        Annotated query AnnData object with transferred labels and UMAP coordinates from the reference.\n    \"\"\"\n    common_genes = list(set(reference_adata.var_names) &amp; set(query_adata.var_names))\n    reference_adata = reference_adata[:, common_genes]\n    query_adata = query_adata[:, common_genes]\n    query_adata.layers[\"raw\"] = query_adata.raw.X if query_adata.raw else query_adata.X\n    query_adata.var[\"raw_counts\"] = query_adata.layers[\"raw\"].sum(axis=0)\n    sc.pp.normalize_total(query_adata, target_sum=1e4)\n    sc.pp.log1p(query_adata)\n    sc.pp.pca(reference_adata)\n    sc.pp.neighbors(reference_adata)\n    sc.tl.umap(reference_adata)\n    sc.tl.ingest(query_adata, reference_adata, obs=transfer_column)\n    query_adata.obsm[\"X_umap\"] = query_adata.obsm[\"X_umap\"]\n    return query_adata\n</code></pre>"},{"location":"api/validation/#segger.validation.calculate_contamination","title":"calculate_contamination","text":"<pre><code>calculate_contamination(adata, markers, radius=15, n_neighs=10, celltype_column='celltype_major', num_cells=10000)\n</code></pre> <p>Calculate normalized contamination from neighboring cells of different cell types based on positive markers.</p> <ul> <li>adata: ad.AnnData     Annotated data object with raw counts and cell type information.</li> <li>markers: dict     Dictionary where keys are cell types and values are dictionaries containing:         'positive': list of top x% highly expressed genes         'negative': list of top x% lowly expressed genes.</li> <li>radius: float, default=15     Radius for spatial neighbor calculation.</li> <li>n_neighs: int, default=10     Maximum number of neighbors to consider.</li> <li>celltype_column: str, default='celltype_major'     Column name in the AnnData object representing cell types.</li> <li>num_cells: int, default=10000     Number of cells to randomly select for the calculation.</li> </ul> <ul> <li>contamination_df: pd.DataFrame     DataFrame containing the normalized level of contamination from each cell type to each other cell type.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def calculate_contamination(\n    adata: ad.AnnData,\n    markers: Dict[str, Dict[str, List[str]]],\n    radius: float = 15,\n    n_neighs: int = 10,\n    celltype_column: str = \"celltype_major\",\n    num_cells: int = 10000,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate normalized contamination from neighboring cells of different cell types based on positive markers.\n\n    Args:\n    - adata: ad.AnnData\n        Annotated data object with raw counts and cell type information.\n    - markers: dict\n        Dictionary where keys are cell types and values are dictionaries containing:\n            'positive': list of top x% highly expressed genes\n            'negative': list of top x% lowly expressed genes.\n    - radius: float, default=15\n        Radius for spatial neighbor calculation.\n    - n_neighs: int, default=10\n        Maximum number of neighbors to consider.\n    - celltype_column: str, default='celltype_major'\n        Column name in the AnnData object representing cell types.\n    - num_cells: int, default=10000\n        Number of cells to randomly select for the calculation.\n\n    Returns:\n    - contamination_df: pd.DataFrame\n        DataFrame containing the normalized level of contamination from each cell type to each other cell type.\n    \"\"\"\n    if celltype_column not in adata.obs:\n        raise ValueError(\"Column celltype_column must be present in adata.obs.\")\n    positive_markers = {ct: markers[ct][\"positive\"] for ct in markers}\n    adata.obsm[\"spatial\"] = adata.obs[[\"cell_centroid_x\", \"cell_centroid_y\"]].copy().to_numpy()\n    sq.gr.spatial_neighbors(adata, radius=radius, n_neighs=n_neighs, coord_type=\"generic\")\n    neighbors = adata.obsp[\"spatial_connectivities\"].tolil()\n    raw_counts = adata[:, adata.var_names].layers[\"raw\"].toarray()\n    cell_types = adata.obs[celltype_column]\n    selected_cells = np.random.choice(adata.n_obs, size=min(num_cells, adata.n_obs), replace=False)\n    contamination = {ct: {ct2: 0 for ct2 in positive_markers.keys()} for ct in positive_markers.keys()}\n    negighborings = {ct: {ct2: 0 for ct2 in positive_markers.keys()} for ct in positive_markers.keys()}\n    for cell_idx in selected_cells:\n        cell_type = cell_types[cell_idx]\n        own_markers = set(positive_markers[cell_type])\n        for marker in own_markers:\n            if marker in adata.var_names:\n                total_counts_in_neighborhood = raw_counts[cell_idx, adata.var_names.get_loc(marker)]\n                for neighbor_idx in neighbors.rows[cell_idx]:\n                    total_counts_in_neighborhood += raw_counts[neighbor_idx, adata.var_names.get_loc(marker)]\n                for neighbor_idx in neighbors.rows[cell_idx]:\n                    neighbor_type = cell_types[neighbor_idx]\n                    if cell_type == neighbor_type:\n                        continue\n                    neighbor_markers = set(positive_markers.get(neighbor_type, []))\n                    contamination_markers = own_markers - neighbor_markers\n                    for marker in contamination_markers:\n                        if marker in adata.var_names:\n                            marker_counts_in_neighbor = raw_counts[neighbor_idx, adata.var_names.get_loc(marker)]\n                            if total_counts_in_neighborhood &gt; 0:\n                                contamination[cell_type][neighbor_type] += (\n                                    marker_counts_in_neighbor / total_counts_in_neighborhood\n                                )\n                                negighborings[cell_type][neighbor_type] += 1\n    contamination_df = pd.DataFrame(contamination).T\n    negighborings_df = pd.DataFrame(negighborings).T\n    contamination_df.index.name = \"Source Cell Type\"\n    contamination_df.columns.name = \"Target Cell Type\"\n    return contamination_df / (negighborings_df + 1)\n</code></pre>"},{"location":"api/validation/#segger.validation.calculate_sensitivity","title":"calculate_sensitivity","text":"<pre><code>calculate_sensitivity(adata, purified_markers, max_cells_per_type=1000)\n</code></pre> <p>Calculate the sensitivity of the purified markers for each cell type.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data.</li> <li>purified_markers: dict     Dictionary where keys are cell types and values are lists of purified marker genes.</li> <li>max_cells_per_type: int, default=1000     Maximum number of cells to consider per cell type.</li> </ul> <ul> <li>sensitivity_results: dict     Dictionary with cell types as keys and lists of sensitivity values for each cell.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def calculate_sensitivity(\n    adata: ad.AnnData, purified_markers: Dict[str, List[str]], max_cells_per_type: int = 1000\n) -&gt; Dict[str, List[float]]:\n    \"\"\"Calculate the sensitivity of the purified markers for each cell type.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data.\n    - purified_markers: dict\n        Dictionary where keys are cell types and values are lists of purified marker genes.\n    - max_cells_per_type: int, default=1000\n        Maximum number of cells to consider per cell type.\n\n    Returns:\n    - sensitivity_results: dict\n        Dictionary with cell types as keys and lists of sensitivity values for each cell.\n    \"\"\"\n    sensitivity_results = {cell_type: [] for cell_type in purified_markers.keys()}\n    for cell_type, markers in purified_markers.items():\n        markers = markers[\"positive\"]\n        subset = adata[adata.obs[\"celltype_major\"] == cell_type]\n        if subset.n_obs &gt; max_cells_per_type:\n            cell_indices = np.random.choice(subset.n_obs, max_cells_per_type, replace=False)\n            subset = subset[cell_indices]\n        for cell_counts in subset.X:\n            expressed_markers = np.asarray((cell_counts[subset.var_names.get_indexer(markers)] &gt; 0).sum())\n            sensitivity = expressed_markers / len(markers) if markers else 0\n            sensitivity_results[cell_type].append(sensitivity)\n    return sensitivity_results\n</code></pre>"},{"location":"api/validation/#segger.validation.compute_MECR","title":"compute_MECR","text":"<pre><code>compute_MECR(adata, gene_pairs)\n</code></pre> <p>Compute the Mutually Exclusive Co-expression Rate (MECR) for each gene pair in an AnnData object.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data.</li> <li>gene_pairs: List[Tuple[str, str]]     List of tuples representing gene pairs to evaluate.</li> </ul> <ul> <li>mecr_dict: Dict[Tuple[str, str], float]     Dictionary where keys are gene pairs (tuples) and values are MECR values.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def compute_MECR(adata: ad.AnnData, gene_pairs: List[Tuple[str, str]]) -&gt; Dict[Tuple[str, str], float]:\n    \"\"\"Compute the Mutually Exclusive Co-expression Rate (MECR) for each gene pair in an AnnData object.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data.\n    - gene_pairs: List[Tuple[str, str]]\n        List of tuples representing gene pairs to evaluate.\n\n    Returns:\n    - mecr_dict: Dict[Tuple[str, str], float]\n        Dictionary where keys are gene pairs (tuples) and values are MECR values.\n    \"\"\"\n    mecr_dict = {}\n    gene_expression = adata.to_df()\n    for gene1, gene2 in gene_pairs:\n        expr_gene1 = gene_expression[gene1] &gt; 0\n        expr_gene2 = gene_expression[gene2] &gt; 0\n        both_expressed = (expr_gene1 &amp; expr_gene2).mean()\n        at_least_one_expressed = (expr_gene1 | expr_gene2).mean()\n        mecr = both_expressed / at_least_one_expressed if at_least_one_expressed &gt; 0 else 0\n        mecr_dict[(gene1, gene2)] = mecr\n    return mecr_dict\n</code></pre>"},{"location":"api/validation/#segger.validation.compute_clustering_scores","title":"compute_clustering_scores","text":"<pre><code>compute_clustering_scores(adata, cell_type_column='celltype_major', use_pca=True)\n</code></pre> <p>Compute the Calinski-Harabasz and Silhouette scores for an AnnData object based on the assigned cell types.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data and cell type assignments.</li> <li>cell_type_column: str, default='celltype_major'     Column name in <code>adata.obs</code> that specifies cell types.</li> <li>use_pca: bool, default=True     Whether to use PCA components as features. If False, use the raw data.</li> </ul> <ul> <li>ch_score: float     The Calinski-Harabasz score.</li> <li>sh_score: float     The Silhouette score.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def compute_clustering_scores(\n    adata: ad.AnnData, cell_type_column: str = \"celltype_major\", use_pca: bool = True\n) -&gt; Tuple[float, float]:\n    \"\"\"Compute the Calinski-Harabasz and Silhouette scores for an AnnData object based on the assigned cell types.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data and cell type assignments.\n    - cell_type_column: str, default='celltype_major'\n        Column name in `adata.obs` that specifies cell types.\n    - use_pca: bool, default=True\n        Whether to use PCA components as features. If False, use the raw data.\n\n    Returns:\n    - ch_score: float\n        The Calinski-Harabasz score.\n    - sh_score: float\n        The Silhouette score.\n    \"\"\"\n    if cell_type_column not in adata.obs:\n        raise ValueError(f\"Column '{cell_type_column}' must be present in adata.obs.\")\n    features = adata.X\n    cell_indices = np.random.choice(adata.n_obs, 10000, replace=False)\n    features = features[cell_indices, :]\n    labels = adata[cell_indices, :].obs[cell_type_column]\n    ch_score = calinski_harabasz_score(features, labels)\n    sh_score = silhouette_score(features, labels)\n    return ch_score, sh_score\n</code></pre>"},{"location":"api/validation/#segger.validation.compute_neighborhood_metrics","title":"compute_neighborhood_metrics","text":"<pre><code>compute_neighborhood_metrics(adata, radius=10, celltype_column='celltype_major', n_neighs=20, subset_size=10000)\n</code></pre> <p>Compute neighborhood entropy and number of neighbors for each cell in the AnnData object.</p> <ul> <li>adata: AnnData     Annotated data object containing spatial information and cell type assignments.</li> <li>radius: int, default=10     Radius for spatial neighbor calculation.</li> <li>celltype_column: str, default='celltype_major'     Column name in <code>adata.obs</code> that specifies cell types.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def compute_neighborhood_metrics(\n    adata: ad.AnnData,\n    radius: float = 10,\n    celltype_column: str = \"celltype_major\",\n    n_neighs: int = 20,\n    subset_size: int = 10000,\n) -&gt; None:\n    \"\"\"Compute neighborhood entropy and number of neighbors for each cell in the AnnData object.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing spatial information and cell type assignments.\n    - radius: int, default=10\n        Radius for spatial neighbor calculation.\n    - celltype_column: str, default='celltype_major'\n        Column name in `adata.obs` that specifies cell types.\n    \"\"\"\n    \"\"\"\n    Compute neighborhood entropy and number of neighbors for a random subset of cells in the AnnData object.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing spatial information and cell type assignments.\n    - radius: int, default=10\n        Radius for spatial neighbor calculation.\n    - celltype_column: str, default='celltype_major'\n        Column name in `adata.obs` that specifies cell types.\n    - subset_size: int, default=10000\n        Number of cells to randomly select for the calculation.\n    \"\"\"\n    # Ensure the subset size does not exceed the number of cells\n    subset_size = min(subset_size, adata.n_obs)\n    # Randomly select a subset of cells\n    subset_indices = np.random.choice(adata.n_obs, subset_size, replace=False)\n    # Compute spatial neighbors for the entire dataset\n    sq.gr.spatial_neighbors(adata, radius=radius, coord_type=\"generic\", n_neighs=n_neighs)\n    neighbors = adata.obsp[\"spatial_distances\"].tolil().rows\n    entropies = []\n    num_neighbors = []\n    # Calculate entropy and number of neighbors only for the selected subset\n    for cell_index in subset_indices:\n        neighbor_indices = neighbors[cell_index]\n        neighbor_cell_types = adata.obs[celltype_column].iloc[neighbor_indices]\n        cell_type_counts = neighbor_cell_types.value_counts()\n        total_neighbors = len(neighbor_cell_types)\n        num_neighbors.append(total_neighbors)\n        if total_neighbors &gt; 0:\n            cell_type_probs = cell_type_counts / total_neighbors\n            cell_type_entropy = entropy(cell_type_probs)\n            entropies.append(cell_type_entropy)\n        else:\n            entropies.append(0)\n    # Store the results back into the original AnnData object\n    # We fill with NaN for cells not in the subset\n    entropy_full = np.full(adata.n_obs, np.nan)\n    neighbors_full = np.full(adata.n_obs, np.nan)\n    entropy_full[subset_indices] = entropies\n    neighbors_full[subset_indices] = num_neighbors\n    adata.obs[\"neighborhood_entropy\"] = entropy_full\n    adata.obs[\"number_of_neighbors\"] = neighbors_full\n</code></pre>"},{"location":"api/validation/#segger.validation.compute_quantized_mecr_area","title":"compute_quantized_mecr_area","text":"<pre><code>compute_quantized_mecr_area(adata, gene_pairs, quantiles=10)\n</code></pre> <p>Compute the average MECR, variance of MECR, and average cell area for quantiles of cell areas.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data.</li> <li>gene_pairs: List[Tuple[str, str]]     List of tuples representing gene pairs to evaluate.</li> <li>quantiles: int, default=10     Number of quantiles to divide the data into.</li> </ul> <ul> <li>quantized_data: pd.DataFrame     DataFrame containing quantile information, average MECR, variance of MECR, average area, and number of cells.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def compute_quantized_mecr_area(\n    adata: sc.AnnData, gene_pairs: List[Tuple[str, str]], quantiles: int = 10\n) -&gt; pd.DataFrame:\n    \"\"\"Compute the average MECR, variance of MECR, and average cell area for quantiles of cell areas.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data.\n    - gene_pairs: List[Tuple[str, str]]\n        List of tuples representing gene pairs to evaluate.\n    - quantiles: int, default=10\n        Number of quantiles to divide the data into.\n\n    Returns:\n    - quantized_data: pd.DataFrame\n        DataFrame containing quantile information, average MECR, variance of MECR, average area, and number of cells.\n    \"\"\"\n    adata.obs[\"quantile\"] = pd.qcut(adata.obs[\"cell_area\"], quantiles, labels=False)\n    quantized_data = []\n    for quantile in range(quantiles):\n        cells_in_quantile = adata.obs[\"quantile\"] == quantile\n        mecr = compute_MECR(adata[cells_in_quantile, :], gene_pairs)\n        average_mecr = np.mean([i for i in mecr.values()])\n        variance_mecr = np.var([i for i in mecr.values()])\n        average_area = adata.obs.loc[cells_in_quantile, \"cell_area\"].mean()\n        quantized_data.append(\n            {\n                \"quantile\": quantile / quantiles,\n                \"average_mecr\": average_mecr,\n                \"variance_mecr\": variance_mecr,\n                \"average_area\": average_area,\n                \"num_cells\": cells_in_quantile.sum(),\n            }\n        )\n    return pd.DataFrame(quantized_data)\n</code></pre>"},{"location":"api/validation/#segger.validation.compute_quantized_mecr_counts","title":"compute_quantized_mecr_counts","text":"<pre><code>compute_quantized_mecr_counts(adata, gene_pairs, quantiles=10)\n</code></pre> <p>Compute the average MECR, variance of MECR, and average transcript counts for quantiles of transcript counts.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data.</li> <li>gene_pairs: List[Tuple[str, str]]     List of tuples representing gene pairs to evaluate.</li> <li>quantiles: int, default=10     Number of quantiles to divide the data into.</li> </ul> <ul> <li>quantized_data: pd.DataFrame     DataFrame containing quantile information, average MECR, variance of MECR, average counts, and number of cells.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def compute_quantized_mecr_counts(\n    adata: sc.AnnData, gene_pairs: List[Tuple[str, str]], quantiles: int = 10\n) -&gt; pd.DataFrame:\n    \"\"\"Compute the average MECR, variance of MECR, and average transcript counts for quantiles of transcript counts.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data.\n    - gene_pairs: List[Tuple[str, str]]\n        List of tuples representing gene pairs to evaluate.\n    - quantiles: int, default=10\n        Number of quantiles to divide the data into.\n\n    Returns:\n    - quantized_data: pd.DataFrame\n        DataFrame containing quantile information, average MECR, variance of MECR, average counts, and number of cells.\n    \"\"\"\n    adata.obs[\"quantile\"] = pd.qcut(adata.obs[\"transcripts\"], quantiles, labels=False)\n    quantized_data = []\n    for quantile in range(quantiles):\n        cells_in_quantile = adata.obs[\"quantile\"] == quantile\n        mecr = compute_MECR(adata[cells_in_quantile, :], gene_pairs)\n        average_mecr = np.mean([i for i in mecr.values()])\n        variance_mecr = np.var([i for i in mecr.values()])\n        average_counts = adata.obs.loc[cells_in_quantile, \"transcripts\"].mean()\n        quantized_data.append(\n            {\n                \"quantile\": quantile / quantiles,\n                \"average_mecr\": average_mecr,\n                \"variance_mecr\": variance_mecr,\n                \"average_counts\": average_counts,\n                \"num_cells\": cells_in_quantile.sum(),\n            }\n        )\n    return pd.DataFrame(quantized_data)\n</code></pre>"},{"location":"api/validation/#segger.validation.compute_transcript_density","title":"compute_transcript_density","text":"<pre><code>compute_transcript_density(adata)\n</code></pre> <p>Compute the transcript density for each cell in the AnnData object.</p> <ul> <li>adata: AnnData     Annotated data object containing transcript and cell area information.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def compute_transcript_density(adata: ad.AnnData) -&gt; None:\n    \"\"\"Compute the transcript density for each cell in the AnnData object.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing transcript and cell area information.\n    \"\"\"\n    try:\n        transcript_counts = adata.obs[\"transcript_counts\"]\n    except:\n        transcript_counts = adata.obs[\"transcripts\"]\n    cell_areas = adata.obs[\"cell_area\"]\n    adata.obs[\"transcript_density\"] = transcript_counts / cell_areas\n</code></pre>"},{"location":"api/validation/#segger.validation.draw_umap","title":"draw_umap","text":"<pre><code>draw_umap(adata, column='leiden')\n</code></pre> <p>Draw UMAP plots for the given AnnData object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object containing the data.</p> required <code>column</code> <code>str</code> <p>The column to color the UMAP plot by.</p> <code>'leiden'</code> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def draw_umap(adata, column: str = \"leiden\") -&gt; None:\n    \"\"\"Draw UMAP plots for the given AnnData object.\n\n    Args:\n        adata (AnnData): The AnnData object containing the data.\n        column (str): The column to color the UMAP plot by.\n    \"\"\"\n    sc.pl.umap(adata, color=[column])\n    plt.show()\n\n    sc.pl.umap(adata, color=[\"KRT5\", \"KRT7\"], vmax=\"p95\")\n    plt.show()\n\n    sc.pl.umap(adata, color=[\"ACTA2\", \"PTPRC\"], vmax=\"p95\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.find_markers","title":"find_markers","text":"<pre><code>find_markers(adata, cell_type_column, pos_percentile=5, neg_percentile=10, percentage=50)\n</code></pre> <p>Identify positive and negative markers for each cell type based on gene expression and filter by expression percentage.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data.</li> <li>cell_type_column: str     Column name in <code>adata.obs</code> that specifies cell types.</li> <li>pos_percentile: float, default=5     Percentile threshold to determine top x% expressed genes.</li> <li>neg_percentile: float, default=10     Percentile threshold to determine top x% lowly expressed genes.</li> <li>percentage: float, default=50     Minimum percentage of cells expressing the marker within a cell type for it to be considered.</li> </ul> <ul> <li>markers: dict     Dictionary where keys are cell types and values are dictionaries containing:         'positive': list of top x% highly expressed genes         'negative': list of top x% lowly expressed genes.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def find_markers(\n    adata: ad.AnnData,\n    cell_type_column: str,\n    pos_percentile: float = 5,\n    neg_percentile: float = 10,\n    percentage: float = 50,\n) -&gt; Dict[str, Dict[str, List[str]]]:\n    \"\"\"Identify positive and negative markers for each cell type based on gene expression and filter by expression percentage.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data.\n    - cell_type_column: str\n        Column name in `adata.obs` that specifies cell types.\n    - pos_percentile: float, default=5\n        Percentile threshold to determine top x% expressed genes.\n    - neg_percentile: float, default=10\n        Percentile threshold to determine top x% lowly expressed genes.\n    - percentage: float, default=50\n        Minimum percentage of cells expressing the marker within a cell type for it to be considered.\n\n    Returns:\n    - markers: dict\n        Dictionary where keys are cell types and values are dictionaries containing:\n            'positive': list of top x% highly expressed genes\n            'negative': list of top x% lowly expressed genes.\n    \"\"\"\n    markers = {}\n    sc.tl.rank_genes_groups(adata, groupby=cell_type_column)\n    genes = adata.var_names\n    for cell_type in adata.obs[cell_type_column].unique():\n        subset = adata[adata.obs[cell_type_column] == cell_type]\n        mean_expression = np.asarray(subset.X.mean(axis=0)).flatten()\n        cutoff_high = np.percentile(mean_expression, 100 - pos_percentile)\n        cutoff_low = np.percentile(mean_expression, neg_percentile)\n        pos_indices = np.where(mean_expression &gt;= cutoff_high)[0]\n        neg_indices = np.where(mean_expression &lt;= cutoff_low)[0]\n        expr_frac = np.asarray((subset.X[:, pos_indices] &gt; 0).mean(axis=0)).flatten()\n        valid_pos_indices = pos_indices[expr_frac &gt;= (percentage / 100)]\n        positive_markers = genes[valid_pos_indices]\n        negative_markers = genes[neg_indices]\n        markers[cell_type] = {\"positive\": list(positive_markers), \"negative\": list(negative_markers)}\n    return markers\n</code></pre>"},{"location":"api/validation/#segger.validation.find_mutually_exclusive_genes","title":"find_mutually_exclusive_genes","text":"<pre><code>find_mutually_exclusive_genes(adata, markers, cell_type_column)\n</code></pre> <p>Identify mutually exclusive genes based on expression criteria.</p> <ul> <li>adata: AnnData     Annotated data object containing gene expression data.</li> <li>markers: dict     Dictionary where keys are cell types and values are dictionaries containing:         'positive': list of top x% highly expressed genes         'negative': list of top x% lowly expressed genes.</li> <li>cell_type_column: str     Column name in <code>adata.obs</code> that specifies cell types.</li> </ul> <ul> <li>exclusive_pairs: list     List of mutually exclusive gene pairs.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def find_mutually_exclusive_genes(\n    adata: ad.AnnData, markers: Dict[str, Dict[str, List[str]]], cell_type_column: str\n) -&gt; List[Tuple[str, str]]:\n    \"\"\"Identify mutually exclusive genes based on expression criteria.\n\n    Args:\n    - adata: AnnData\n        Annotated data object containing gene expression data.\n    - markers: dict\n        Dictionary where keys are cell types and values are dictionaries containing:\n            'positive': list of top x% highly expressed genes\n            'negative': list of top x% lowly expressed genes.\n    - cell_type_column: str\n        Column name in `adata.obs` that specifies cell types.\n\n    Returns:\n    - exclusive_pairs: list\n        List of mutually exclusive gene pairs.\n    \"\"\"\n    exclusive_genes = {}\n    all_exclusive = []\n    gene_expression = adata.to_df()\n    for cell_type, marker_sets in markers.items():\n        positive_markers = marker_sets[\"positive\"]\n        exclusive_genes[cell_type] = []\n        for gene in positive_markers:\n            gene_expr = adata[:, gene].X\n            cell_type_mask = adata.obs[cell_type_column] == cell_type\n            non_cell_type_mask = ~cell_type_mask\n            if (gene_expr[cell_type_mask] &gt; 0).mean() &gt; 0.2 and (gene_expr[non_cell_type_mask] &gt; 0).mean() &lt; 0.05:\n                exclusive_genes[cell_type].append(gene)\n                all_exclusive.append(gene)\n    unique_genes = list({gene for i in exclusive_genes.keys() for gene in exclusive_genes[i] if gene in all_exclusive})\n    filtered_exclusive_genes = {\n        i: [gene for gene in exclusive_genes[i] if gene in unique_genes] for i in exclusive_genes.keys()\n    }\n    mutually_exclusive_gene_pairs = [\n        (gene1, gene2)\n        for key1, key2 in combinations(filtered_exclusive_genes.keys(), 2)\n        for gene1 in filtered_exclusive_genes[key1]\n        for gene2 in filtered_exclusive_genes[key2]\n    ]\n    return mutually_exclusive_gene_pairs\n</code></pre>"},{"location":"api/validation/#segger.validation.generate_experiment_file","title":"generate_experiment_file","text":"<pre><code>generate_experiment_file(template_path, output_path, cells_name='seg_cells', analysis_name='seg_analysis')\n</code></pre> <p>Generate the experiment file for Xenium.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>The path to the template file.</p> required <code>output_path</code> <code>str</code> <p>The path to the output file.</p> required <code>cells_name</code> <code>str</code> <p>The name of the cells file.</p> <code>'seg_cells'</code> <code>analysis_name</code> <code>str</code> <p>The name of the analysis file.</p> <code>'seg_analysis'</code> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def generate_experiment_file(\n    template_path: str, output_path: str, cells_name: str = \"seg_cells\", analysis_name: str = \"seg_analysis\"\n) -&gt; None:\n    \"\"\"Generate the experiment file for Xenium.\n\n    Args:\n        template_path (str): The path to the template file.\n        output_path (str): The path to the output file.\n        cells_name (str): The name of the cells file.\n        analysis_name (str): The name of the analysis file.\n    \"\"\"\n    import json\n\n    with open(template_path) as f:\n        experiment = json.load(f)\n\n    experiment[\"images\"].pop(\"morphology_filepath\")\n    experiment[\"images\"].pop(\"morphology_focus_filepath\")\n\n    experiment[\"xenium_explorer_files\"][\"cells_zarr_filepath\"] = f\"{cells_name}.zarr.zip\"\n    experiment[\"xenium_explorer_files\"].pop(\"cell_features_zarr_filepath\")\n    experiment[\"xenium_explorer_files\"][\"analysis_zarr_filepath\"] = f\"{analysis_name}.zarr.zip\"\n\n    with open(output_path, \"w\") as f:\n        json.dump(experiment, f, indent=2)\n</code></pre>"},{"location":"api/validation/#segger.validation.get_flatten_version","title":"get_flatten_version","text":"<pre><code>get_flatten_version(polygons, max_value=21)\n</code></pre> <p>Get the flattened version of polygon vertices.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>List[ndarray]</code> <p>List of polygon vertices.</p> required <code>max_value</code> <code>int</code> <p>The maximum number of vertices to keep.</p> <code>21</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The flattened array of polygon vertices.</p> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def get_flatten_version(polygons: List[np.ndarray], max_value: int = 21) -&gt; np.ndarray:\n    \"\"\"Get the flattened version of polygon vertices.\n\n    Args:\n        polygons (List[np.ndarray]): List of polygon vertices.\n        max_value (int): The maximum number of vertices to keep.\n\n    Returns:\n        np.ndarray: The flattened array of polygon vertices.\n    \"\"\"\n    n = max_value + 1\n    result = np.zeros((len(polygons), n * 2))\n    for i, polygon in tqdm(enumerate(polygons), total=len(polygons)):\n        num_points = len(polygon)\n        if num_points == 0:\n            result[i] = np.zeros(n * 2)\n            continue\n        elif num_points &lt; max_value:\n            repeated_points = np.tile(polygon[0], (n - num_points, 1))\n            padded_polygon = np.concatenate((polygon, repeated_points), axis=0)\n        else:\n            padded_polygon = np.zeros((n, 2))\n            padded_polygon[: min(num_points, n)] = polygon[: min(num_points, n)]\n            padded_polygon[-1] = polygon[0]\n        result[i] = padded_polygon.flatten()\n    return result\n</code></pre>"},{"location":"api/validation/#segger.validation.get_indices_indptr","title":"get_indices_indptr","text":"<pre><code>get_indices_indptr(input_array)\n</code></pre> <p>Get the indices and indptr arrays for sparse matrix representation.</p> <p>Parameters:</p> Name Type Description Default <code>input_array</code> <code>ndarray</code> <p>The input array containing cluster labels.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The indices and indptr arrays.</p> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def get_indices_indptr(input_array: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices and indptr arrays for sparse matrix representation.\n\n    Args:\n        input_array (np.ndarray): The input array containing cluster labels.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The indices and indptr arrays.\n    \"\"\"\n    clusters = sorted(np.unique(input_array[input_array != 0]))\n    indptr = np.zeros(len(clusters), dtype=np.uint32)\n    indices = []\n\n    for cluster in clusters:\n        cluster_indices = np.where(input_array == cluster)[0]\n        indptr[cluster - 1] = len(indices)\n        indices.extend(cluster_indices)\n\n    indices.extend(-np.zeros(len(input_array[input_array == 0])))\n    indices = np.array(indices, dtype=np.int32).astype(np.uint32)\n    return indices, indptr\n</code></pre>"},{"location":"api/validation/#segger.validation.get_leiden_umap","title":"get_leiden_umap","text":"<pre><code>get_leiden_umap(adata, draw=False)\n</code></pre> <p>Perform Leiden clustering and UMAP visualization on the given AnnData object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object containing the data.</p> required <code>draw</code> <code>bool</code> <p>Whether to draw the UMAP plots.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>AnnData</code> <p>The AnnData object with Leiden clustering and UMAP results.</p> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def get_leiden_umap(adata, draw: bool = False):\n    \"\"\"Perform Leiden clustering and UMAP visualization on the given AnnData object.\n\n    Args:\n        adata (AnnData): The AnnData object containing the data.\n        draw (bool): Whether to draw the UMAP plots.\n\n    Returns:\n        AnnData: The AnnData object with Leiden clustering and UMAP results.\n    \"\"\"\n    sc.pp.filter_cells(adata, min_genes=5)\n    sc.pp.filter_genes(adata, min_cells=5)\n\n    gene_names = adata.var_names\n    mean_expression_values = adata.X.mean(axis=0)\n    gene_mean_expression_df = pd.DataFrame({\"gene_name\": gene_names, \"mean_expression\": mean_expression_values})\n    top_genes = gene_mean_expression_df.sort_values(by=\"mean_expression\", ascending=False).head(30)\n    top_gene_names = top_genes[\"gene_name\"].tolist()\n\n    sc.pp.normalize_total(adata)\n    sc.pp.log1p(adata)\n    sc.pp.neighbors(adata, n_neighbors=10, n_pcs=30)\n    sc.tl.umap(adata)\n    sc.tl.leiden(adata)\n\n    if draw:\n        draw_umap(adata, \"leiden\")\n\n    return adata\n</code></pre>"},{"location":"api/validation/#segger.validation.get_median_expression_table","title":"get_median_expression_table","text":"<pre><code>get_median_expression_table(adata, column='leiden')\n</code></pre> <p>Get the median expression table for the given AnnData object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object containing the data.</p> required <code>column</code> <code>str</code> <p>The column to group by.</p> <code>'leiden'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The median expression table.</p> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def get_median_expression_table(adata, column: str = \"leiden\") -&gt; pd.DataFrame:\n    \"\"\"Get the median expression table for the given AnnData object.\n\n    Args:\n        adata (AnnData): The AnnData object containing the data.\n        column (str): The column to group by.\n\n    Returns:\n        pd.DataFrame: The median expression table.\n    \"\"\"\n    top_genes = [\n        \"GATA3\",\n        \"ACTA2\",\n        \"KRT7\",\n        \"KRT8\",\n        \"KRT5\",\n        \"AQP1\",\n        \"SERPINA3\",\n        \"PTGDS\",\n        \"CXCR4\",\n        \"SFRP1\",\n        \"ENAH\",\n        \"MYH11\",\n        \"SVIL\",\n        \"KRT14\",\n        \"CD4\",\n    ]\n    top_gene_indices = [adata.var_names.get_loc(gene) for gene in top_genes]\n\n    clusters = adata.obs[column]\n    cluster_data = {}\n\n    for cluster in clusters.unique():\n        cluster_cells = adata[clusters == cluster].X\n        cluster_expression = cluster_cells[:, top_gene_indices]\n        gene_medians = [\n            pd.Series(cluster_expression[:, gene_idx]).median() for gene_idx in range(len(top_gene_indices))\n        ]\n        cluster_data[f\"Cluster_{cluster}\"] = gene_medians\n\n    cluster_expression_df = pd.DataFrame(cluster_data, index=top_genes)\n    sorted_columns = sorted(cluster_expression_df.columns.values, key=lambda x: int(x.split(\"_\")[-1]))\n    cluster_expression_df = cluster_expression_df[sorted_columns]\n    return cluster_expression_df.T.style.background_gradient(cmap=\"Greens\")\n</code></pre>"},{"location":"api/validation/#segger.validation.load_segmentations","title":"load_segmentations","text":"<pre><code>load_segmentations(segmentation_paths)\n</code></pre> <p>Load segmentation data from provided paths and handle special cases like separating 'segger' into 'segger_n0' and 'segger_n1'.</p> <p>Args: segmentation_paths (Dict[str, Path]): Dictionary mapping segmentation method names to their file paths.</p> <p>Returns: Dict[str, sc.AnnData]: Dictionary mapping segmentation method names to loaded AnnData objects.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def load_segmentations(segmentation_paths: Dict[str, Path]) -&gt; Dict[str, sc.AnnData]:\n    \"\"\"Load segmentation data from provided paths and handle special cases like separating 'segger' into 'segger_n0' and 'segger_n1'.\n\n    Args:\n    segmentation_paths (Dict[str, Path]): Dictionary mapping segmentation method names to their file paths.\n\n    Returns:\n    Dict[str, sc.AnnData]: Dictionary mapping segmentation method names to loaded AnnData objects.\n    \"\"\"\n    segmentations_dict = {}\n    for method, path in segmentation_paths.items():\n        adata = sc.read(path)\n        # Special handling for 'segger' to separate into 'segger_n0' and 'segger_n1'\n        if method == \"segger\":\n            cells_n1 = [i for i in adata.obs_names if not i.endswith(\"-nx\")]\n            cells_n0 = [i for i in adata.obs_names if i.endswith(\"-nx\")]\n            segmentations_dict[\"segger_n1\"] = adata[cells_n1, :]\n            segmentations_dict[\"segger_n0\"] = adata[cells_n0, :]\n        segmentations_dict[method] = adata\n    return segmentations_dict\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_cell_area","title":"plot_cell_area","text":"<pre><code>plot_cell_area(segmentations_dict, output_path, palette)\n</code></pre> <p>Plot the cell area (log2) for each segmentation method.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_cell_area(segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]) -&gt; None:\n    \"\"\"Plot the cell area (log2) for each segmentation method.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the plot will be saved.\n    \"\"\"\n    # Prepare the data for the violin plot\n    violin_data = pd.DataFrame({\"Segmentation Method\": [], \"Cell Area (log2)\": []})\n    for method in segmentations_dict.keys():\n        if \"cell_area\" in segmentations_dict[method].obs.columns:\n            method_area = segmentations_dict[method].obs[\"cell_area\"] + 1\n            method_df = pd.DataFrame(\n                {\"Segmentation Method\": [method] * len(method_area), \"Cell Area (log2)\": method_area.values}\n            )\n            violin_data = pd.concat([violin_data, method_df], axis=0)\n    violin_data.to_csv(output_path / \"cell_area_log2_data.csv\", index=True)\n    # Plot the violin plots\n    plt.figure(figsize=(4, 6))\n    ax = sns.violinplot(x=\"Segmentation Method\", y=\"Cell Area (log2)\", data=violin_data, palette=palette)\n    ax.set(ylim=(5, 100))\n    # Add a dashed line for the 10X-nucleus median\n    if \"10X-nucleus\" in segmentations_dict:\n        median_10X_nucleus_area = np.median(segmentations_dict[\"10X-nucleus\"].obs[\"cell_area\"] + 1)\n        ax.axhline(y=median_10X_nucleus_area, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"10X-nucleus Median\")\n    # Set plot titles and labels\n    plt.title(\"\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"Cell Area (log2)\")\n    plt.xticks(rotation=0)\n    # Save the figure as a PDF\n    plt.savefig(output_path / \"cell_area_log2_violin_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_cell_counts","title":"plot_cell_counts","text":"<pre><code>plot_cell_counts(segmentations_dict, output_path, palette)\n</code></pre> <p>Plot the number of cells per segmentation method and save the cell count data as a CSV.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_cell_counts(segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]) -&gt; None:\n    \"\"\"Plot the number of cells per segmentation method and save the cell count data as a CSV.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the plot will be saved.\n    \"\"\"\n    # Calculate the number of cells in each segmentation method\n    cell_counts = {method: seg.n_obs for method, seg in segmentations_dict.items()}\n\n    # Create a DataFrame for the bar plot\n    df = pd.DataFrame(cell_counts, index=[\"Number of Cells\"]).T\n\n    # Save the DataFrame to CSV\n    df.to_csv(output_path / \"cell_counts_data.csv\", index=True)\n\n    # Generate the bar plot\n    ax = df.plot(\n        kind=\"bar\", stacked=False, color=[palette.get(key, \"#333333\") for key in df.index], figsize=(3, 6), width=0.9\n    )\n\n    # Add a dashed line for the 10X baseline\n    if \"10X\" in cell_counts:\n        baseline_height = cell_counts[\"10X\"]\n        ax.axhline(y=baseline_height, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"10X Baseline\")\n\n    # Set plot titles and labels\n    plt.title(\"Number of Cells per Segmentation Method\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"Number of Cells\")\n    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n    # Save the figure as a PDF\n    plt.savefig(output_path / \"cell_counts_bar_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_contamination_boxplots","title":"plot_contamination_boxplots","text":"<pre><code>plot_contamination_boxplots(boxplot_data, output_path, palette)\n</code></pre> <p>Plot boxplots for contamination values across different segmentation methods.</p> <p>Args: boxplot_data (pd.DataFrame): DataFrame containing contamination data for all segmentation methods. output_path (Path): Path to the directory where the plot will be saved. palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_contamination_boxplots(boxplot_data: pd.DataFrame, output_path: Path, palette: Dict[str, str]) -&gt; None:\n    \"\"\"Plot boxplots for contamination values across different segmentation methods.\n\n    Args:\n    boxplot_data (pd.DataFrame): DataFrame containing contamination data for all segmentation methods.\n    output_path (Path): Path to the directory where the plot will be saved.\n    palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    boxplot_data.to_csv(output_path / \"contamination_box_results.csv\", index=True)\n    plt.figure(figsize=(14, 8))\n    sns.boxplot(x=\"Source Cell Type\", y=\"Contamination\", hue=\"Segmentation Method\", data=boxplot_data, palette=palette)\n    plt.title(\"Neighborhood Contamination\")\n    plt.xlabel(\"Source Cell Type\")\n    plt.ylabel(\"Contamination\")\n    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.xticks(rotation=45, ha=\"right\")\n\n    plt.tight_layout()\n    plt.savefig(output_path / \"contamination_boxplots.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_contamination_results","title":"plot_contamination_results","text":"<pre><code>plot_contamination_results(contamination_results, output_path, palette)\n</code></pre> <p>Plot contamination results for each segmentation method.</p> <p>Parameters:</p> Name Type Description Default <code>contamination_results</code> <code>Dict[str, DataFrame]</code> <p>Dictionary of contamination data for each segmentation method.</p> required <code>output_path</code> <code>Path</code> <p>Path to the directory where the plot will be saved.</p> required <code>palette</code> <code>Dict[str, str]</code> <p>Dictionary mapping segmentation method names to color codes.</p> required Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_contamination_results(\n    contamination_results: Dict[str, pd.DataFrame], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot contamination results for each segmentation method.\n\n    Args:\n        contamination_results (Dict[str, pd.DataFrame]): Dictionary of contamination data for each segmentation method.\n        output_path (Path): Path to the directory where the plot will be saved.\n        palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    contamination_results.to_csv(output_path / \"contamination_results.csv\", index=True)\n    for method, df in contamination_results.items():\n        plt.figure(figsize=(10, 6))\n        sns.heatmap(df, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n        plt.title(f\"Contamination Matrix for {method}\")\n        plt.xlabel(\"Target Cell Type\")\n        plt.ylabel(\"Source Cell Type\")\n        plt.tight_layout()\n        plt.savefig(output_path / f\"{method}_contamination_matrix.pdf\", bbox_inches=\"tight\")\n        plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_counts_per_cell","title":"plot_counts_per_cell","text":"<pre><code>plot_counts_per_cell(segmentations_dict, output_path, palette)\n</code></pre> <p>Plot the counts per cell (log2) for each segmentation method.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_counts_per_cell(segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]) -&gt; None:\n    \"\"\"Plot the counts per cell (log2) for each segmentation method.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the plot will be saved.\n    \"\"\"\n    # Prepare the data for the violin plot\n    violin_data = pd.DataFrame({\"Segmentation Method\": [], \"Counts per Cell (log2)\": []})\n    for method, adata in segmentations_dict.items():\n        method_counts = adata.obs[\"transcripts\"] + 1\n        method_df = pd.DataFrame(\n            {\"Segmentation Method\": [method] * len(method_counts), \"Counts per Cell (log2)\": method_counts.values}\n        )\n        violin_data = pd.concat([violin_data, method_df], axis=0)\n\n    violin_data.to_csv(output_path / \"counts_per_cell_data.csv\", index=True)\n    # Plot the violin plots\n    plt.figure(figsize=(4, 6))\n    ax = sns.violinplot(x=\"Segmentation Method\", y=\"Counts per Cell (log2)\", data=violin_data, palette=palette)\n    ax.set(ylim=(5, 300))\n    # Add a dashed line for the 10X-nucleus median\n    if \"10X-nucleus\" in segmentations_dict:\n        median_10X_nucleus = np.median(segmentations_dict[\"10X-nucleus\"].obs[\"transcripts\"] + 1)\n        ax.axhline(y=median_10X_nucleus, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"10X-nucleus Median\")\n    # Set plot titles and labels\n    plt.title(\"\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"Counts per Cell (log2)\")\n    plt.xticks(rotation=0)\n    # Save the figure as a PDF\n    plt.savefig(output_path / \"counts_per_cell_violin_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_entropy_boxplots","title":"plot_entropy_boxplots","text":"<pre><code>plot_entropy_boxplots(entropy_boxplot_data, output_path, palette)\n</code></pre> <p>Plot boxplots for neighborhood entropy across different segmentation methods by cell type.</p> <p>Args: entropy_boxplot_data (pd.DataFrame): DataFrame containing neighborhood entropy data for all segmentation methods. output_path (Path): Path to the directory where the plot will be saved. palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_entropy_boxplots(entropy_boxplot_data: pd.DataFrame, output_path: Path, palette: Dict[str, str]) -&gt; None:\n    \"\"\"Plot boxplots for neighborhood entropy across different segmentation methods by cell type.\n\n    Args:\n    entropy_boxplot_data (pd.DataFrame): DataFrame containing neighborhood entropy data for all segmentation methods.\n    output_path (Path): Path to the directory where the plot will be saved.\n    palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    plt.figure(figsize=(14, 8))\n    sns.boxplot(\n        x=\"Cell Type\", y=\"Neighborhood Entropy\", hue=\"Segmentation Method\", data=entropy_boxplot_data, palette=palette\n    )\n    plt.title(\"Neighborhood Entropy\")\n    plt.xlabel(\"Cell Type\")\n    plt.ylabel(\"Neighborhood Entropy\")\n    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(output_path / \"neighborhood_entropy_boxplots.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_gene_counts","title":"plot_gene_counts","text":"<pre><code>plot_gene_counts(segmentations_dict, output_path, palette)\n</code></pre> <p>Plot the normalized gene counts for each segmentation method.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_gene_counts(segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]) -&gt; None:\n    \"\"\"Plot the normalized gene counts for each segmentation method.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the plot will be saved.\n    \"\"\"\n    # Calculate total counts per gene for each segmentation method\n    total_counts_per_gene = pd.DataFrame()\n\n    for method, adata in segmentations_dict.items():\n        gene_counts = adata.X.sum(axis=0).flatten()\n        gene_counts = pd.Series(gene_counts, index=adata.var_names, name=method)\n        total_counts_per_gene = pd.concat([total_counts_per_gene, gene_counts], axis=1)\n\n    # Normalize by the maximum count per gene across all segmentations\n    max_counts_per_gene = total_counts_per_gene.max(axis=1)\n    normalized_counts_per_gene = total_counts_per_gene.divide(max_counts_per_gene, axis=0)\n\n    # Prepare the data for the box plot\n    boxplot_data = pd.DataFrame({\"Segmentation Method\": [], \"Normalized Counts\": []})\n\n    for method in segmentations_dict.keys():\n        method_counts = normalized_counts_per_gene[method]\n        method_df = pd.DataFrame(\n            {\"Segmentation Method\": [method] * len(method_counts), \"Normalized Counts\": method_counts.values}\n        )\n        boxplot_data = pd.concat([boxplot_data, method_df], axis=0)\n\n    boxplot_data.to_csv(output_path / \"gene_counts_normalized_data.csv\", index=True)\n\n    # Plot the box plots\n    plt.figure(figsize=(3, 6))\n    ax = sns.boxplot(x=\"Segmentation Method\", y=\"Normalized Counts\", data=boxplot_data, palette=palette, width=0.9)\n\n    # Add a dashed line for the 10X baseline\n    if \"10X\" in normalized_counts_per_gene:\n        baseline_height = normalized_counts_per_gene[\"10X\"].mean()\n        plt.axhline(y=baseline_height, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"10X Baseline\")\n\n    # Set plot titles and labels\n    plt.title(\"\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"Normalized Counts\")\n    plt.xticks(rotation=0)\n\n    # Save the figure as a PDF\n    plt.savefig(output_path / \"gene_counts_normalized_boxplot_by_method.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_general_statistics_plots","title":"plot_general_statistics_plots","text":"<pre><code>plot_general_statistics_plots(segmentations_dict, output_path, palette)\n</code></pre> <p>Create a summary plot with all the general statistics subplots.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the summary plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_general_statistics_plots(\n    segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Create a summary plot with all the general statistics subplots.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the summary plot will be saved.\n    \"\"\"\n    plt.figure(figsize=(15, 20))\n\n    plt.subplot(3, 2, 1)\n    plot_cell_counts(segmentations_dict, output_path, palette=palette)\n\n    plt.subplot(3, 2, 2)\n    plot_percent_assigned(segmentations_dict, output_path, palette=palette)\n\n    plt.subplot(3, 2, 3)\n    plot_gene_counts(segmentations_dict, output_path, palette=palette)\n\n    plt.subplot(3, 2, 4)\n    plot_counts_per_cell(segmentations_dict, output_path, palette=palette)\n    plt.subplot(3, 2, 5)\n    plot_cell_area(segmentations_dict, output_path, palette=palette)\n\n    plt.subplot(3, 2, 6)\n    plot_transcript_density(segmentations_dict, output_path, palette=palette)\n\n    plt.tight_layout()\n    plt.savefig(output_path / \"general_statistics_plots.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_mecr_results","title":"plot_mecr_results","text":"<pre><code>plot_mecr_results(mecr_results, output_path, palette)\n</code></pre> <p>Plot the MECR (Mutually Exclusive Co-expression Rate) results for each segmentation method.</p> <p>Args: mecr_results (Dict[str, Dict[Tuple[str, str], float]]): Dictionary of MECR results for each segmentation method. output_path (Path): Path to the directory where the plot will be saved. palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_mecr_results(\n    mecr_results: Dict[str, Dict[Tuple[str, str], float]], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot the MECR (Mutually Exclusive Co-expression Rate) results for each segmentation method.\n\n    Args:\n    mecr_results (Dict[str, Dict[Tuple[str, str], float]]): Dictionary of MECR results for each segmentation method.\n    output_path (Path): Path to the directory where the plot will be saved.\n    palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    # Prepare the data for plotting\n    plot_data = []\n    for method, mecr_dict in mecr_results.items():\n        for gene_pair, mecr_value in mecr_dict.items():\n            plot_data.append(\n                {\"Segmentation Method\": method, \"Gene Pair\": f\"{gene_pair[0]} - {gene_pair[1]}\", \"MECR\": mecr_value}\n            )\n    df = pd.DataFrame(plot_data)\n    df.to_csv(output_path / \"mcer_box.csv\", index=True)\n    plt.figure(figsize=(3, 6))\n    sns.boxplot(x=\"Segmentation Method\", y=\"MECR\", data=df, palette=palette)\n    plt.title(\"Mutually Exclusive Co-expression Rate (MECR)\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"MECR\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(output_path / \"mecr_results_boxplot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_metric_comparison","title":"plot_metric_comparison","text":"<pre><code>plot_metric_comparison(ax, data, metric, label, method1, method2, output_path)\n</code></pre> <p>Plot a comparison of a specific metric between two methods and save the comparison data.</p> <ul> <li>ax: plt.Axes     Matplotlib axis to plot on.</li> <li>data: pd.DataFrame     DataFrame containing the data for plotting.</li> <li>metric: str     The metric to compare.</li> <li>label: str     Label for the metric.</li> <li>method1: str     The first method to compare.</li> <li>method2: str     The second method to compare.</li> <li>output_path: Path     Path to save the merged DataFrame as a CSV.</li> </ul> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_metric_comparison(\n    ax: plt.Axes, data: pd.DataFrame, metric: str, label: str, method1: str, method2: str, output_path: Path\n) -&gt; None:\n    \"\"\"Plot a comparison of a specific metric between two methods and save the comparison data.\n\n    Args:\n    - ax: plt.Axes\n        Matplotlib axis to plot on.\n    - data: pd.DataFrame\n        DataFrame containing the data for plotting.\n    - metric: str\n        The metric to compare.\n    - label: str\n        Label for the metric.\n    - method1: str\n        The first method to compare.\n    - method2: str\n        The second method to compare.\n    - output_path: Path\n        Path to save the merged DataFrame as a CSV.\n    \"\"\"\n    subset1 = data[data[\"method\"] == method1]\n    subset2 = data[data[\"method\"] == method2]\n    merged_data = pd.merge(subset1, subset2, on=\"celltype_major\", suffixes=(f\"_{method1}\", f\"_{method2}\"))\n\n    # Save the merged data used in the plot to CSV\n    merged_data.to_csv(output_path / f\"metric_comparison_{metric}_{method1}_vs_{method2}.csv\", index=False)\n\n    for cell_type in merged_data[\"celltype_major\"].unique():\n        cell_data = merged_data[merged_data[\"celltype_major\"] == cell_type]\n        ax.scatter(cell_data[f\"{metric}_{method1}\"], cell_data[f\"{metric}_{method2}\"], label=cell_type)\n\n    max_value = max(merged_data[f\"{metric}_{method1}\"].max(), merged_data[f\"{metric}_{method2}\"].max())\n    ax.plot([0, max_value], [0, max_value], \"k--\", alpha=0.5)\n    ax.set_xlabel(f\"{label} ({method1})\")\n    ax.set_ylabel(f\"{label} ({method2})\")\n    ax.set_title(f\"{label}: {method1} vs {method2}\")\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_percent_assigned","title":"plot_percent_assigned","text":"<pre><code>plot_percent_assigned(segmentations_dict, output_path, palette)\n</code></pre> <p>Plot the percentage of assigned transcripts (normalized) for each segmentation method.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_percent_assigned(\n    segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot the percentage of assigned transcripts (normalized) for each segmentation method.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the plot will be saved.\n    \"\"\"\n    # Calculate total counts per gene for each segmentation method\n    total_counts_per_gene = pd.DataFrame()\n\n    for method, adata in segmentations_dict.items():\n        gene_counts = adata.X.sum(axis=0).flatten()  # Sum across cells for each gene and flatten to 1D\n        gene_counts = pd.Series(gene_counts, index=adata.var_names, name=method)\n        total_counts_per_gene = pd.concat([total_counts_per_gene, gene_counts], axis=1)\n\n    # Normalize by the maximum count per gene across all segmentations\n    max_counts_per_gene = total_counts_per_gene.max(axis=1)\n    percent_assigned_normalized = total_counts_per_gene.divide(max_counts_per_gene, axis=0) * 100\n\n    # Prepare the data for the violin plot\n    violin_data = pd.DataFrame({\"Segmentation Method\": [], \"Percent Assigned (Normalized)\": []})\n\n    # Add normalized percent_assigned data for each method\n    for method in segmentations_dict.keys():\n        method_data = percent_assigned_normalized[method].dropna()\n        method_df = pd.DataFrame(\n            {\"Segmentation Method\": [method] * len(method_data), \"Percent Assigned (Normalized)\": method_data.values}\n        )\n        violin_data = pd.concat([violin_data, method_df], axis=0)\n\n    violin_data.to_csv(output_path / \"percent_assigned_normalized.csv\", index=True)\n\n    # Plot the violin plots\n    plt.figure(figsize=(12, 8))\n    ax = sns.violinplot(x=\"Segmentation Method\", y=\"Percent Assigned (Normalized)\", data=violin_data, palette=palette)\n\n    # Add a dashed line for the 10X baseline\n    if \"10X\" in segmentations_dict:\n        baseline_height = percent_assigned_normalized[\"10X\"].mean()\n        ax.axhline(y=baseline_height, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"10X Baseline\")\n\n    # Set plot titles and labels\n    plt.title(\"\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"Percent Assigned (Normalized)\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n    # Save the figure as a PDF\n    plt.savefig(output_path / \"percent_assigned_normalized_violin_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_quantized_mecr_area","title":"plot_quantized_mecr_area","text":"<pre><code>plot_quantized_mecr_area(quantized_mecr_area, output_path, palette)\n</code></pre> <p>Plot the quantized MECR values against cell areas for each segmentation method, with point size proportional to the variance of MECR.</p> <p>Args: quantized_mecr_area (Dict[str, pd.DataFrame]): Dictionary of quantized MECR area data for each segmentation method. output_path (Path): Path to the directory where the plot will be saved. palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_quantized_mecr_area(\n    quantized_mecr_area: Dict[str, pd.DataFrame], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot the quantized MECR values against cell areas for each segmentation method, with point size proportional to the variance of MECR.\n\n    Args:\n    quantized_mecr_area (Dict[str, pd.DataFrame]): Dictionary of quantized MECR area data for each segmentation method.\n    output_path (Path): Path to the directory where the plot will be saved.\n    palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    # quantized_mecr_area.to_csv(output_path / 'quantized_mecr_area.csv', index=True)\n    plt.figure(figsize=(6, 4))\n    for method, df in quantized_mecr_area.items():\n        plt.plot(\n            df[\"average_area\"],\n            df[\"average_mecr\"],\n            marker=\"o\",\n            # s=df['variance_mecr']  * 1e5,\n            linestyle=\"-\",\n            color=palette.get(method, \"#333333\"),\n            label=method,\n            markersize=0,\n        )\n        plt.scatter(\n            df[\"average_area\"],\n            df[\"average_mecr\"],\n            s=df[\"variance_mecr\"] * 1e5,  # Size of points based on the variance of MECR\n            color=palette.get(method, \"#333333\"),\n            alpha=0.7,  # Slight transparency for overlapping points\n            edgecolor=\"w\",  # White edge color for better visibility\n            linewidth=0.5,  # Thin edge line\n        )\n    plt.title(\"Quantized MECR by Cell Area\")\n    plt.xlabel(\"Average Cell Area\")\n    plt.ylabel(\"Average MECR\")\n    # Place the legend outside the plot on the top right\n    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.tight_layout()\n    plt.savefig(output_path / \"quantized_mecr_area_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_quantized_mecr_counts","title":"plot_quantized_mecr_counts","text":"<pre><code>plot_quantized_mecr_counts(quantized_mecr_counts, output_path, palette)\n</code></pre> <p>Plot the quantized MECR values against transcript counts for each segmentation method, with point size proportional to the variance of MECR.</p> <p>Parameters:</p> Name Type Description Default <code>quantized_mecr_counts</code> <code>Dict[str, DataFrame]</code> <p>Dictionary of quantized MECR count data for each segmentation method.</p> required <code>output_path</code> <code>Path</code> <p>Path to the directory where the plot will be saved.</p> required <code>palette</code> <code>Dict[str, str]</code> <p>Dictionary mapping segmentation method names to color codes.</p> required Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_quantized_mecr_counts(\n    quantized_mecr_counts: Dict[str, pd.DataFrame], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot the quantized MECR values against transcript counts for each segmentation method, with point size proportional to the variance of MECR.\n\n    Args:\n        quantized_mecr_counts (Dict[str, pd.DataFrame]): Dictionary of quantized MECR count data for each segmentation method.\n        output_path (Path): Path to the directory where the plot will be saved.\n        palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    # quantized_mecr_counts.to_csv(output_path / 'quantized_mecr_counts.csv', index=True)\n    plt.figure(figsize=(9, 6))\n    for method, df in quantized_mecr_counts.items():\n        plt.plot(\n            df[\"average_counts\"],\n            df[\"average_mecr\"],\n            marker=\"o\",\n            linestyle=\"-\",\n            color=palette.get(method, \"#333333\"),\n            label=method,\n            markersize=0,  # No markers, only lines\n        )\n        plt.scatter(\n            df[\"average_counts\"],\n            df[\"average_mecr\"],\n            s=df[\"variance_mecr\"] * 1e5,  # Size of points based on the variance of MECR\n            color=palette.get(method, \"#333333\"),\n            alpha=0.7,  # Slight transparency for overlapping points\n            edgecolor=\"w\",  # White edge color for better visibility\n            linewidth=0.5,  # Thin edge line\n        )\n    plt.title(\"Quantized MECR by Transcript Counts\")\n    plt.xlabel(\"Average Transcript Counts\")\n    plt.ylabel(\"Average MECR\")\n    # Place the legend outside the plot on the top right\n    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.tight_layout()\n    plt.savefig(output_path / \"quantized_mecr_counts_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_sensitivity_boxplots","title":"plot_sensitivity_boxplots","text":"<pre><code>plot_sensitivity_boxplots(sensitivity_boxplot_data, output_path, palette)\n</code></pre> <p>Plot boxplots for sensitivity across different segmentation methods by cell type. Args:     sensitivity_boxplot_data (pd.DataFrame): DataFrame containing sensitivity data for all segmentation methods.     output_path (Path): Path to the directory where the plot will be saved.     palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_sensitivity_boxplots(\n    sensitivity_boxplot_data: pd.DataFrame, output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot boxplots for sensitivity across different segmentation methods by cell type.\n    Args:\n        sensitivity_boxplot_data (pd.DataFrame): DataFrame containing sensitivity data for all segmentation methods.\n        output_path (Path): Path to the directory where the plot will be saved.\n        palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    sensitivity_boxplot_data.to_csv(output_path / \"sensitivity_results.csv\", index=True)\n    plt.figure(figsize=(14, 8))\n    sns.boxplot(\n        x=\"Cell Type\", y=\"Sensitivity\", hue=\"Segmentation Method\", data=sensitivity_boxplot_data, palette=palette\n    )\n    plt.title(\"Sensitivity Score\")\n    plt.xlabel(\"Cell Type\")\n    plt.ylabel(\"Sensitivity\")\n    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(output_path / \"sensitivity_boxplots.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_transcript_density","title":"plot_transcript_density","text":"<pre><code>plot_transcript_density(segmentations_dict, output_path, palette)\n</code></pre> <p>Plot the transcript density (log2) for each segmentation method.</p> <p>Args: segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects. output_path (Path): Path to the directory where the plot will be saved.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_transcript_density(\n    segmentations_dict: Dict[str, sc.AnnData], output_path: Path, palette: Dict[str, str]\n) -&gt; None:\n    \"\"\"Plot the transcript density (log2) for each segmentation method.\n\n    Args:\n    segmentations_dict (Dict[str, sc.AnnData]): Dictionary mapping segmentation method names to loaded AnnData objects.\n    output_path (Path): Path to the directory where the plot will be saved.\n    \"\"\"\n    # Prepare the data for the violin plot\n    violin_data = pd.DataFrame({\"Segmentation Method\": [], \"Transcript Density (log2)\": []})\n\n    for method in segmentations_dict.keys():\n        if \"cell_area\" in segmentations_dict[method].obs.columns:\n            method_density = segmentations_dict[method].obs[\"transcripts\"] / segmentations_dict[method].obs[\"cell_area\"]\n            method_density_log2 = np.log2(method_density + 1)\n            method_df = pd.DataFrame(\n                {\n                    \"Segmentation Method\": [method] * len(method_density_log2),\n                    \"Transcript Density (log2)\": method_density_log2.values,\n                }\n            )\n            violin_data = pd.concat([violin_data, method_df], axis=0)\n\n    violin_data.to_csv(output_path / \"transcript_density_log2_data.csv\", index=True)\n\n    # Plot the violin plots\n    plt.figure(figsize=(4, 6))\n    ax = sns.violinplot(x=\"Segmentation Method\", y=\"Transcript Density (log2)\", data=violin_data, palette=palette)\n\n    # Add a dashed line for the 10X-nucleus median\n    if \"10X-nucleus\" in segmentations_dict:\n        median_10X_nucleus_density_log2 = np.median(\n            np.log2(\n                segmentations_dict[\"10X-nucleus\"].obs[\"transcripts\"]\n                / segmentations_dict[\"10X-nucleus\"].obs[\"cell_area\"]\n                + 1\n            )\n        )\n        ax.axhline(\n            y=median_10X_nucleus_density_log2, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"10X-nucleus Median\"\n        )\n\n    # Set plot titles and labels\n    plt.title(\"\")\n    plt.xlabel(\"Segmentation Method\")\n    plt.ylabel(\"Transcript Density (log2)\")\n    plt.xticks(rotation=0)\n\n    # Save the figure as a PDF\n    plt.savefig(output_path / \"transcript_density_log2_violin_plot.pdf\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.plot_umaps_with_scores","title":"plot_umaps_with_scores","text":"<pre><code>plot_umaps_with_scores(segmentations_dict, clustering_scores, output_path, palette)\n</code></pre> <p>Plot UMAPs colored by cell type for each segmentation method and display clustering scores in the title. Args: segmentations_dict (Dict[str, AnnData]): Dictionary of AnnData objects for each segmentation method. clustering_scores (Dict[str, Tuple[float, float]]): Dictionary of clustering scores for each method. output_path (Path): Path to the directory where the plots will be saved. palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.</p> Source code in <code>src/segger/validation/utils.py</code> <pre><code>def plot_umaps_with_scores(\n    segmentations_dict: Dict[str, sc.AnnData],\n    clustering_scores: Dict[str, Tuple[float, float]],\n    output_path: Path,\n    palette: Dict[str, str],\n) -&gt; None:\n    \"\"\"Plot UMAPs colored by cell type for each segmentation method and display clustering scores in the title.\n    Args:\n    segmentations_dict (Dict[str, AnnData]): Dictionary of AnnData objects for each segmentation method.\n    clustering_scores (Dict[str, Tuple[float, float]]): Dictionary of clustering scores for each method.\n    output_path (Path): Path to the directory where the plots will be saved.\n    palette (Dict[str, str]): Dictionary mapping segmentation method names to color codes.\n    \"\"\"\n    for method, adata in segmentations_dict.items():\n        print(method)\n        adata_copy = adata.copy()\n        sc.pp.subsample(adata_copy, n_obs=10000)\n        sc.pp.normalize_total(adata_copy)\n        # Plot UMAP colored by cell type\n        plt.figure(figsize=(8, 6))\n        sc.pp.neighbors(adata_copy, n_neighbors=5)\n        sc.tl.umap(adata_copy, spread=5)\n        sc.pl.umap(adata_copy, color=\"celltype_major\", palette=palette, show=False)\n        # Add clustering scores to the title\n        ch_score, sh_score = compute_clustering_scores(adata_copy, cell_type_column=\"celltype_major\")\n        plt.title(f\"{method} - UMAP\\nCalinski-Harabasz: {ch_score:.2f}, Silhouette: {sh_score:.2f}\")\n        # Save the figure\n        plt.savefig(output_path / f\"{method}_umap_with_scores.pdf\", bbox_inches=\"tight\")\n        plt.show()\n</code></pre>"},{"location":"api/validation/#segger.validation.save_cell_clustering","title":"save_cell_clustering","text":"<pre><code>save_cell_clustering(merged, zarr_path, columns)\n</code></pre> <p>Save cell clustering information to a Zarr file.</p> <p>Parameters:</p> Name Type Description Default <code>merged</code> <code>DataFrame</code> <p>The merged dataframe containing cell clustering information.</p> required <code>zarr_path</code> <code>str</code> <p>The path to the Zarr file.</p> required <code>columns</code> <code>List[str]</code> <p>The list of columns to save.</p> required Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def save_cell_clustering(merged: pd.DataFrame, zarr_path: str, columns: List[str]) -&gt; None:\n    \"\"\"Save cell clustering information to a Zarr file.\n\n    Args:\n        merged (pd.DataFrame): The merged dataframe containing cell clustering information.\n        zarr_path (str): The path to the Zarr file.\n        columns (List[str]): The list of columns to save.\n    \"\"\"\n    import zarr\n\n    new_zarr = zarr.open(zarr_path, mode=\"w\")\n    new_zarr.create_group(\"/cell_groups\")\n\n    mappings = []\n    for index, column in enumerate(columns):\n        new_zarr[\"cell_groups\"].create_group(index)\n        classes = list(np.unique(merged[column].astype(str)))\n        mapping_dict = {key: i for i, key in zip(range(1, len(classes)), [k for k in classes if k != \"nan\"])}\n        mapping_dict[\"nan\"] = 0\n\n        clusters = merged[column].astype(str).replace(mapping_dict).values.astype(int)\n        indices, indptr = get_indices_indptr(clusters)\n\n        new_zarr[\"cell_groups\"][index].create_dataset(\"indices\", data=indices)\n        new_zarr[\"cell_groups\"][index].create_dataset(\"indptr\", data=indptr)\n        mappings.append(mapping_dict)\n\n    new_zarr[\"cell_groups\"].attrs.update(\n        {\n            \"major_version\": 1,\n            \"minor_version\": 0,\n            \"number_groupings\": len(columns),\n            \"grouping_names\": columns,\n            \"group_names\": [\n                [k for k, v in sorted(mapping_dict.items(), key=lambda item: item[1])][1:] for mapping_dict in mappings\n            ],\n        }\n    )\n    new_zarr.store.close()\n</code></pre>"},{"location":"api/validation/#segger.validation.seg2explorer","title":"seg2explorer","text":"<pre><code>seg2explorer(seg_df, source_path, output_dir, cells_filename='seg_cells', analysis_filename='seg_analysis', xenium_filename='seg_experiment.xenium', analysis_df=None, draw=False, cell_id_columns='seg_cell_id', area_low=10, area_high=100)\n</code></pre> <p>Convert seg output to a format compatible with Xenium explorer.</p> <p>Parameters:</p> Name Type Description Default <code>seg_df</code> <code>DataFrame</code> <p>The seg DataFrame.</p> required <code>source_path</code> <code>str</code> <p>The source path.</p> required <code>output_dir</code> <code>str</code> <p>The output directory.</p> required <code>cells_filename</code> <code>str</code> <p>The filename for cells.</p> <code>'seg_cells'</code> <code>analysis_filename</code> <code>str</code> <p>The filename for analysis.</p> <code>'seg_analysis'</code> <code>xenium_filename</code> <code>str</code> <p>The filename for Xenium.</p> <code>'seg_experiment.xenium'</code> <code>analysis_df</code> <code>Optional[DataFrame]</code> <p>The analysis DataFrame.</p> <code>None</code> <code>draw</code> <code>bool</code> <p>Whether to draw the plots.</p> <code>False</code> <code>cell_id_columns</code> <code>str</code> <p>The cell ID columns.</p> <code>'seg_cell_id'</code> <code>area_low</code> <code>float</code> <p>The lower area threshold.</p> <code>10</code> <code>area_high</code> <code>float</code> <p>The upper area threshold.</p> <code>100</code> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def seg2explorer(\n    seg_df: pd.DataFrame,\n    source_path: str,\n    output_dir: str,\n    cells_filename: str = \"seg_cells\",\n    analysis_filename: str = \"seg_analysis\",\n    xenium_filename: str = \"seg_experiment.xenium\",\n    analysis_df: Optional[pd.DataFrame] = None,\n    draw: bool = False,\n    cell_id_columns: str = \"seg_cell_id\",\n    area_low: float = 10,\n    area_high: float = 100,\n) -&gt; None:\n    \"\"\"Convert seg output to a format compatible with Xenium explorer.\n\n    Args:\n        seg_df (pd.DataFrame): The seg DataFrame.\n        source_path (str): The source path.\n        output_dir (str): The output directory.\n        cells_filename (str): The filename for cells.\n        analysis_filename (str): The filename for analysis.\n        xenium_filename (str): The filename for Xenium.\n        analysis_df (Optional[pd.DataFrame]): The analysis DataFrame.\n        draw (bool): Whether to draw the plots.\n        cell_id_columns (str): The cell ID columns.\n        area_low (float): The lower area threshold.\n        area_high (float): The upper area threshold.\n    \"\"\"\n    import zarr\n    import json\n\n    source_path = Path(source_path)\n    storage = Path(output_dir)\n\n    cell_id2old_id = {}\n    cell_id = []\n    cell_summary = []\n    polygon_num_vertices = [[], []]\n    polygon_vertices = [[], []]\n    seg_mask_value = []\n    tma_id = []\n\n    grouped_by = seg_df.groupby(cell_id_columns)\n    for cell_incremental_id, (seg_cell_id, seg_cell) in tqdm(enumerate(grouped_by), total=len(grouped_by)):\n        if len(seg_cell) &lt; 5:\n            continue\n\n        cell_convex_hull = ConvexHull(seg_cell[[\"x_location\", \"y_location\"]])\n        if cell_convex_hull.area &gt; area_high:\n            continue\n        if cell_convex_hull.area &lt; area_low:\n            continue\n\n        uint_cell_id = cell_incremental_id + 1\n        cell_id2old_id[uint_cell_id] = seg_cell_id\n\n        seg_nucleous = seg_cell[seg_cell[\"overlaps_nucleus\"] == 1]\n        if len(seg_nucleous) &gt;= 3:\n            nucleus_convex_hull = ConvexHull(seg_nucleous[[\"x_location\", \"y_location\"]])\n\n        cell_id.append(uint_cell_id)\n        cell_summary.append(\n            {\n                \"cell_centroid_x\": seg_cell[\"x_location\"].mean(),\n                \"cell_centroid_y\": seg_cell[\"y_location\"].mean(),\n                \"cell_area\": cell_convex_hull.area,\n                \"nucleus_centroid_x\": seg_cell[\"x_location\"].mean(),\n                \"nucleus_centroid_y\": seg_cell[\"y_location\"].mean(),\n                \"nucleus_area\": cell_convex_hull.area,\n                \"z_level\": (seg_cell.z_location.mean() // 3).round(0) * 3,\n            }\n        )\n\n        polygon_num_vertices[0].append(len(cell_convex_hull.vertices))\n        polygon_num_vertices[1].append(len(nucleus_convex_hull.vertices) if len(seg_nucleous) &gt;= 3 else 0)\n        polygon_vertices[0].append(seg_cell[[\"x_location\", \"y_location\"]].values[cell_convex_hull.vertices])\n        polygon_vertices[1].append(\n            seg_nucleous[[\"x_location\", \"y_location\"]].values[nucleus_convex_hull.vertices]\n            if len(seg_nucleous) &gt;= 3\n            else np.array([[], []]).T\n        )\n        seg_mask_value.append(cell_incremental_id + 1)\n\n    cell_polygon_vertices = get_flatten_version(polygon_vertices[0], max_value=21)\n    nucl_polygon_vertices = get_flatten_version(polygon_vertices[1], max_value=21)\n\n    cells = {\n        \"cell_id\": np.array([np.array(cell_id), np.ones(len(cell_id))], dtype=np.uint32).T,\n        \"cell_summary\": pd.DataFrame(cell_summary).values.astype(np.float64),\n        \"polygon_num_vertices\": np.array(\n            [\n                [min(x + 1, x + 1) for x in polygon_num_vertices[1]],\n                [min(x + 1, x + 1) for x in polygon_num_vertices[0]],\n            ],\n            dtype=np.int32,\n        ),\n        \"polygon_vertices\": np.array([nucl_polygon_vertices, cell_polygon_vertices]).astype(np.float32),\n        \"seg_mask_value\": np.array(seg_mask_value, dtype=np.int32),\n    }\n\n    existing_store = zarr.open(source_path / \"cells.zarr.zip\", mode=\"r\")\n    new_store = zarr.open(storage / f\"{cells_filename}.zarr.zip\", mode=\"w\")\n\n    new_store[\"cell_id\"] = cells[\"cell_id\"]\n    new_store[\"polygon_num_vertices\"] = cells[\"polygon_num_vertices\"]\n    new_store[\"polygon_vertices\"] = cells[\"polygon_vertices\"]\n    new_store[\"seg_mask_value\"] = cells[\"seg_mask_value\"]\n\n    new_store.attrs.update(existing_store.attrs)\n    new_store.attrs[\"number_cells\"] = len(cells[\"cell_id\"])\n    new_store.store.close()\n\n    if analysis_df is None:\n        analysis_df = pd.DataFrame([cell_id2old_id[i] for i in cell_id], columns=[cell_id_columns])\n        analysis_df[\"default\"] = \"seg\"\n\n    zarr_df = pd.DataFrame([cell_id2old_id[i] for i in cell_id], columns=[cell_id_columns])\n    clustering_df = pd.merge(zarr_df, analysis_df, how=\"left\", on=cell_id_columns)\n\n    clusters_names = [i for i in analysis_df.columns if i != cell_id_columns]\n    clusters_dict = {\n        cluster: {\n            j: i\n            for i, j in zip(\n                range(1, len(sorted(np.unique(clustering_df[cluster].dropna()))) + 1),\n                sorted(np.unique(clustering_df[cluster].dropna())),\n            )\n        }\n        for cluster in clusters_names\n    }\n\n    new_zarr = zarr.open(storage / (analysis_filename + \".zarr.zip\"), mode=\"w\")\n    new_zarr.create_group(\"/cell_groups\")\n\n    clusters = [[clusters_dict[cluster].get(x, 0) for x in list(clustering_df[cluster])] for cluster in clusters_names]\n\n    for i in range(len(clusters)):\n        new_zarr[\"cell_groups\"].create_group(i)\n        indices, indptr = get_indices_indptr(np.array(clusters[i]))\n        new_zarr[\"cell_groups\"][i].create_dataset(\"indices\", data=indices)\n        new_zarr[\"cell_groups\"][i].create_dataset(\"indptr\", data=indptr)\n\n    new_zarr[\"cell_groups\"].attrs.update(\n        {\n            \"major_version\": 1,\n            \"minor_version\": 0,\n            \"number_groupings\": len(clusters_names),\n            \"grouping_names\": clusters_names,\n            \"group_names\": [\n                [x[0] for x in sorted(clusters_dict[cluster].items(), key=lambda x: x[1])] for cluster in clusters_names\n            ],\n        }\n    )\n\n    new_zarr.store.close()\n    generate_experiment_file(\n        template_path=source_path / \"experiment.xenium\",\n        output_path=storage / xenium_filename,\n        cells_name=cells_filename,\n        analysis_name=analysis_filename,\n    )\n</code></pre>"},{"location":"api/validation/#segger.validation.str_to_uint32","title":"str_to_uint32","text":"<pre><code>str_to_uint32(cell_id_str)\n</code></pre> <p>Convert a string cell ID back to uint32 format.</p> <p>Parameters:</p> Name Type Description Default <code>cell_id_str</code> <code>str</code> <p>The cell ID in string format.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple[int, int]: The cell ID in uint32 format and the dataset suffix.</p> Source code in <code>src/segger/validation/xenium_explorer.py</code> <pre><code>def str_to_uint32(cell_id_str: str) -&gt; Tuple[int, int]:\n    \"\"\"Convert a string cell ID back to uint32 format.\n\n    Args:\n        cell_id_str (str): The cell ID in string format.\n\n    Returns:\n        Tuple[int, int]: The cell ID in uint32 format and the dataset suffix.\n    \"\"\"\n    prefix, suffix = cell_id_str.split(\"-\")\n    str_to_hex_mapping = {\n        \"a\": \"0\",\n        \"b\": \"1\",\n        \"c\": \"2\",\n        \"d\": \"3\",\n        \"e\": \"4\",\n        \"f\": \"5\",\n        \"g\": \"6\",\n        \"h\": \"7\",\n        \"i\": \"8\",\n        \"j\": \"9\",\n        \"k\": \"a\",\n        \"l\": \"b\",\n        \"m\": \"c\",\n        \"n\": \"d\",\n        \"o\": \"e\",\n        \"p\": \"f\",\n    }\n    hex_prefix = \"\".join([str_to_hex_mapping[char] for char in prefix])\n    cell_id_uint32 = int(hex_prefix, 16)\n    dataset_suffix = int(suffix)\n    return cell_id_uint32, dataset_suffix\n</code></pre>"},{"location":"notebooks/benchmark_bc/","title":"Benchmark bc","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport scanpy as sc\nimport numpy as np\nfrom typing import Dict\nfrom segger.validation.utils import *\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from pathlib import Path import scanpy as sc import numpy as np from typing import Dict from segger.validation.utils import * In\u00a0[\u00a0]: Copied! <pre># Define paths and output directories\nbenchmarks_path = Path(\"/dkfz/cluster/gpu/data/OE0606/elihei/segger_experiments/data_tidy/benchmarks/xe_rep1_bc\")\noutput_path = benchmarks_path / \"results+\"\nfigures_path = output_path / \"figures\"\nfigures_path.mkdir(parents=True, exist_ok=True)  # Ensure the figures directory exists\n</pre> # Define paths and output directories benchmarks_path = Path(\"/dkfz/cluster/gpu/data/OE0606/elihei/segger_experiments/data_tidy/benchmarks/xe_rep1_bc\") output_path = benchmarks_path / \"results+\" figures_path = output_path / \"figures\" figures_path.mkdir(parents=True, exist_ok=True)  # Ensure the figures directory exists In\u00a0[\u00a0]: Copied! <pre># Define colors for segmentation methods\nmethod_colors = {\n    \"segger\": \"#D55E00\",\n    \"segger_n0\": \"#E69F00\",\n    \"segger_n1\": \"#F0E442\",\n    \"segger_embedding\": \"#C72228\",\n    \"Baysor\": \"#000075\",\n    \"Baysor_n0\": \"#0F4A9C\",\n    \"Baysor_n1\": \"#0072B2\",\n    \"10X\": \"#8B008B\",\n    \"10X-nucleus\": \"#CC79A7\",\n    # 'BIDCell': '#009E73'\n}\n</pre> # Define colors for segmentation methods method_colors = {     \"segger\": \"#D55E00\",     \"segger_n0\": \"#E69F00\",     \"segger_n1\": \"#F0E442\",     \"segger_embedding\": \"#C72228\",     \"Baysor\": \"#000075\",     \"Baysor_n0\": \"#0F4A9C\",     \"Baysor_n1\": \"#0072B2\",     \"10X\": \"#8B008B\",     \"10X-nucleus\": \"#CC79A7\",     # 'BIDCell': '#009E73' } In\u00a0[\u00a0]: Copied! <pre># Define colors for cell types\nmajor_colors = {\n    \"B-cells\": \"#d8f55e\",\n    \"CAFs\": \"#532C8A\",\n    \"Cancer Epithelial\": \"#C72228\",\n    \"Endothelial\": \"#9e6762\",\n    \"Myeloid\": \"#ffe012\",\n    \"T-cells\": \"#3cb44b\",\n    \"Normal Epithelial\": \"#0F4A9C\",\n    \"PVL\": \"#c09d9a\",\n    \"Plasmablasts\": \"#000075\",\n}\n</pre> # Define colors for cell types major_colors = {     \"B-cells\": \"#d8f55e\",     \"CAFs\": \"#532C8A\",     \"Cancer Epithelial\": \"#C72228\",     \"Endothelial\": \"#9e6762\",     \"Myeloid\": \"#ffe012\",     \"T-cells\": \"#3cb44b\",     \"Normal Epithelial\": \"#0F4A9C\",     \"PVL\": \"#c09d9a\",     \"Plasmablasts\": \"#000075\", } In\u00a0[\u00a0]: Copied! <pre># Define segmentation file paths\nsegmentation_paths = {\n    \"segger\": benchmarks_path / \"adata_segger.h5ad\",\n    \"Baysor\": benchmarks_path / \"adata_baysor.h5ad\",\n    \"10X\": benchmarks_path / \"adata_10X.h5ad\",\n    \"10X-nucleus\": benchmarks_path / \"adata_10X_nuc.h5ad\",\n    \"BIDCell\": benchmarks_path / \"adata_BIDCell.h5ad\",\n}\n</pre> # Define segmentation file paths segmentation_paths = {     \"segger\": benchmarks_path / \"adata_segger.h5ad\",     \"Baysor\": benchmarks_path / \"adata_baysor.h5ad\",     \"10X\": benchmarks_path / \"adata_10X.h5ad\",     \"10X-nucleus\": benchmarks_path / \"adata_10X_nuc.h5ad\",     \"BIDCell\": benchmarks_path / \"adata_BIDCell.h5ad\", } In\u00a0[\u00a0]: Copied! <pre># Load the segmentations and the scRNAseq data\nsegmentations_dict = load_segmentations(segmentation_paths)\nsegmentations_dict = {k: segmentations_dict[k] for k in method_colors.keys() if k in segmentations_dict}\nscRNAseq_adata = sc.read(benchmarks_path / \"scRNAseq.h5ad\")\n</pre> # Load the segmentations and the scRNAseq data segmentations_dict = load_segmentations(segmentation_paths) segmentations_dict = {k: segmentations_dict[k] for k in method_colors.keys() if k in segmentations_dict} scRNAseq_adata = sc.read(benchmarks_path / \"scRNAseq.h5ad\") <p>Generate general statistics plots plot_general_statistics_plots(segmentations_dict, figures_path, method_colors)</p> In\u00a0[\u00a0]: Copied! <pre>plot_cell_counts(segmentations_dict, figures_path, palette=method_colors)\nplot_cell_area(segmentations_dict, figures_path, palette=method_colors)\n</pre> plot_cell_counts(segmentations_dict, figures_path, palette=method_colors) plot_cell_area(segmentations_dict, figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Find markers for scRNAseq data\nmarkers = find_markers(scRNAseq_adata, cell_type_column=\"celltype_major\", pos_percentile=30, neg_percentile=5)\n</pre> # Find markers for scRNAseq data markers = find_markers(scRNAseq_adata, cell_type_column=\"celltype_major\", pos_percentile=30, neg_percentile=5) <p>Annotate spatial segmentations with scRNAseq reference data for method in segmentation_paths.keys(): segmentations_dict[method] = annotate_query_with_reference( reference_adata=scRNAseq_adata, query_adata=segmentations_dict[method], transfer_column='celltype_major' ) segmentations_dict[method].write(segmentation_paths[method])</p> In\u00a0[\u00a0]: Copied! <pre>sc._settings.ScanpyConfig.figdir = figures_path\nsegmentations_dict[\"segger_embedding\"].obsm[\"spatial\"] = (\n    segmentations_dict[\"segger_embedding\"].obs[[\"cell_centroid_x\", \"cell_centroid_y\"]].values\n)\nsc.pl.spatial(\n    segmentations_dict[\"segger_embedding\"],\n    spot_size=10,\n    save=\"embedding.pdf\",\n    color=\"celltype_major\",\n    palette=major_colors,\n)\n</pre> sc._settings.ScanpyConfig.figdir = figures_path segmentations_dict[\"segger_embedding\"].obsm[\"spatial\"] = (     segmentations_dict[\"segger_embedding\"].obs[[\"cell_centroid_x\", \"cell_centroid_y\"]].values ) sc.pl.spatial(     segmentations_dict[\"segger_embedding\"],     spot_size=10,     save=\"embedding.pdf\",     color=\"celltype_major\",     palette=major_colors, ) In\u00a0[\u00a0]: Copied! <pre># Find mutually exclusive genes based on scRNAseq data\nexclusive_gene_pairs = find_mutually_exclusive_genes(\n    adata=scRNAseq_adata, markers=markers, cell_type_column=\"celltype_major\"\n)\n</pre> # Find mutually exclusive genes based on scRNAseq data exclusive_gene_pairs = find_mutually_exclusive_genes(     adata=scRNAseq_adata, markers=markers, cell_type_column=\"celltype_major\" ) In\u00a0[\u00a0]: Copied! <pre># Compute MECR for each segmentation method\nmecr_results = {}\nfor method in segmentations_dict.keys():\n    mecr_results[method] = compute_MECR(segmentations_dict[method], exclusive_gene_pairs)\n</pre> # Compute MECR for each segmentation method mecr_results = {} for method in segmentations_dict.keys():     mecr_results[method] = compute_MECR(segmentations_dict[method], exclusive_gene_pairs) In\u00a0[\u00a0]: Copied! <pre># Compute quantized MECR area and counts using the mutually exclusive gene pairs\nquantized_mecr_area = {}\nquantized_mecr_counts = {}\n</pre> # Compute quantized MECR area and counts using the mutually exclusive gene pairs quantized_mecr_area = {} quantized_mecr_counts = {} In\u00a0[\u00a0]: Copied! <pre>for method in segmentations_dict.keys():\n    if \"cell_area\" in segmentations_dict[method].obs.columns:\n        quantized_mecr_area[method] = compute_quantized_mecr_area(\n            adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs\n        )\n    quantized_mecr_counts[method] = compute_quantized_mecr_counts(\n        adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs\n    )\n</pre> for method in segmentations_dict.keys():     if \"cell_area\" in segmentations_dict[method].obs.columns:         quantized_mecr_area[method] = compute_quantized_mecr_area(             adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs         )     quantized_mecr_counts[method] = compute_quantized_mecr_counts(         adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs     ) In\u00a0[\u00a0]: Copied! <pre># Plot MECR results\nplot_mecr_results(mecr_results, output_path=figures_path, palette=method_colors)\nplot_quantized_mecr_area(quantized_mecr_area, output_path=figures_path, palette=method_colors)\nplot_quantized_mecr_counts(quantized_mecr_counts, output_path=figures_path, palette=method_colors)\n</pre> # Plot MECR results plot_mecr_results(mecr_results, output_path=figures_path, palette=method_colors) plot_quantized_mecr_area(quantized_mecr_area, output_path=figures_path, palette=method_colors) plot_quantized_mecr_counts(quantized_mecr_counts, output_path=figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Filter segmentation methods for contamination analysis\nnew_segmentations_dict = {\n    k: v for k, v in segmentations_dict.items() if k in [\"segger\", \"Baysor\", \"10X\", \"10X-nucleus\", \"BIDCell\"]\n}\n</pre> # Filter segmentation methods for contamination analysis new_segmentations_dict = {     k: v for k, v in segmentations_dict.items() if k in [\"segger\", \"Baysor\", \"10X\", \"10X-nucleus\", \"BIDCell\"] } In\u00a0[\u00a0]: Copied! <pre># Compute contamination results\ncontamination_results = {}\nfor method, adata in new_segmentations_dict.items():\n    if \"cell_centroid_x\" in adata.obs.columns and \"cell_centroid_y\" in adata.obs.columns:\n        contamination_results[method] = calculate_contamination(\n            adata=adata,\n            markers=markers,  # Assuming you have a dictionary of markers for cell types\n            radius=15,\n            n_neighs=20,\n            celltype_column=\"celltype_major\",\n            num_cells=10000,\n        )\n</pre> # Compute contamination results contamination_results = {} for method, adata in new_segmentations_dict.items():     if \"cell_centroid_x\" in adata.obs.columns and \"cell_centroid_y\" in adata.obs.columns:         contamination_results[method] = calculate_contamination(             adata=adata,             markers=markers,  # Assuming you have a dictionary of markers for cell types             radius=15,             n_neighs=20,             celltype_column=\"celltype_major\",             num_cells=10000,         ) In\u00a0[\u00a0]: Copied! <pre># Prepare contamination data for boxplots\nboxplot_data = []\nfor method, df in contamination_results.items():\n    melted_df = df.reset_index().melt(\n        id_vars=[\"Source Cell Type\"], var_name=\"Target Cell Type\", value_name=\"Contamination\"\n    )\n    melted_df[\"Segmentation Method\"] = method\n    boxplot_data.append(melted_df)\n</pre> # Prepare contamination data for boxplots boxplot_data = [] for method, df in contamination_results.items():     melted_df = df.reset_index().melt(         id_vars=[\"Source Cell Type\"], var_name=\"Target Cell Type\", value_name=\"Contamination\"     )     melted_df[\"Segmentation Method\"] = method     boxplot_data.append(melted_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all contamination dataframes into one\nboxplot_data = pd.concat(boxplot_data)\n</pre> # Concatenate all contamination dataframes into one boxplot_data = pd.concat(boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot contamination results\nplot_contamination_results(contamination_results, output_path=figures_path, palette=method_colors)\nplot_contamination_boxplots(boxplot_data, output_path=figures_path, palette=method_colors)\n</pre> # Plot contamination results plot_contamination_results(contamination_results, output_path=figures_path, palette=method_colors) plot_contamination_boxplots(boxplot_data, output_path=figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Separate Segger into nucleus-positive and nucleus-negative cells\nsegmentations_dict[\"segger_n1\"] = segmentations_dict[\"segger\"][segmentations_dict[\"segger\"].obs.has_nucleus]\nsegmentations_dict[\"segger_n0\"] = segmentations_dict[\"segger\"][~segmentations_dict[\"segger\"].obs.has_nucleus]\n</pre> # Separate Segger into nucleus-positive and nucleus-negative cells segmentations_dict[\"segger_n1\"] = segmentations_dict[\"segger\"][segmentations_dict[\"segger\"].obs.has_nucleus] segmentations_dict[\"segger_n0\"] = segmentations_dict[\"segger\"][~segmentations_dict[\"segger\"].obs.has_nucleus] In\u00a0[\u00a0]: Copied! <pre># Compute clustering scores for all segmentation methods\nclustering_scores = {}\nfor method, adata in segmentations_dict.items():\n    ch_score, sh_score = compute_clustering_scores(adata, cell_type_column=\"celltype_major\")\n    clustering_scores[method] = (ch_score, sh_score)\n</pre> # Compute clustering scores for all segmentation methods clustering_scores = {} for method, adata in segmentations_dict.items():     ch_score, sh_score = compute_clustering_scores(adata, cell_type_column=\"celltype_major\")     clustering_scores[method] = (ch_score, sh_score) In\u00a0[\u00a0]: Copied! <pre># Plot UMAPs with clustering scores in the title\nplot_umaps_with_scores(segmentations_dict, clustering_scores, figures_path, palette=major_colors)\n</pre> # Plot UMAPs with clustering scores in the title plot_umaps_with_scores(segmentations_dict, clustering_scores, figures_path, palette=major_colors) In\u00a0[\u00a0]: Copied! <pre># Compute neighborhood metrics for methods with spatial data\nfor method, adata in segmentations_dict.items():\n    if \"spatial\" in list(adata.obsm.keys()):\n        compute_neighborhood_metrics(adata, radius=15, celltype_column=\"celltype_major\")\n</pre> # Compute neighborhood metrics for methods with spatial data for method, adata in segmentations_dict.items():     if \"spatial\" in list(adata.obsm.keys()):         compute_neighborhood_metrics(adata, radius=15, celltype_column=\"celltype_major\") In\u00a0[\u00a0]: Copied! <pre># Prepare neighborhood entropy data for boxplots\nentropy_boxplot_data = []\nfor method, adata in segmentations_dict.items():\n    if \"neighborhood_entropy\" in adata.obs.columns:\n        entropy_df = pd.DataFrame(\n            {\n                \"Cell Type\": adata.obs[\"celltype_major\"],\n                \"Neighborhood Entropy\": adata.obs[\"neighborhood_entropy\"],\n                \"Segmentation Method\": method,\n            }\n        )\n        # Filter out NaN values, keeping only the subsetted cells\n        entropy_df = entropy_df.dropna(subset=[\"Neighborhood Entropy\"])\n        entropy_boxplot_data.append(entropy_df)\n</pre> # Prepare neighborhood entropy data for boxplots entropy_boxplot_data = [] for method, adata in segmentations_dict.items():     if \"neighborhood_entropy\" in adata.obs.columns:         entropy_df = pd.DataFrame(             {                 \"Cell Type\": adata.obs[\"celltype_major\"],                 \"Neighborhood Entropy\": adata.obs[\"neighborhood_entropy\"],                 \"Segmentation Method\": method,             }         )         # Filter out NaN values, keeping only the subsetted cells         entropy_df = entropy_df.dropna(subset=[\"Neighborhood Entropy\"])         entropy_boxplot_data.append(entropy_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all entropy dataframes into one\nentropy_boxplot_data = pd.concat(entropy_boxplot_data)\n</pre> # Concatenate all entropy dataframes into one entropy_boxplot_data = pd.concat(entropy_boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot neighborhood entropy boxplots\nplot_entropy_boxplots(entropy_boxplot_data, figures_path, palette=method_colors)\n</pre> # Plot neighborhood entropy boxplots plot_entropy_boxplots(entropy_boxplot_data, figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Find markers for sensitivity calculation\npurified_markers = find_markers(scRNAseq_adata, \"celltype_major\", pos_percentile=20, percentage=75)\n</pre> # Find markers for sensitivity calculation purified_markers = find_markers(scRNAseq_adata, \"celltype_major\", pos_percentile=20, percentage=75) In\u00a0[\u00a0]: Copied! <pre># Calculate sensitivity for each segmentation method\nsensitivity_results_per_method = {}\nfor method, adata in segmentations_dict.items():\n    sensitivity_results = calculate_sensitivity(adata, purified_markers, max_cells_per_type=2000)\n    sensitivity_results_per_method[method] = sensitivity_results\n</pre> # Calculate sensitivity for each segmentation method sensitivity_results_per_method = {} for method, adata in segmentations_dict.items():     sensitivity_results = calculate_sensitivity(adata, purified_markers, max_cells_per_type=2000)     sensitivity_results_per_method[method] = sensitivity_results In\u00a0[\u00a0]: Copied! <pre># Prepare data for sensitivity boxplots\nsensitivity_boxplot_data = []\nfor method, sensitivity_results in sensitivity_results_per_method.items():\n    for cell_type, sensitivities in sensitivity_results.items():\n        method_df = pd.DataFrame({\"Cell Type\": cell_type, \"Sensitivity\": sensitivities, \"Segmentation Method\": method})\n        sensitivity_boxplot_data.append(method_df)\n</pre> # Prepare data for sensitivity boxplots sensitivity_boxplot_data = [] for method, sensitivity_results in sensitivity_results_per_method.items():     for cell_type, sensitivities in sensitivity_results.items():         method_df = pd.DataFrame({\"Cell Type\": cell_type, \"Sensitivity\": sensitivities, \"Segmentation Method\": method})         sensitivity_boxplot_data.append(method_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all sensitivity dataframes into one\nsensitivity_boxplot_data = pd.concat(sensitivity_boxplot_data)\n</pre> # Concatenate all sensitivity dataframes into one sensitivity_boxplot_data = pd.concat(sensitivity_boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot sensitivity boxplots\nplot_sensitivity_boxplots(sensitivity_boxplot_data, figures_path, palette=method_colors)\n</pre> # Plot sensitivity boxplots plot_sensitivity_boxplots(sensitivity_boxplot_data, figures_path, palette=method_colors)"},{"location":"notebooks/segger_tutorial/","title":"Introduction to Segger","text":"<p>Installing segger from the GitHub repository:</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/EliHei2/segger_dev.git\n%cd segger_dev\n!pip install \".[rapids12]\" -q\n</pre> !git clone https://github.com/EliHei2/segger_dev.git %cd segger_dev !pip install \".[rapids12]\" -q <p>Downloading the Xenium Human Pancreatic Dataset:</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir data_xenium\n%cd data_xenium\n!wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n!unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n%cd ..\n</pre> !mkdir data_xenium %cd data_xenium !wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip !unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip %cd .. In\u00a0[\u00a0]: Copied! <pre>from segger.data.parquet.sample import STSampleParquet\nfrom segger.training.segger_data_module import SeggerDataModule\nfrom segger.training.train import LitSegger\nfrom segger.prediction.predict_parquet import segment, load_model\nfrom lightning.pytorch.loggers import CSVLogger\nfrom pytorch_lightning import Trainer\nfrom pathlib import Path\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport scanpy as sc\n</pre> from segger.data.parquet.sample import STSampleParquet from segger.training.segger_data_module import SeggerDataModule from segger.training.train import LitSegger from segger.prediction.predict_parquet import segment, load_model from lightning.pytorch.loggers import CSVLogger from pytorch_lightning import Trainer from pathlib import Path import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import scanpy as sc In\u00a0[\u00a0]: Copied! <pre>xenium_data_dir = Path('data_xenium')\nsegger_data_dir = Path('data_segger')\n\nsample = STSampleParquet(\n    base_dir=xenium_data_dir,\n    n_workers=4,\n    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n    # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available\n)\n\nsample.save(\n      data_dir=segger_data_dir,\n      k_bd=3,\n      dist_bd=15.0,\n      k_tx=3,\n      dist_tx=5.0,\n      tile_width=120,\n      tile_height=120,\n      neg_sampling_ratio=5.0,\n      frac=1.0,\n      val_prob=0.1,\n      test_prob=0.2,\n)\n</pre> xenium_data_dir = Path('data_xenium') segger_data_dir = Path('data_segger')  sample = STSampleParquet(     base_dir=xenium_data_dir,     n_workers=4,     sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.     # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available )  sample.save(       data_dir=segger_data_dir,       k_bd=3,       dist_bd=15.0,       k_tx=3,       dist_tx=5.0,       tile_width=120,       tile_height=120,       neg_sampling_ratio=5.0,       frac=1.0,       val_prob=0.1,       test_prob=0.2, ) In\u00a0[\u00a0]: Copied! <pre>from segger.data.utils import calculate_gene_celltype_abundance_embedding\nscrnaseq_file = Path('my_scRNAseq_file.h5ad')\ncelltype_column = 'celltype_column'\ngene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n    sc.read(scrnaseq_file),\n    celltype_column\n)\n\nsample = STSampleParquet(\n    base_dir=xenium_data_dir,\n    n_workers=4,\n    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n    weights=gene_celltype_abundance_embedding, \n)\n</pre> from segger.data.utils import calculate_gene_celltype_abundance_embedding scrnaseq_file = Path('my_scRNAseq_file.h5ad') celltype_column = 'celltype_column' gene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(     sc.read(scrnaseq_file),     celltype_column )  sample = STSampleParquet(     base_dir=xenium_data_dir,     n_workers=4,     sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.     weights=gene_celltype_abundance_embedding,  ) In\u00a0[\u00a0]: Copied! <pre># Base directory to store Pytorch Lightning models\nmodels_dir = Path('models')\n\n# Initialize the Lightning data module\ndm = SeggerDataModule(\n    data_dir=segger_data_dir,\n    batch_size=2,\n    num_workers=2,\n)\n\ndm.setup()\n\nis_token_based = True\nnum_tx_tokens = 500\n\n# If you use custom gene embeddings, use the following two lines instead:\n# is_token_based = False\n# num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes\n\n\nnum_bd_features = dm.train[0].x_dict[\"bd\"].shape[1]\n\n# Initialize the Lightning model\nls = LitSegger(\n    is_token_based = is_token_based,\n    num_node_features = {\"tx\": num_tx_tokens, \"bd\": num_bd_features},\n    init_emb=8,    \n    hidden_channels=64,\n    out_channels=16,\n    heads=4,\n    num_mid_layers=1,\n    aggr='sum',\n)\n\n# Initialize the Lightning trainer\ntrainer = Trainer(\n    accelerator='cuda',\n    strategy='auto',\n    precision='16-mixed',\n    devices=1, # set higher number if more gpus are available\n    max_epochs=100,\n    default_root_dir=models_dir,\n    logger=CSVLogger(models_dir),\n)\n</pre> # Base directory to store Pytorch Lightning models models_dir = Path('models')  # Initialize the Lightning data module dm = SeggerDataModule(     data_dir=segger_data_dir,     batch_size=2,     num_workers=2, )  dm.setup()  is_token_based = True num_tx_tokens = 500  # If you use custom gene embeddings, use the following two lines instead: # is_token_based = False # num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes   num_bd_features = dm.train[0].x_dict[\"bd\"].shape[1]  # Initialize the Lightning model ls = LitSegger(     is_token_based = is_token_based,     num_node_features = {\"tx\": num_tx_tokens, \"bd\": num_bd_features},     init_emb=8,         hidden_channels=64,     out_channels=16,     heads=4,     num_mid_layers=1,     aggr='sum', )  # Initialize the Lightning trainer trainer = Trainer(     accelerator='cuda',     strategy='auto',     precision='16-mixed',     devices=1, # set higher number if more gpus are available     max_epochs=100,     default_root_dir=models_dir,     logger=CSVLogger(models_dir), ) In\u00a0[\u00a0]: Copied! <pre># Fit model\ntrainer.fit(\n    model=ls,\n    datamodule=dm\n)\n</pre> # Fit model trainer.fit(     model=ls,     datamodule=dm ) <p>Key parameters for training:</p> <ul> <li><code>--data_dir</code>: Directory containing the training data.</li> <li><code>--model_dir</code>: Directory in which to store models.</li> <li><code>--epochs</code>: Specifies the number of training epochs.</li> <li><code>--batch_size</code>: Batch sizes for training and validation data.</li> <li><code>--learning_rate</code>: The initial learning rate for the optimizer.</li> <li><code>--hidden_channels</code>: Number of hidden channels in the GNN layers.</li> <li><code>--heads</code>: Number of attention heads used in each graph convolutional layer.</li> <li><code>--init_emb</code>: Sets the dimensionality of the initial embeddings applied to the input node features (e.g., transcripts). A higher embedding dimension may capture more feature complexity but also requires more computation.</li> <li><code>--out_channels</code>: Specifies the number of output channels after the final graph attention layer, e.g. the final learned representations of the graph nodes.</li> </ul> <p>Additional Options for Training the Segger Model:</p> <ul> <li><code>--aggr</code>: This option controls the aggregation method used in the graph convolution layers.</li> <li><code>--accelerator</code>: Controls the hardware used for training, such as <code>cuda</code> for GPU training. This enables Segger to leverage GPU resources for faster training, especially useful for large datasets.</li> <li><code>--strategy</code>: Defines the distributed training strategy, with <code>auto</code> allowing PyTorch Lightning to automatically configure the best strategy based on the hardware setup.</li> <li><code>--precision</code>: Enables mixed precision training (e.g., <code>16-mixed</code>), which can speed up training and reduce memory usage while maintaining accuracy.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Evaluate results\nmodel_version = 0  # 'v_num' from training output above\nmodel_path = models_dir / 'lightning_logs' / f'version_{model_version}'\nmetrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)\n\nfig, ax = plt.subplots(1,1, figsize=(2,2))\n\nfor col in metrics.columns.difference(['epoch']):\n    metric = metrics[col].dropna()\n    ax.plot(metric.index, metric.values, label=col)\n\nax.legend(loc=(1, 0.33))\nax.set_ylim(0, 1)\nax.set_xlabel('Step')\n</pre> # Evaluate results model_version = 0  # 'v_num' from training output above model_path = models_dir / 'lightning_logs' / f'version_{model_version}' metrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)  fig, ax = plt.subplots(1,1, figsize=(2,2))  for col in metrics.columns.difference(['epoch']):     metric = metrics[col].dropna()     ax.plot(metric.index, metric.values, label=col)  ax.legend(loc=(1, 0.33)) ax.set_ylim(0, 1) ax.set_xlabel('Step') Out[\u00a0]: <pre>Text(0.5, 0, 'Step')</pre> In\u00a0[\u00a0]: Copied! <pre>dm = SeggerDataModule(\n    data_dir='data_segger',\n    batch_size=1,\n    num_workers=4,\n)\n\ndm.setup()\n\nmodel_version = 0\nmodel_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\"\nmodel = load_model(model_path / \"checkpoints\")\n\nreceptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}\n\nsegment(\n    model,\n    dm,\n    save_dir='benchmarks',\n    seg_tag='segger_output',\n    transcript_file='data_xenium/transcripts.parquet',\n    receptive_field=receptive_field,\n    min_transcripts=5,\n    cell_id_col='segger_cell_id',\n    use_cc=False,\n    knn_method='kd_tree',\n    verbose=True,\n)\n</pre> dm = SeggerDataModule(     data_dir='data_segger',     batch_size=1,     num_workers=4, )  dm.setup()  model_version = 0 model_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\" model = load_model(model_path / \"checkpoints\")  receptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}  segment(     model,     dm,     save_dir='benchmarks',     seg_tag='segger_output',     transcript_file='data_xenium/transcripts.parquet',     receptive_field=receptive_field,     min_transcripts=5,     cell_id_col='segger_cell_id',     use_cc=False,     knn_method='kd_tree',     verbose=True, )  <pre>Starting segmentation for segger_embedding_1001...\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(2,2))\nsns.histplot(\n    segmentation['score'],\n    bins=50,\n    ax=ax,\n)\nax.set_ylabel('Count')\nax.set_xlabel('Segger Similarity Score')\nax.set_yscale('log')\n</pre> fig, ax = plt.subplots(1,1, figsize=(2,2)) sns.histplot(     segmentation['score'],     bins=50,     ax=ax, ) ax.set_ylabel('Count') ax.set_xlabel('Segger Similarity Score') ax.set_yscale('log') In\u00a0[\u00a0]: Copied! <pre>import itertools\nimport pandas as pd\n</pre> import itertools import pandas as pd In\u00a0[\u00a0]: Copied! <pre>tuning_dir = Path('path/to/tutorial/tuning/')\nsampling_rate = 0.125\n</pre> tuning_dir = Path('path/to/tutorial/tuning/') sampling_rate = 0.125 In\u00a0[\u00a0]: Copied! <pre># Fixed function arguments used for each trial\ntranscripts_path = xenium_data_dir / 'transcripts.parquet'\n\nboundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'\n\ndataset_kwargs = dict(\n    x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,\n    num_workers=4, sampling_rate=sampling_rate,\n)\n\nmodel_kwargs = dict(\n    metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),\n    num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,\n    heads=2, num_mid_layers=2, aggr='sum',\n)\n\ntrainer_kwargs = dict(\n    accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,\n    max_epochs=100,\n)\n\npredict_kwargs = dict(score_cut=0.2, use_cc=True)\n</pre> # Fixed function arguments used for each trial transcripts_path = xenium_data_dir / 'transcripts.parquet'  boundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'  dataset_kwargs = dict(     x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,     num_workers=4, sampling_rate=sampling_rate, )  model_kwargs = dict(     metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),     num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,     heads=2, num_mid_layers=2, aggr='sum', )  trainer_kwargs = dict(     accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,     max_epochs=100, )  predict_kwargs = dict(score_cut=0.2, use_cc=True) In\u00a0[\u00a0]: Copied! <pre>def trainable(config):\n\n    receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}\n\n    # Dataset creation\n    xs = XeniumSample(verbose=False)\n    xs.set_file_paths(transcripts_path, boundaries_path)\n    xs.set_metadata()\n    try:\n        xs.save_dataset_for_segger(\n            processed_dir=config['data_dir'],\n            receptive_field=receptive_field,\n            **dataset_kwargs,\n        )\n    except:\n        pass\n\n    # Model training\n    ls = LitSegger(**model_kwargs)\n    dm = SeggerDataModule(\n        data_dir=config['data_dir'],\n        batch_size=2,\n        num_workers=dataset_kwargs['num_workers'],\n    )\n    trainer = Trainer(\n        default_root_dir=config['model_dir'],\n        logger=CSVLogger(config['model_dir']),\n        **trainer_kwargs,\n    )\n    trainer.fit(model=ls, datamodule=dm)\n\n    segmentation = predict(\n        load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),\n        dm.train_dataloader(),\n        receptive_field=receptive_field,\n        **predict_kwargs,\n    )\n\n    metrics = evaluate(segmentation)\n\n\ndef evaluate(segmentation: pd.DataFrame, score_cut: float) -&gt; pd.Series:\n\n    assigned = segmentation['score'] &gt; score_cut\n    metrics = pd.Series(dtype=float)\n    metrics['frac_assigned'] = assigned.mean()\n    cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()\n    assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()\n    cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()\n    metrics['cell_size_assigned'] = assigned_avg\n    metrics['cell_size_cc'] = cc_avg\n    return metrics\n</pre> def trainable(config):      receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}      # Dataset creation     xs = XeniumSample(verbose=False)     xs.set_file_paths(transcripts_path, boundaries_path)     xs.set_metadata()     try:         xs.save_dataset_for_segger(             processed_dir=config['data_dir'],             receptive_field=receptive_field,             **dataset_kwargs,         )     except:         pass      # Model training     ls = LitSegger(**model_kwargs)     dm = SeggerDataModule(         data_dir=config['data_dir'],         batch_size=2,         num_workers=dataset_kwargs['num_workers'],     )     trainer = Trainer(         default_root_dir=config['model_dir'],         logger=CSVLogger(config['model_dir']),         **trainer_kwargs,     )     trainer.fit(model=ls, datamodule=dm)      segmentation = predict(         load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),         dm.train_dataloader(),         receptive_field=receptive_field,         **predict_kwargs,     )      metrics = evaluate(segmentation)   def evaluate(segmentation: pd.DataFrame, score_cut: float) -&gt; pd.Series:      assigned = segmentation['score'] &gt; score_cut     metrics = pd.Series(dtype=float)     metrics['frac_assigned'] = assigned.mean()     cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()     assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()     cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()     metrics['cell_size_assigned'] = assigned_avg     metrics['cell_size_cc'] = cc_avg     return metrics In\u00a0[\u00a0]: Copied! <pre>param_space = {\n    \"k_bd\": [3, 5, 10],\n    \"dist_bd\": [5, 10, 15, 20],\n    \"k_tx\": [3, 5, 10],\n    \"dist_tx\": [3, 5, 10],\n}\n\nmetrics = []\n\nfor params in itertools.product(*param_space.values()):\n\n    config = dict(zip(param_space.keys(), params))\n\n    # Setup directories\n    trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])\n\n    data_dir = trial_dir / 'segger_data'\n    data_dir.mkdir(exist_ok=True, parents=True)\n    config['data_dir'] = data_dir\n\n    model_dir = trial_dir / 'models'\n    model_dir.mkdir(exist_ok=True, parents=True)\n    config['model_dir'] = model_dir\n\n    segmentation = trainable(config)\n    trial = evaluate(segmentation, predict_kwargs['score_cut'])\n    trial = pd.concat([pd.Series(config), trial])\n    metrics.append(trial)\n\nmetrics = pd.DataFrame(metrics)\n</pre> param_space = {     \"k_bd\": [3, 5, 10],     \"dist_bd\": [5, 10, 15, 20],     \"k_tx\": [3, 5, 10],     \"dist_tx\": [3, 5, 10], }  metrics = []  for params in itertools.product(*param_space.values()):      config = dict(zip(param_space.keys(), params))      # Setup directories     trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])      data_dir = trial_dir / 'segger_data'     data_dir.mkdir(exist_ok=True, parents=True)     config['data_dir'] = data_dir      model_dir = trial_dir / 'models'     model_dir.mkdir(exist_ok=True, parents=True)     config['model_dir'] = model_dir      segmentation = trainable(config)     trial = evaluate(segmentation, predict_kwargs['score_cut'])     trial = pd.concat([pd.Series(config), trial])     metrics.append(trial)  metrics = pd.DataFrame(metrics) In\u00a0[\u00a0]: Copied! <pre>metrics\n</pre> metrics"},{"location":"notebooks/segger_tutorial/#introduction-to-segger","title":"Introduction to Segger\u00b6","text":"<p>Important note (Dec 2024): As segger is currently undergoing constant development we highly recommend installing directly via github.</p> <p>Segger is a cutting-edge cell segmentation model specifically designed for single-molecule resolved spatial omics datasets. It addresses the challenge of accurately segmenting individual cells in complex imaging datasets, leveraging a unique approach based on graph neural networks (GNNs).</p> <p>The core idea behind Segger is to model both nuclei and transcripts as graph nodes, with edges connecting them based on their spatial proximity. This allows the model to learn from the co-occurrence of nucleic and cytoplasmic molecules, resulting in more refined and accurate cell boundaries. By using spatial information and GNNs, Segger achieves state-of-the-art performance in segmenting single cells in datasets such as 10X Xenium and MERSCOPE, outperforming traditional methods like Baysor and Cellpose.</p> <p>Segger's workflow consists of:</p> <ol> <li>Dataset creation: Converting raw transcriptomic data into a graph-based dataset.</li> <li>Training: Training the Segger model on the graph to learn cell boundaries.</li> <li>Prediction: Using the trained model to make predictions on new datasets.</li> </ol> <p>This tutorial will guide you through each step of the process, ensuring you can train and apply Segger for your own data.</p>"},{"location":"notebooks/segger_tutorial/#1-create-your-segger-dataset","title":"1. Create your Segger Dataset\u00b6","text":"<p>In this step, we generate the dataset required for Segger's cell segmentation tasks.</p> <p>Segger relies on spatial transcriptomics data, combining staining boundaries (e.g., nuclei or membrane stainings) and transcripts from single-cell resolved imaging datasets. These nuclei and transcript nodes are represented in a graph, and the spatial proximity of transcripts to nuclei is used to establish edges between them.</p> <p>To use Segger with a Xenium dataset, you need the <code>transcripts.parquet</code> and <code>nucleus_boundaries.parquet</code> (or <code>cell_boundaries.parquet</code>, in case the Xenium samples comes with the segmentation kit) files. The transcripts file contains spatial coordinates and information for each transcript, while the boundaries file defines the polygon boundaries of the nuclei or cells. These files enable segger to map transcripts to their respective nuclei and perform cell segmentation based on spatial relationships. Segger can also be extended to other platforms by modifying the column names or formats in the input files to match its expected structure, making it adaptable for various spatial transcriptomics technologies. See this for Xenium settings.</p>"},{"location":"notebooks/segger_tutorial/#11-fast-dataset-creation-with-segger","title":"1.1. Fast Dataset Creation with segger\u00b6","text":"<p>Segger introduces a fast and efficient pipeline for processing spatial transcriptomics data. This method accelerates dataset creation, particularly for large datasets, by using ND-tree-based spatial partitioning and parallel processing. This results in a much faster preparation of the dataset, which is saved in PyTorch Geometric (PyG) format, similar to the previous method.</p> <p>Note: The previous dataset creation method will soon be deprecated in favor of this optimized pipeline.</p> <p>The pipeline requires the following inputs:</p> <ul> <li>base_dir: The directory containing the raw dataset.</li> <li>data_dir: The directory where the processed dataset (tiles in PyG format) will be saved.</li> </ul> <p>The core improvements in this method come from the use of ND-tree partitioning, which splits the data efficiently into spatial regions, and parallel processing, which speeds up the handling of these regions across multiple CPU cores. For example, using this pipeline, the Xenium Human Pancreatic Dataset can be processed in just a few minutes when running with 16 workers.</p> <p>Below is an example of how to create a dataset using the faster Segger pipeline:</p>"},{"location":"notebooks/segger_tutorial/#parameters","title":"Parameters\u00b6","text":"<p>Here is a complete list of parameters you can use to control the dataset creation process:</p> <ul> <li>--base_dir: Directory containing the raw spatial transcriptomics dataset.</li> <li>--data_dir: Directory where the processed Segger dataset (in PyG format) will be saved.</li> <li>--sample_type: (Optional) Specifies the type of dataset (e.g., \"xenium\" or \"merscope\"). Defaults to None.</li> <li>--scrnaseq_file: Path to the scRNAseq file (default: None).</li> <li>--celltype_column: Column name for cell type annotations in the scRNAseq file (default: None).</li> <li>--k_bd: Number of nearest neighbors for boundary nodes (default: 3).</li> <li>--dist_bd: Maximum distance for boundary neighbors (default: 15.0).</li> <li>--k_tx: Number of nearest neighbors for transcript nodes (default: 3).</li> <li>--dist_tx: Maximum distance for transcript neighbors (default: 5.0).</li> <li>--tile_size: Specifies the size of the tile. If provided, it overrides both tile_width and tile_height.</li> <li>--tile_width: Width of the tiles in pixels (ignored if tile_size is provided).</li> <li>--tile_height: Height of the tiles in pixels (ignored if tile_size is provided).</li> <li>--neg_sampling_ratio: Ratio of negative samples (default: 5.0).</li> <li>--frac: Fraction of the dataset to process (default: 1.0).</li> <li>--val_prob: Proportion of data used for validation split (default: 0.1).</li> <li>--test_prob: Proportion of data used for testing split (default: 0.2).</li> <li>--n_workers: Number of workers for parallel processing (default: 1).</li> </ul>"},{"location":"notebooks/segger_tutorial/#12-using-custom-gene-embeddings","title":"1.2. Using custom gene embeddings\u00b6","text":"<p>In the default mode, segger initially tokenizes transcripts based on their gene type simply in a one-hot manner. However, one can use other genes embeddings (e.g., pre-trained embeddings). The following example shows how one can employ a cell-type-annotated scRNAseq reference of the same tissue type (not necessary same sample or experiment) to embed genes based on their abaundance in different cell types:</p>"},{"location":"notebooks/segger_tutorial/#2-train-your-segger-model","title":"2. Train your Segger Model\u00b6","text":"<p>The Segger model training process begins after the dataset has been created. This model is a heterogeneous graph neural network (GNN) designed to segment single cells by leveraging both nuclei and transcript data.</p> <p>Segger uses graph attention layers to propagate information across nodes (nuclei and transcripts) and refine cell boundaries. The model architecture includes initial embedding layers, attention-based graph convolutions, and residual connections for stable learning.</p> <p>Segger leverages the PyTorch Lightning framework to streamline the training and evaluation of its graph neural network (GNN). PyTorch Lightning simplifies the training process by abstracting away much of the boilerplate code, allowing users to focus on model development and experimentation. It also supports multi-GPU training, mixed-precision, and efficient scaling, making it an ideal framework for training complex models like Segger.</p>"},{"location":"notebooks/segger_tutorial/#troubleshooting-1","title":"Troubleshooting #1\u00b6","text":"<p>In the cell below, we are visualizing key metrics from the model training and validation process. The plot displays training loss, validation loss, F1 validation score, and AUROC validation score over training steps. We expect to see the loss curves decreasing over time, signaling the model's improvement, and the F1 and AUROC scores increasing, reflecting improved segmentation performance as the model learns.</p> <p>If training is not working effectively, you might observe the following in the plot displaying training loss, validation loss, F1 score, and AUROC:</p> <ul> <li>Training loss not decreasing: If the training loss remains high or fluctuates without a consistent downward trend, this indicates that the model is not learning effectively from the training data.</li> <li>Validation loss decreases, then increases: If validation loss decreases initially but starts to increase while training loss continues to drop, this could be a sign of overfitting, where the model is performing well on the training data but not generalizing to the validation data.</li> <li>F1 score and AUROC not improving: If these metrics remain flat or show inconsistent improvement, the model may be struggling to correctly segment cells or classify transcripts, indicating an issue with learning performance.</li> </ul>"},{"location":"notebooks/segger_tutorial/#3-make-predictions","title":"3. Make Predictions\u00b6","text":"<p>Once the Segger model is trained, it can be used to make predictions on seen (partially trained) data or be transfered to unseen data. This step involves using a trained checkpoint to predict cell boundaries and refine transcript-nuclei associations.</p>"},{"location":"notebooks/segger_tutorial/#requirements-for-the-faster-prediction-pipeline","title":"Requirements for the Faster Prediction Pipeline\u00b6","text":"<p>The pipeline requires the following inputs:</p> <ul> <li>segger_data_dir: The directory containing the processed Segger dataset (in PyG format).</li> <li>models_dir: The directory containing the trained Segger model checkpoints.</li> <li>benchmarks_dir: The directory where the segmentation results will be saved.</li> <li>transcripts_file: Path to the file containing the transcript data for prediction.</li> </ul>"},{"location":"notebooks/segger_tutorial/#running-the-faster-prediction-pipeline","title":"Running the Faster Prediction Pipeline\u00b6","text":"<p>Below is an example of how to run the faster Segger prediction pipeline using the command line:</p>"},{"location":"notebooks/segger_tutorial/#parameters","title":"Parameters\u00b6","text":"<p>Here is a detailed explanation of each parameter used in the faster prediction pipeline:</p> <ul> <li>--segger_data_dir: The directory containing the processed Segger dataset, saved as PyTorch Geometric data objects, that will be used for prediction.</li> <li>--models_dir: The directory containing the trained Segger model checkpoints. These checkpoints store the learned weights required for making predictions.</li> <li>--benchmarks_dir: The directory where the segmentation results will be saved.</li> <li>--transcripts_file: Path to the transcripts.parquet file.</li> <li>--batch_size: Specifies the batch size for processing during prediction. Larger batch sizes speed up inference but use more memory (default: 1).</li> <li>--num_workers: Number of workers to use for parallel data loading (default: 1).</li> <li>--model_version: Version of the trained model to load for predictions, based on the version number from the training logs (default: 0).</li> <li>--save_tag: A tag used to name and organize the segmentation results (default: segger_embedding).</li> <li>--min_transcripts: The minimum number of transcripts required for segmentation (default: 5).</li> <li>--cell_id_col: The name of the column that stores the cell IDs (default: segger_cell_id).</li> <li>--use_cc: Enables the use of connected components (CC) for grouping transcripts that are not associated with any nucleus (default: False).</li> <li>--knn_method: Method for KNN (K-Nearest Neighbors) computation. Only option is \"cuda\" for this pipeline (default: cuda).</li> <li>--file_format: The format for saving the output segmentation data. Only option is \"anndata\" for this pipeline (default: anndata).</li> <li>--k_bd: Number of nearest neighbors for boundary nodes during segmentation (default: 4).</li> <li>--dist_bd: Maximum distance for boundary nodes during segmentation (default: 12.0).</li> <li>--k_tx: Number of nearest neighbors for transcript nodes during segmentation (default: 5).</li> <li>--dist_tx: Maximum distance for transcript nodes during segmentation (default: 5.0).</li> </ul>"},{"location":"notebooks/segger_tutorial/#troubleshooting-2","title":"Troubleshooting #2\u00b6","text":"<p>In the cell below, we are visualizing the distribution of Segger similarity scores using a histogram. The Segger similarity score reflects how closely transcripts are associated with their respective nuclei in the segmentation process. Higher scores indicate stronger associations between transcripts and their nuclei, suggesting more accurate cell boundaries. Lower scores might indicate weaker associations, which could highlight potential segmentation errors or challenging regions in the data. We expect to see a large number of the scores clustering toward higher values, which would indicate strong overall performance of the model in associating transcripts with nuclei.</p> <p>The following would indicate potential issues with the model's predictions:</p> <ul> <li>A very large portion of scores near zero: If many scores are concentrated at the lower end of the scale, this suggests that the model is frequently failing to associate transcripts with their corresponding nuclei, indicating poor segmentation quality.</li> <li>No clear peak in the distribution: If the histogram is flat or shows a wide, spread-out distribution, this could indicate that the model is struggling to consistently assign similarity scores, which may be a sign that the training process did not optimize the model correctly.</li> </ul> <p>Both cases would suggest that the model requires further tuning, such as adjusting hyperparameters, data preprocessing, or the training procedure (see below)</p>"},{"location":"notebooks/segger_tutorial/#the-importance-of-the-receptive-field-in-segger","title":"The Importance of the Receptive Field in Segger\u00b6","text":"<p>The receptive field is a critical parameter in Segger, as it directly influences how the model interprets the spatial relationships between transcripts and nuclei. In the context of spatial transcriptomics, the receptive field determines the size of the neighborhood that each node (representing transcripts or nuclei) can \"see\" during graph construction and model training. Segger is particularly sensitive to the size of the receptive field because it affects the model's ability to propagate information across the graph. If the receptive field is too small, the model may fail to capture sufficient context for correct cell boundary delineation. Conversely, a very large receptive field may introduce noise by linking unrelated or distant nodes, reducing segmentation accuracy.</p>"},{"location":"notebooks/segger_tutorial/#parameters-affecting-the-receptive-field-in-segger","title":"Parameters affecting the receptive field in Segger:\u00b6","text":"<ul> <li><code>--r</code>: This parameter defines the radius used when connecting transcripts to nuclei. A larger <code>r</code> expands the receptive field, linking more distant nodes. Fine-tuning this parameter helps ensure that Segger captures the right level of spatial interaction in the dataset.</li> <li><code>--k_bd</code> and <code>--k_tx</code>: These control the number of nearest neighbors (nuclei and transcripts, respectively) considered in the graph. By increasing these values, the receptive field is effectively broadened, allowing more nodes to contribute to the information propagation.</li> <li><code>--dist_bd</code> and <code>--dist_tx</code>: These parameters specify the maximum distances used to connect nuclei (<code>dist_bd</code>) and transcripts (<code>dist_tx</code>) to their neighbors during graph construction. They directly affect the receptive field by defining the cut-off distance for forming edges in the graph. Larger distance values expand the receptive field, connecting nodes that are further apart spatially. Careful tuning of these values is necessary to ensure that Segger captures relevant spatial relationships without introducing noise.</li> </ul>"},{"location":"notebooks/segger_tutorial/#4-tune-parameters","title":"4. Tune Parameters\u00b6","text":""},{"location":"notebooks/segger_tutorial/#evaluating-receptive-field-parameters-with-grid-search","title":"Evaluating Receptive Field Parameters with Grid Search\u00b6","text":"<p>To evaluate the impact of different receptive field parameters in Segger, we use a grid search approach. The parameters <code>k_bd</code>, <code>k_tx</code>, <code>dist_bd</code>, and <code>dist_tx</code> (which control the number of neighbors and distances for nuclei and transcripts) are explored through various configurations defined in <code>param_space</code>. Each combination of these parameters is passed to the <code>trainable</code> function, which creates the dataset, trains the model, and makes predictions based on the specified receptive field.</p> <p>For each parameter combination:</p> <ol> <li>A dataset is created with the specified receptive field.</li> <li>The Segger model is trained on this dataset.</li> <li>Predictions are made, and segmentation results are evaluated using the custom <code>evaluate</code> function. This function computes metrics like the fraction of assigned transcripts and average cell sizes.</li> </ol> <p>The results from each configuration are saved, allowing us to compare how different receptive field settings impact the model\u2019s performance. This process enables a thorough search of the parameter space, optimizing the model for accurate segmentation.</p>"},{"location":"notebooks/segger_tutorial/#interpreting-output-metrics","title":"Interpreting Output Metrics\u00b6","text":"<p>The key output metrics include:</p> <ul> <li><code>frac_assigned</code>: The fraction of transcripts that were successfully assigned to a cell. A higher value indicates that the model is doing a good job associating transcripts with nuclei, which is a strong indicator of successful segmentation.</li> <li><code>cell_size_assigned</code>: The average size of cells that have assigned transcripts. This helps assess how well the model is predicting cell boundaries, with unusually large or small values indicating potential issues with segmentation accuracy.</li> <li><code>cell_size_cc</code>: The average size of connected components that were not assigned to a cell (i.e., nucleus-less regions). Large values here may suggest that transcripts are being incorrectly grouped together in the absence of a nucleus, which could indicate problems with the receptive field parameters or the segmentation process.</li> </ul> <p>These metrics illuminate the effectiveness of the model by highlighting both the success in associating transcripts with cells and potential areas where the model may need further tuning.</p>"},{"location":"source/conf/","title":"Conf","text":"<p>Configuration file for the Sphinx documentation builder.</p> <p>For the full list of built-in configuration values, see the documentation: https://www.sphinx-doc.org/en/master/usage/configuration.html</p> <p>-- Project information ----------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information</p> In\u00a0[\u00a0]: Copied! <pre>project = \"segger\"\ncopyright = \"2024, Elyas Heidari\"\nauthor = \"Elyas Heidari\"\nrelease = \"0.01\"\n</pre> project = \"segger\" copyright = \"2024, Elyas Heidari\" author = \"Elyas Heidari\" release = \"0.01\" <p>-- General configuration --------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration</p> In\u00a0[\u00a0]: Copied! <pre>extensions = []\n</pre> extensions = [] In\u00a0[\u00a0]: Copied! <pre>templates_path = [\"_templates\"]\nexclude_patterns = []\n</pre> templates_path = [\"_templates\"] exclude_patterns = [] <p>-- Options for HTML output ------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output</p> In\u00a0[\u00a0]: Copied! <pre>html_theme = \"alabaster\"\nhtml_static_path = [\"_static\"]\n</pre> html_theme = \"alabaster\" html_static_path = [\"_static\"]"},{"location":"user_guide/","title":"API Reference","text":""},{"location":"user_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Dataset Creation</li> <li>Training</li> <li>Validation</li> </ul>"},{"location":"user_guide/data_creation/","title":"Data Preparation for <code>segger</code>","text":"<p>The <code>segger</code> package provides a comprehensive data preparation module for cell segmentation and subsequent graph-based deep learning tasks by leveraging scalable and efficient processing tools. </p> <p>Note</p> <p>Currently, <code>segger</code> supports Xenium and Merscope datasets. </p>"},{"location":"user_guide/data_creation/#steps","title":"Steps","text":"<p>The data preparation module offers the following key functionalities:</p> <ol> <li>Lazy Loading of Large Datasets: Utilizes Dask to handle large-scale transcriptomics and boundary datasets efficiently, avoiding memory bottlenecks.</li> <li>Initial Filtering: Filters transcripts based on quality metrics and dataset-specific criteria to ensure data integrity and relevance.</li> <li>Tiling: Divides datasets into spatial tiles, essential for localized graph-based models and parallel processing.</li> <li>Graph Construction: Converts spatial data into graph formats using PyTorch Geometric (PyG), enabling the application of graph neural networks (GNNs).</li> <li>Boundary Processing: Handles polygons, performs spatial geometrical calculations, and checks transcript overlaps with boundaries.</li> </ol> <p>Key Technologies</p> <ul> <li>Dask: Facilitates parallel and lazy data processing, enabling scalable handling of large datasets.</li> <li>PyTorch Geometric (PyG): Enables the construction of graph-based data representations suitable for GNNs.</li> <li>Shapely &amp; Geopandas: Utilized for spatial operations such as polygon creation, scaling, and spatial relationship computations.</li> <li>Dask-Geopandas: Extends Geopandas for parallel processing of geospatial data, enhancing scalability.</li> </ul>"},{"location":"user_guide/data_creation/#core-components","title":"Core Components","text":""},{"location":"user_guide/data_creation/#1-spatialtranscriptomicssample-abstract-base-class","title":"1. <code>SpatialTranscriptomicsSample</code> (Abstract Base Class)","text":"<p>This abstract class defines the foundational structure for managing spatial transcriptomics datasets. It provides essential methods for:</p> <ul> <li>Loading Data: Scalable loading of transcript and boundary data using Dask.</li> <li>Filtering Transcripts: Applying quality-based or dataset-specific filtering criteria.</li> <li>Spatial Relationships: Computing overlaps and spatial relationships between transcripts and boundaries.</li> <li>Tiling: Dividing datasets into smaller spatial tiles for localized processing.</li> <li>Graph Preparation: Converting data tiles into <code>PyTorch Geometric</code> graph structures.</li> </ul>"},{"location":"user_guide/data_creation/#key-methods","title":"Key Methods:","text":"<ul> <li><code>load_transcripts()</code>: Loads transcriptomic data from Parquet files, applies quality filtering, and incorporates additional gene embeddings.</li> <li><code>load_boundaries()</code>: Loads boundary data (e.g., cell or nucleus boundaries) from Parquet files.</li> <li><code>get_tile_data()</code>: Retrieves transcriptomic and boundary data within specified spatial bounds.</li> <li><code>generate_and_scale_polygons()</code>: Creates and scales polygon representations of boundaries for spatial computations.</li> <li><code>compute_transcript_overlap_with_boundaries()</code>: Determines the association of transcripts with boundary polygons.</li> <li><code>build_pyg_data_from_tile()</code>: Converts tile-specific data into <code>HeteroData</code> objects suitable for PyG models.</li> </ul>"},{"location":"user_guide/data_creation/#2-xeniumsample-and-merscopesample-child-classes","title":"2. <code>XeniumSample</code> and <code>MerscopeSample</code> (Child Classes)","text":"<p>These classes inherit from <code>SpatialTranscriptomicsSample</code> and implement dataset-specific processing logic:</p> <ul> <li><code>XeniumSample</code>: Tailored for Xenium datasets, it includes specific filtering rules to exclude unwanted transcripts based on naming patterns (e.g., <code>NegControlProbe_</code>, <code>BLANK_</code>).</li> <li><code>MerscopeSample</code>: Designed for Merscope datasets, allowing for custom filtering and processing logic as needed.</li> </ul>"},{"location":"user_guide/data_creation/#workflow","title":"Workflow","text":"<p>The dataset creation and processing workflow involves several key steps, each ensuring that the spatial transcriptomics data is appropriately prepared for downstream machine learning tasks.</p>"},{"location":"user_guide/data_creation/#step-1-data-loading-and-filtering","title":"Step 1: Data Loading and Filtering","text":"<ul> <li>Transcriptomic Data: Loaded lazily using Dask to handle large datasets efficiently. Custom filtering rules specific to the dataset (Xenium or Merscope) are applied to ensure data quality.</li> <li>Boundary Data: Loaded similarly using Dask, representing spatial structures such as cell or nucleus boundaries.</li> </ul>"},{"location":"user_guide/data_creation/#step-2-tiling","title":"Step 2: Tiling","text":"<ul> <li>Spatial Segmentation: The dataset is divided into smaller, manageable tiles of size \\(x_{\\text{size}} \\times y_{\\text{size}}\\), defined by their top-left corner coordinates \\((x_i, y_j)\\).</li> </ul> \\[ n_x = \\left\\lfloor \\frac{x_{\\text{max}} - x_{\\text{min}}}{d_x} \\right\\rfloor, \\quad n_y = \\left\\lfloor \\frac{y_{\\text{max}} - y_{\\text{min}}}{d_y} \\right\\rfloor \\] <p>Where:   - \\(x_{\\text{min}}, y_{\\text{min}}\\): Minimum spatial coordinates.   - \\(x_{\\text{max}}, y_{\\text{max}}\\): Maximum spatial coordinates.   - \\(d_x, d_y\\): Step sizes along the \\(x\\)- and \\(y\\)-axes, respectively.</p> <ul> <li>Transcript and Boundary Inclusion: For each tile, transcripts and boundaries within the spatial bounds (with optional margins) are included:</li> </ul> \\[  x_i - \\text{margin}_x \\leq x_t &lt; x_i + x_{\\text{size}} + \\text{margin}_x, \\quad y_j - \\text{margin}_y \\leq y_t &lt; y_j + y_{\\text{size}} + \\text{margin}_y  \\] <p>Where:   - \\(x_t, y_t\\): Transcript coordinates.   - \\(\\text{margin}_x, \\text{margin}_y\\): Optional margins to include contextual data.</p>"},{"location":"user_guide/data_creation/#step-3-graph-construction","title":"Step 3: Graph Construction","text":"<p>For each tile, a graph \\(G\\) is constructed with:</p> <ul> <li>Nodes (\\(V\\)):</li> <li>Transcripts: Represented by their spatial coordinates \\((x_t, y_t)\\) and feature vectors \\(\\mathbf{f}_t\\).</li> <li> <p>Boundaries: Represented by centroid coordinates \\((x_b, y_b)\\) and associated properties (e.g., area).</p> </li> <li> <p>Edges (\\(E\\)):</p> </li> <li>Created based on spatial proximity using methods like KD-Tree or FAISS.</li> <li>Defined by a distance threshold \\(d\\) and the number of nearest neighbors \\(k\\):</li> </ul> \\[  E = \\{ (v_i, v_j) \\mid \\text{dist}(v_i, v_j) &lt; d, \\, v_i \\in V, \\, v_j \\in V \\} \\]"},{"location":"user_guide/data_creation/#step-4-label-computation","title":"Step 4: Label Computation","text":"<p>If enabled, edges can be labeled based on relationships, such as whether a transcript belongs to a boundary:</p> \\[ \\text{label}(t, b) =  \\begin{cases} 1 &amp; \\text{if } t \\text{ belongs to } b \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"user_guide/data_creation/#step-5-train-test-validation-splitting","title":"Step 5: Train, Test, Validation Splitting","text":"<p>The dataset is partitioned into training, validation, and test sets based on predefined probabilities \\(p_{\\text{train}}, p_{\\text{val}}, p_{\\text{test}}\\):</p> \\[ p_{\\text{train}} + p_{\\text{val}} + p_{\\text{test}} = 1 \\] <p>Each tile is randomly assigned to one of these sets according to the specified probabilities.</p>"},{"location":"user_guide/data_creation/#output","title":"Output","text":"<p>The final output consists of a set of tiles, each containing a graph representation of the spatial transcriptomics data. These tiles are stored in designated directories (<code>train_tiles</code>, <code>val_tiles</code>, <code>test_tiles</code>) and are ready for integration into machine learning pipelines.</p>"},{"location":"user_guide/data_creation/#example-usage","title":"Example Usage","text":"<p>Below are examples demonstrating how to utilize the <code>segger</code> data preparation module for both Xenium and Merscope datasets.</p>"},{"location":"user_guide/data_creation/#xenium-data","title":"Xenium Data","text":"<pre><code>from segger.data import XeniumSample\nfrom pathlib import Path\nimport scanpy as sc\n\n# Set up the file paths\nraw_data_dir = Path(\"/path/to/xenium_output\")\nprocessed_data_dir = Path(\"path/to/processed_files\")\nsample_tag = \"sample/tag\"\n\n# Load scRNA-seq data using Scanpy and subsample for efficiency\nscRNAseq_path = \"path/to/scRNAseq.h5ad\"\nscRNAseq = sc.read(scRNAseq_path)\nsc.pp.subsample(scRNAseq, fraction=0.1)\n\n# Calculate gene cell type abundance embedding from scRNA-seq data\nfrom segger.utils import calculate_gene_celltype_abundance_embedding\n\ncelltype_column = \"celltype_column\"\ngene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n    scRNAseq, celltype_column\n)\n\n# Create a XeniumSample instance for spatial transcriptomics processing\nxenium_sample = XeniumSample()\n\n# Load transcripts and include the calculated cell type abundance embedding\nxenium_sample.load_transcripts(\n    base_path=raw_data_dir,\n    sample=sample_tag,\n    transcripts_filename=\"transcripts.parquet\",\n    file_format=\"parquet\",\n    additional_embeddings={\"cell_type_abundance\": gene_celltype_abundance_embedding},\n)\n\n# Set the embedding to \"cell_type_abundance\" to use it in further processing\nxenium_sample.set_embedding(\"cell_type_abundance\")\n\n# Load nuclei data to define boundaries\nnuclei_path = raw_data_dir / sample_tag / \"nucleus_boundaries.parquet\"\nxenium_sample.load_boundaries(path=nuclei_path, file_format=\"parquet\")\n\n# Build PyTorch Geometric (PyG) data from a tile of the dataset\ntile_pyg_data = xenium_sample.build_pyg_data_from_tile(\n    boundaries_df=xenium_sample.boundaries_df,\n    transcripts_df=xenium_sample.transcripts_df,\n    r_tx=20,\n    k_tx=20,\n    use_precomputed=False,\n    workers=1,\n)\n\n# Save dataset in processed format for segmentation\nxenium_sample.save_dataset_for_segger(\n    processed_dir=processed_data_dir,\n    x_size=360,\n    y_size=360,\n    d_x=180,\n    d_y=180,\n    margin_x=10,\n    margin_y=10,\n    compute_labels=False,\n    r_tx=5,\n    k_tx=5,\n    val_prob=0.1,\n    test_prob=0.2,\n    neg_sampling_ratio_approx=5,\n    sampling_rate=1,\n    num_workers=1,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#merscope-data","title":"Merscope Data","text":"<pre><code>from segger.data import MerscopeSample\nfrom pathlib import Path\n\n# Set up the file paths\nraw_data_dir = Path(\"path/to/merscope_outputs\")\nprocessed_data_dir = Path(\"path/to/processed_files\")\nsample_tag = \"sample_tag\"\n\n# Create a MerscopeSample instance for spatial transcriptomics processing\nmerscope_sample = MerscopeSample()\n\n# Load transcripts from a CSV file\nmerscope_sample.load_transcripts(\n    base_path=raw_data_dir,\n    sample=sample_tag,\n    transcripts_filename=\"transcripts.csv\",\n    file_format=\"csv\",\n)\n\n# Optionally load cell boundaries\ncell_boundaries_path = raw_data_dir / sample_tag / \"cell_boundaries.parquet\"\nmerscope_sample.load_boundaries(path=cell_boundaries_path, file_format=\"parquet\")\n\n# Filter transcripts based on specific criteria\nfiltered_transcripts = merscope_sample.filter_transcripts(\n    merscope_sample.transcripts_df\n)\n\n# Build PyTorch Geometric (PyG) data from a tile of the dataset\ntile_pyg_data = merscope_sample.build_pyg_data_from_tile(\n    boundaries_df=merscope_sample.boundaries_df,\n    transcripts_df=filtered_transcripts,\n    r_tx=15,\n    k_tx=15,\n    use_precomputed=True,\n    workers=2,\n)\n\n# Save dataset in processed format for segmentation\nmerscope_sample.save_dataset_for_segger(\n    processed_dir=processed_data_dir,\n    x_size=360,\n    y_size=360,\n    d_x=180,\n    d_y=180,\n    margin_x=10,\n    margin_y=10,\n    compute_labels=True,\n    r_tx=5,\n    k_tx=5,\n    val_prob=0.1,\n    test_prob=0.2,\n    neg_sampling_ratio_approx=3,\n    sampling_rate=1,\n    num_workers=2,\n)\n</code></pre>"},{"location":"user_guide/training/","title":"Training the <code>segger</code> Model","text":""},{"location":"user_guide/training/#the-model","title":"The Model","text":"<p>The <code>segger</code> model is a graph neural network designed to handle heterogeneous graphs with two primary node types: transcripts and nuclei or cell boundaries. It leverages attention-based graph convolutional layers to compute node embeddings and relationships in spatial transcriptomics data. The architecture includes an initial embedding layer for node feature transformation, multiple graph attention layers (GATv2Conv), and residual linear connections.</p>"},{"location":"user_guide/training/#model-architecture","title":"Model Architecture","text":"<ul> <li> <p>Input Node Features:    For input node features \\(\\mathbf{x}\\), the model distinguishes between transcript nodes and boundary (or nucleus) nodes.</p> </li> <li> <p>Transcript Nodes: If \\(\\mathbf{x}\\) is 1-dimensional (e.g., for tokenized transcript data), the model applies an embedding layer:</p> </li> </ul> \\[ \\mathbf{h}_{i}^{(0)} = \\text{nn.Embedding}(i) \\] <p>where \\(i\\) is the transcript token index.</p> <ul> <li>Nuclei or Cell Boundary Nodes: If \\(\\mathbf{x}\\) has multiple dimensions, the model applies a linear transformation:</li> </ul> \\[ \\mathbf{h}_{i}^{(0)} = \\mathbf{W} \\mathbf{x}_{i} \\] <p>where \\(\\mathbf{W}\\) is a learnable weight matrix.</p> <ul> <li>Graph Attention Layers (GATv2Conv):    The node embeddings are updated through multiple attention-based layers. The update for a node \\(i\\) at layer \\(l+1\\) is given by:</li> </ul> \\[ \\mathbf{h}_{i}^{(l+1)} = \\text{ReLU}\\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W}^{(l)} \\mathbf{h}_{j}^{(l)} \\right) \\] <p>where:    - \\(\\alpha_{ij}\\) is the attention coefficient between node \\(i\\) and node \\(j\\), computed as:</p> \\[ \\alpha_{ij} = \\frac{\\exp\\left( \\text{LeakyReLU}\\left( \\mathbf{a}^{\\top} \\left[\\mathbf{W}^{(l)} \\mathbf{h}_{i}^{(l)} || \\mathbf{W}^{(l)} \\mathbf{h}_{j}^{(l)}\\right] \\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left( \\text{LeakyReLU}\\left( \\mathbf{a}^{\\top} \\left[\\mathbf{W}^{(l)} \\mathbf{h}_{i}^{(l)} || \\mathbf{W}^{(l)} \\mathbf{h}_{k}^{(l)}\\right] \\right)\\right)} \\] <ul> <li> <p>\\(\\mathbf{a}\\) is a learnable attention vector.</p> </li> <li> <p>Residual Linear Connections:    After each attention layer, a residual connection is added via a linear transformation to stabilize learning:</p> </li> </ul> \\[ \\mathbf{h}_{i}^{(l+1)} = \\text{ReLU}\\left( \\mathbf{h}_{i}^{(l+1)} + \\mathbf{W}_{res} \\mathbf{h}_{i}^{(l)} \\right) \\] <p>where \\(\\mathbf{W}_{res}\\) is a residual weight matrix.</p> <ul> <li>L2 Normalization:    Finally, the embeddings are normalized using L2 normalization:</li> </ul> \\[ \\mathbf{h}_{i} = \\frac{\\mathbf{h}_{i}}{\\|\\mathbf{h}_{i}\\|} \\] <p>ensuring the final node embeddings have unit norm.</p>"},{"location":"user_guide/training/#heterogeneous-graph-transformation","title":"Heterogeneous Graph Transformation","text":"<p>In the next step, the <code>segger</code> model is transformed into a heterogeneous graph neural network using PyTorch Geometric's <code>to_hetero</code> function. This transformation enables the model to handle distinct node and edge types (transcripts and nuclei or cell boundaries) with separate mechanisms for modeling their relationships.</p>"},{"location":"user_guide/training/#usage","title":"Usage","text":"<p>To instantiate and run the <code>segger</code> model:</p> <pre><code>model = segger(\n    num_tx_tokens=5000,  # Number of unique 'tx' tokens\n    init_emb=32,  # Initial embedding dimension\n    hidden_channels=64,  # Number of hidden channels\n    num_mid_layers=2,  # Number of middle layers\n    out_channels=128,  # Number of output channels\n    heads=4,  # Number of attention heads\n)\n\noutput = model(x, edge_index)\n</code></pre> <p>Once transformed to a heterogeneous model and trained using PyTorch Lightning, the model can efficiently learn relationships between transcripts and nuclei or cell boundaries.</p>"},{"location":"user_guide/training/#training-the-heterogeneous-gnn-with-pytorch-lightning","title":"Training the heterogeneous GNN with <code>pytorch-lightning</code>","text":"<p>The training module makes use of PyTorch Lightning for efficient and scalable training, alongside PyTorch Geometric for processing the graph-based data. The module is built to handle multi-GPU setups and allows the flexibility to adjust hyperparameters, aggregation methods, and embedding sizes.</p> <p>The <code>SpatialTranscriptomicsDataset</code> class is used to load and manage spatial transcriptomics data stored in the format of PyTorch Geometric <code>Data</code> objects. It inherits from <code>InMemoryDataset</code> to load preprocessed datasets, ensuring efficient in-memory data handling for training and validation phases.</p>"},{"location":"user_guide/training/#example-training-command","title":"Example Training Command","text":"<pre><code>python scripts/train_model.py \\ \n  --train_dir path/to/train/tiles \\\n  --val_dir path/to/val/tiles \\\n  --batch_size_train 4 \\\n  --batch_size_val 4 \\\n  --num_tx_tokens 500 \\\n  --init_emb 8 \\\n  --hidden_channels 64 \\\n  --out_channels 16 \\\n  --heads 4 \\\n  --mid_layers 1 \\\n  --aggr sum \\\n  --accelerator cuda \\\n  --strategy auto \\\n  --precision 16-mixed \\\n  --devices 4 \\\n  --epochs 100 \\\n  --default_root_dir ./models/clean2\n</code></pre> <p>The <code>scripts/train_model.py</code> file can be found on the github repo. This example submits a job to train the <code>segger</code> model on 4 GPUs with a batch size of 4 for both training and validation, utilizing 16-bit mixed precision.</p>"},{"location":"user_guide/validation/","title":"Validating the segmentations","text":"<p>This module provides utilities for validating segmentation methods in single-cell transcriptomics, focusing on evaluating performance across metrics such as sensitivity, specificity, and spatial localization.</p>"},{"location":"user_guide/validation/#benchmarking-and-validation-of-segmentation-methods","title":"Benchmarking and Validation of Segmentation Methods","text":"<p>To rigorously evaluate segmentation performance, we use a suite of metrics grouped into four categories: General Statistics, Sensitivity, Spatial Localization, and Specificity and Contamination. These metrics provide a comprehensive framework for assessing the accuracy and precision of segmentation methods.</p>"},{"location":"user_guide/validation/#general-statistics","title":"General Statistics","text":"<ul> <li>Percent Assigned Transcripts: Measures the proportion of transcripts correctly assigned to cells.</li> </ul> \\[ \\text{Percent Assigned} = \\frac{N_{\\text{assigned}}}{N_{\\text{total}}} \\times 100 \\] <ul> <li>Transcript Density: Assesses transcript counts relative to cell size.</li> </ul> \\[ D = \\frac{\\text{transcript counts}}{\\text{cell area}} \\]"},{"location":"user_guide/validation/#sensitivity","title":"Sensitivity","text":"<ul> <li>F1 Score for Cell Type Purity: Evaluates how well a segmentation method can identify cells based on marker genes.</li> </ul> \\[ \\text{F1}_{\\text{purity}} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\] <ul> <li>Gene-specific Assignment Metrics: Measures the proportion of correctly assigned transcripts for a specific gene.</li> </ul> \\[ \\text{Percent Assigned}_g = \\frac{N_{\\text{assigned}}^g}{N_{\\text{total}}^g} \\times 100 \\]"},{"location":"user_guide/validation/#spatial-localization","title":"Spatial Localization","text":"<ul> <li>Percent Cytoplasmic and Percent Nucleus Transcripts: Evaluates the spatial distribution of transcripts within cells.</li> </ul> \\[ \\text{Percent Cytoplasmic} = \\frac{N_{\\text{cytoplasmic}}}{N_{\\text{assigned}}} \\times 100 \\] \\[ \\text{Percent Nucleus} = \\frac{N_{\\text{nucleus}}}{N_{\\text{assigned}}} \\times 100 \\] <ul> <li>Neighborhood Entropy: Measures the diversity of neighboring cell types.</li> </ul> \\[ E = -\\sum_{c} p(c) \\log(p(c)) \\]"},{"location":"user_guide/validation/#specificity-and-contamination","title":"Specificity and Contamination","text":"<ul> <li>Mutually Exclusive Co-expression Rate (MECR): Quantifies how mutually exclusive gene expression is across cells.</li> </ul> \\[ \\text{MECR}(g_1, g_2) = \\frac{P(g_1 \\cap g_2)}{P(g_1 \\cup g_2)} \\] <ul> <li>Contamination from Neighboring Cells: Assesses transcript contamination from adjacent cells.</li> </ul> \\[ C_{ij} = \\frac{\\sum_{k \\in \\text{neighbors}} m_{ik} \\cdot w_{kj}}{\\sum_{k \\in \\text{neighbors}} m_{ik}} \\]"},{"location":"user_guide/validation/#comparison-across-segmentation-methods","title":"Comparison Across Segmentation Methods","text":"<p>A correlation analysis is used to compare different segmentation methods based on metrics such as transcript count and cell area. The Comparison Metric is defined as:</p> \\[ \\text{Comparison Metric}(m_1, m_2) = \\frac{\\sum_{i=1}^{n} (M_1(i) - \\bar{M_1}) (M_2(i) - \\bar{M_2})}{\\sqrt{\\sum_{i=1}^{n} (M_1(i) - \\bar{M_1})^2 \\sum_{i=1}^{n} (M_2(i) - \\bar{M_2})^2}} \\]"}]}