{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to segger","text":"<p>segger is a cutting-edge tool for cell segmentation in single-molecule spatial omics datasets. By leveraging graph neural networks (GNNs) and heterogeneous graphs, segger offers unmatched accuracy and scalability.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p> Installation Guide   Get started with installing segger on your machine.</p> </li> <li> <p> User Guide   Learn how to use segger for cell segmentation tasks.</p> </li> <li> <p> Command-Line Interface (CLI)   Explore the CLI options for working with segger.</p> </li> <li> <p> API Reference   Dive into the detailed API documentation for advanced usage.</p> </li> </ul>"},{"location":"#why-segger","title":"Why segger?","text":"<ul> <li> Highly parallelizable  \u2013 Optimized for multi-GPU environments</li> <li> Fast and efficient \u2013 Trains in a fraction of the time compared to alternatives</li> <li> Transfer learning \u2013 Easily adaptable to new datasets and technologies</li> </ul>"},{"location":"#challenges-in-segmentation","title":"Challenges in Segmentation","text":"<p>Spatial omics segmentation faces issues like:</p> <ul> <li>Over/Under-segmentation</li> <li>Transcript contamination</li> <li>Scalability limitations</li> </ul> <p>segger tackles these with a graph-based approach, achieving superior segmentation accuracy.</p>"},{"location":"#how-segger-works","title":"How segger Works","text":""},{"location":"#_1","title":"Home","text":""},{"location":"#powered-by","title":"Powered by","text":"<ul> <li> PyTorch Lightning &amp; PyTorch Geometric: Enables fast, efficient graph neural network (GNN) implementation for heterogeneous graphs.</li> <li> Dask: Scalable parallel processing and distributed task scheduling, ideal for handling large transcriptomic datasets.</li> <li> Shapely &amp; Geopandas: Utilized for spatial operations such as polygon creation, scaling, and spatial relationship computations.</li> <li> RAPIDS: Provides GPU-accelerated computation for tasks like k-nearest neighbors (KNN) graph construction.</li> <li> AnnData &amp; Scanpy: Efficient processing for single-cell datasets.</li> <li> SciPy: Facilitates spatial graph construction, including distance metrics and convex hull calculations for transcript clustering.</li> </ul>"},{"location":"#contributions","title":"Contributions","text":"<p>segger is open-source and welcomes contributions. Join us in advancing spatial omics segmentation!</p> <ul> <li> <p> Source Code GitHub</p> </li> <li> <p> Bug Tracker Report Issues</p> </li> <li> <p> Full Documentation API Reference</p> </li> </ul>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#segger-command-line-interface","title":"Segger Command Line Interface","text":""},{"location":"cli/#1-creating-a-dataset","title":"1. Creating a Dataset","text":"<p>The <code>create_dataset</code> command helps you to build a dataset for spatial transcriptomics. Here\u2019s a breakdown of the options available:</p> // Example: Creating a dataset for spatial transcriptomicspython3 src/segger/cli/create_dataset_fast.py \\    --base_dir /path/to/raw_data \\    --data_dir /path/to/save/processed_data \\    --sample_type xenium \\    --scrnaseq_file /path/to/scrnaseq_file \\    --celltype_column celltype_column_name \\    --k_bd 3 \\    --dist_bd 15.0 \\    --k_tx 3 \\    --dist_tx 5.0 \\    --tile_width 200 \\    --tile_height 200 \\    --neg_sampling_ratio 5.0 \\    --frac 1.0 \\    --val_prob 0.1 \\    --test_prob 0.2 \\    --n_workers 16"},{"location":"cli/#parameters","title":"Parameters","text":"Parameter Description Default Value <code>base_dir</code> Directory containing the raw dataset (e.g., transcripts, boundaries). - <code>data_dir</code> Directory to save the processed Segger dataset (in PyTorch Geometric format). - <code>sample_type</code> The sample type of the raw data, e.g., \"xenium\" or \"merscope\". None <code>scrnaseq_file</code> Path to the scRNAseq file. None <code>celltype_column</code> Column name for cell type annotations in the scRNAseq file. None <code>k_bd</code> Number of nearest neighbors for boundary nodes. <code>3</code> <code>dist_bd</code> Maximum distance for boundary neighbors. <code>15.0</code> <code>k_tx</code> Number of nearest neighbors for transcript nodes. <code>3</code> <code>dist_tx</code> Maximum distance for transcript neighbors. <code>5.0</code> <code>tile_width</code> Width of the tiles in pixels (ignored if <code>tile_size</code> is provided). None <code>tile_height</code> Height of the tiles in pixels (ignored if <code>tile_size</code> is provided). None <code>neg_sampling_ratio</code> Ratio of negative samples. <code>5.0</code> <code>frac</code> Fraction of the dataset to process. Useful for subsampling large datasets. <code>1.0</code> <code>val_prob</code> Proportion of the dataset used for validation split. <code>0.1</code> <code>test_prob</code> Proportion of the dataset used for testing split. <code>0.2</code> <code>n_workers</code> Number of workers for parallel processing. <code>1</code>"},{"location":"cli/#key-updates","title":"Key Updates","text":"<ul> <li>Faster Dataset Creation This method is way faster due to the use of ND-tree-based partitioning and parallel processing.</li> </ul> <p>Customizing Your Dataset</p> <ul> <li>dataset_type: Defines the type of spatial transcriptomics data. Currently, xenium and merscope are supported and have been tested.</li> <li>val_prob, test_prob: Control the dataset portions for validation and testing. Adjust based on your dataset size and evaluation needs.</li> <li>frac: Specifies the fraction of the dataset to process. Reducing <code>frac</code> can be useful when working with very large datasets, allowing for faster dataset creation by only processing a subset of the data.</li> </ul> <p>Faster Dataset Creation</p> <p>Increasing the number of workers (<code>n_workers</code>) can significantly accelerate the dataset creation process, especially for large datasets, by taking advantage of parallel processing across multiple CPU cores.</p> <p>Enhancing Segmentation Accuracy with scRNA-seq</p> <p>Incorporating single cell RNA sequencing (scRNA-seq) data as features can provide additional biological context, improving the accuracy of the segger model.</p>"},{"location":"cli/#2-training-a-model","title":"2. Training a Model","text":"<p>The <code>train</code> command initializes and trains a model using the dataset created. Here are the key parameters:</p> // Example: Training a segger modelpython3 src/segger/cli/train_model.py \\     --dataset_dir /path/to/saved/processed_data \\     --models_dir /path/to/save/model/checkpoints \\     --sample_tag first_training \\     --init_emb 8 \\     --hidden_channels 32 \\     --num_tx_tokens 500 \\     --out_channels 8 \\     --heads 2 \\     --num_mid_layers 2 \\     --batch_size 4 \\     --num_workers 2 \\     --accelerator cuda \\     --max_epochs 200 \\     --devices 4 \\     --strategy auto \\     --precision 16-mixed"},{"location":"cli/#parameters_1","title":"Parameters","text":"Parameter Description Default Value <code>dataset_dir</code> Directory containing the processed Segger dataset (in PyTorch Geometric format). - <code>models_dir</code> Directory to save the trained model and training logs. - <code>sample_tag</code> Tag used to identify the dataset during training. - <code>init_emb</code> Size of the embedding layer for input data. <code>8</code> <code>hidden_channels</code> Number of hidden units in each layer of the neural network. <code>32</code> <code>num_tx_tokens</code> Number of transcript tokens used during training. <code>500</code> <code>out_channels</code> Number of output channels from the model. <code>8</code> <code>heads</code> Number of attention heads used in graph attention layers. <code>2</code> <code>num_mid_layers</code> Number of mid layers in the model. <code>2</code> <code>batch_size</code> Number of samples to process per training batch. <code>4</code> <code>num_workers</code> Number of workers to use for parallel data loading. <code>2</code> <code>accelerator</code> Device used for training (e.g., <code>cuda</code> for GPU or <code>cpu</code>). <code>cuda</code> <code>max_epochs</code> Number of training epochs. <code>200</code> <code>devices</code> Number of devices (GPUs) to use during training. <code>4</code> <code>strategy</code> Strategy used for training (e.g., <code>ddp</code> for distributed training or <code>auto</code>). <code>auto</code> <code>precision</code> Precision used for training (e.g., <code>16-mixed</code> for mixed precision training). <code>16-mixed</code> <p>Optimizing training time</p> <ul> <li>devices: Use multiple GPUs by increasing the <code>devices</code> parameter to further accelerate training.</li> <li>batch_size: A larger batch size can speed up training, but requires more memory. Adjust based on your hardware capabilities.</li> <li>epochs: Increasing the number of epochs can improve model performance by allowing more learning cycles, but it will also extend the overall training time. Balance this based on your time constraints and hardware capacity.</li> </ul> <p>Ensure Correct CUDA and PyTorch Setup</p> <p>Before using the <code>--accelerator cuda</code> flag, ensure your system has CUDA installed and configured correctly. Also, check that the installed CUDA version is compatible with your PyTorch and PyTorch Geometric versions.</p>"},{"location":"cli/#3-making-predictions","title":"3. Making Predictions","text":"<p>After training the model, use the <code>predict</code> command to make predictions on new data:</p> // Example: Make predictions using a trained modelpython3 src/segger/cli/predict_fast.py \\     --segger_data_dir /path/to/saved/processed_data \\     --models_dir /path/to/saved/model/checkpoints \\     --benchmarks_dir /path/to/save/segmentation/results \\     --transcripts_file /path/to/raw_data/transcripts.parquet \\     --batch_size 1 \\     --num_workers 1 \\     --model_version 0 \\     --save_tag segger_embedding_1001 \\     --min_transcripts 5 \\     --cell_id_col segger_cell_id \\     --use_cc false \\     --knn_method cuda \\     --file_format anndata \\     --k_bd 4 \\     --dist_bd 12.0 \\     --k_tx 5 \\     --dist_tx 5.0"},{"location":"cli/#parameters_2","title":"Parameters","text":"Parameter Description Default Value <code>segger_data_dir</code> Directory containing the processed Segger dataset (in PyTorch Geometric format). - <code>models_dir</code> Directory containing the trained models. - <code>benchmarks_dir</code> Directory to save the segmentation results, including cell boundaries and associations. - <code>transcripts_file</code> Path to the transcripts.parquet file. - <code>batch_size</code> Number of samples to process per batch during prediction. <code>1</code> <code>num_workers</code> Number of workers for parallel data loading. <code>1</code> <code>model_version</code> Model version number to load for predictions, corresponding to the version from training logs. <code>0</code> <code>save_tag</code> Tag used to name and organize the segmentation results. <code>segger_embedding_1001</code> <code>min_transcripts</code> Minimum number of transcripts required for segmentation. <code>5</code> <code>cell_id_col</code> Column name for cell IDs in the output data. <code>segger_cell_id</code> <code>use_cc</code> Whether to use connected components for grouping transcripts without direct nucleus association. <code>False</code> <code>knn_method</code> Method for KNN computation (e.g., <code>cuda</code> for GPU-based computation). <code>cuda</code> <code>file_format</code> Format for the output segmentation data (e.g., <code>anndata</code>). <code>anndata</code> <code>k_bd</code> Number of nearest neighbors for boundary nodes. <code>4</code> <code>dist_bd</code> Maximum distance for boundary nodes. <code>12.0</code> <code>k_tx</code> Number of nearest neighbors for transcript nodes. <code>5</code> <code>dist_tx</code> Maximum distance for transcript nodes. <code>5.0</code> <p>Improving Prediction Pipeline</p> <ul> <li>batch_size: A larger batch size can speed up training, but requires more memory. Adjust based on your hardware capabilities.</li> <li>use_cc: Enabling connected component analysis can improve the accuracy of transcript assignments.</li> </ul> <p>Ensure Correct CUDA, cuVS, and PyTorch Setup</p> <p>Before using the <code>knn_method cuda</code> flag, ensure your system has CUDA installed and configured properly. Also, verify that the installed CUDA version is compatible with your cuPy, cuVS, PyTorch, and PyTorch Geometric versions.</p>"},{"location":"cli/#4-running-the-entire-pipeline","title":"4. Running the Entire Pipeline","text":"<p>The <code>submit_job.py</code> script allows you to run the complete Segger pipeline or specific stages like dataset creation, training, or prediction. The pipeline execution is determined by the configuration provided in a YAML file, supporting various environments like Docker, Singularity, and HPC systems (with LSF, Slurm support is planned).</p>"},{"location":"cli/#selecting-pipelines","title":"Selecting Pipelines","text":"<p>You can run the three stages\u2014dataset creation, training, and prediction\u2014sequentially or independently by specifying the pipelines in the YAML configuration file:</p> <pre><code>- `1` for dataset creation\n- `2` for model training\n- `3` for prediction\n</code></pre> <p>This allows you to run the full pipeline or just specific steps. Set the desired stages under the pipelines field in your YAML file.</p>"},{"location":"cli/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Use the following command to run the pipeline:</p> <pre><code>python3 submit_job.py --config_file=filename.yaml\n</code></pre> <ul> <li>If no <code>--config_file</code> is provided, the default <code>config.yaml</code> file will be used.</li> </ul>"},{"location":"cli/#5-containerization","title":"5. Containerization","text":"<p>For users who want a portable, containerized environment, segger supports both Docker and Singularity containers. These containers provide a consistent runtime environment with all dependencies pre-installed.</p>"},{"location":"cli/#using-docker","title":"Using Docker","text":"<p>You can pull the segger Docker image from Docker Hub with this command:</p> <pre><code>docker pull danielunyi42/segger_dev:cuda121\n</code></pre> <p>To run the pipeline in Docker, make sure your YAML configuration includes the following settings:</p> <ul> <li><code>use_singularity</code>: false</li> <li><code>use_lsf</code>: false</li> </ul> <p>Afterwards, run the pipeline inside the Docker container with the same <code>submit_job.py</code> command.</p>"},{"location":"cli/#using-singularity","title":"Using Singularity","text":"<p>For a Singularity environment, pull the image with:</p> <pre><code>singularity pull docker://danielunyi42/segger_dev:cuda121\n</code></pre> <p>Ensure <code>use_singularity: true</code> in the YAML file and specify the Singularity image file (e.g., <code>segger_dev_latest.sif</code>) in the <code>singularity_image</code> field.</p> <p>Containerization</p> <ul> <li>The segger Docker image currently supports CUDA 11.8 and CUDA 12.1.</li> </ul>"},{"location":"cli/#6-hpc-environments","title":"6. HPC Environments","text":"<p>Segger also supports HPC environments with LSF job scheduling. To run the pipeline on an HPC cluster using LSF, set <code>use_lsf: true</code> in your YAML configuration.</p> <p>If your HPC system supports Slurm, a similar setup is planned and will be introduced soon.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#segger-installation-guide","title":"segger Installation Guide","text":"<p>segger provides multiple installation options to suit your requirements. You can install it using:</p> <ul> <li>Virtual environments (recommended for most users)</li> <li>Containerized environments (Docker or Singularity)</li> <li>Editable mode from GitHub (for developers or users who want to modify the source code)</li> </ul> <p>Recommendation</p> <p>To avoid dependency conflicts, we recommend installing segger in a virtual environment or a container environment.</p> <p>segger requires CUDA 11 or CUDA 12 for GPU acceleration.</p>"},{"location":"installation/#installation-in-virtual-environment","title":"Installation in Virtual Environment","text":""},{"location":"installation/#using-venv","title":"Using <code>venv</code>","text":"<pre><code># Step 1: Create and activate the virtual environment.\npython3.10 -m venv segger-venv\nsource segger-venv/bin/activate\n\n# Step 2: Install segger with CUDA support.\npip install --upgrade pip\npip install .[cuda12]\n\n# Step 3: Verify the installation.\npython --version\npip show segger\n\n# step 4 [Optional]: If your system doesn't have a universally installed CUDA toolkit, you can link CuPy to PyTorch's CUDA runtime library.\nexport LD_LIBRARY_PATH=$(pwd)/segger-venv/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"installation/#using-conda","title":"Using <code>conda</code>","text":"<pre><code># Step 1: Create and activate the conda environment.\nconda create -n segger-env python=3.10\nconda activate segger-env\n\n# Step 2: Install segger with CUDA support.\npip install --upgrade pip\npip install .[cuda12]\n\n# Step 3: Verify the installation.\npython --version\npip show segger\n\n# Step 4 [Optional]: If your system doesn't have a universally installed CUDA toolkit, you can link CuPy to PyTorch's CUDA runtime library.\nexport LD_LIBRARY_PATH=$(conda info --base)/envs/segger-env/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"installation/#how-to-choose-between-cuda11-and-cuda12","title":"How to Choose Between <code>[cuda11]</code> and <code>[cuda12]</code>","text":"<ol> <li>Check Your NVIDIA Driver Version: Run <code>nvidia-smi</code>. Use <code>[cuda11]</code> for driver version \u2265 450.80.02 or <code>[cuda12]</code> for version \u2265 525.60.13.</li> <li>Check for a CUDA Toolkit: Run <code>nvcc --version</code>. If it outputs a CUDA version (11.x or 12.x), choose the corresponding <code>[cuda11]</code> or <code>[cuda12]</code>.</li> <li>Default to PyTorch CUDA Runtime: If CUDA toolkit is not installed, segger can use PyTorch's bundled CUDA runtime. You can link CuPy as shown in Step 4 of the venv/conda installation.</li> </ol>"},{"location":"installation/#installation-in-container-environment","title":"Installation in Container Environment","text":""},{"location":"installation/#using-docker","title":"Using <code>docker</code>","text":"<pre><code># Step 1: Pull the official Docker image.\ndocker pull danielunyi42/segger_dev:cuda121\n\n# Step 2: Run the Docker container with GPU support.\ndocker run --gpus all -it danielunyi42/segger_dev:cuda121\n</code></pre> <p>The official Docker image comes with all dependencies pre-installed, including the CUDA toolkit, PyTorch, and CuPy. The current images support CUDA 11.8 and CUDA 12.1, which can be specified in the image tag.</p>"},{"location":"installation/#using-singularity","title":"Using <code>singularity</code>","text":"<pre><code># Step 1: Pull the official Docker image.\nsingularity pull docker://danielunyi42/segger_dev:cuda121\n\n# Step 2: Run the Singularity container with GPU support.\nsingularity exec --nv segger_dev_cuda121.sif\n</code></pre> <p>The Singularity image is derived from the official Docker image and includes all pre-installed dependencies.</p>"},{"location":"installation/#directory-mapping-for-input-and-output-data","title":"Directory Mapping for Input and Output Data","text":"<p>Directory mapping allows:</p> <ul> <li>Access to input data (spatial transcriptomics datasets) from your local machine inside the container.</li> <li>Saving output data (segmentation results and logs) generated by segger to your local machine.</li> </ul> <p>Setting up directory mapping is really easy:</p> <ul> <li> <p>For Docker: <pre><code>docker run --gpus all -it -v /path/to/local/data:/workspace/data danielunyi42/segger_dev:cuda121\n</code></pre></p> </li> <li> <p>For Singularity: <pre><code>singularity exec --nv -B /path/to/local/data:/workspace/data segger_dev_cuda121.sif\n</code></pre></p> </li> <li>Place your input datasets in <code>/path/to/local/data</code> on your host machine.</li> <li>Inside the container, access these datasets from <code>/workspace/data</code>.</li> <li>Save results to <code>/workspace/data</code>, which will be available in <code>/path/to/local/data</code> on the host machine.</li> </ul>"},{"location":"installation/#editable-github-installation","title":"Editable GitHub installation","text":"<p>For developers or users who want to modify the source code:</p> <pre><code>git clone https://github.com/EliHei2/segger_dev.git\ncd segger_dev\npip install -e \".[cuda12]\"\n</code></pre> <p>Common Installation Issues</p> <ul> <li> <p>Python Version: Ensure you are using Python &gt;= 3.10. Check your Python version by running:   <pre><code>python --version\n</code></pre>   If your version is lower than 3.10, please upgrade Python.</p> </li> <li> <p>CUDA Compatibility (GPU): For GPU installations, verify that your system has the correct NVIDIA drivers installed. Run:   <pre><code>nvidia-smi\n</code></pre>   Ensure that the displayed CUDA version is compatible with your selected <code>[cuda11]</code> or <code>[cuda12]</code> extra.</p> <ul> <li>Minimum driver version for CUDA 11.x: <code>450.80.02</code></li> <li>Minimum driver version for CUDA 12.x: <code>525.60.13</code></li> </ul> </li> <li> <p>Permissions: If you encounter permission errors during installation, use the --user flag to install the package without requiring administrative privileges:   <pre><code>pip install --user .[cuda12]\n</code></pre>   Alternatively, consider using a virtual environment (venv or conda) to isolate the installation.</p> </li> <li> <p>Environment Configuration: Ensure that all required dependencies are installed in your environment.</p> </li> </ul>"},{"location":"_build/html/_sources/cli/","title":"Command-Line Interface (CLI)","text":"<p>Documentation for the Segger CLI.</p> <p>```python import click</p> <p>@click.group() def cli():     pass</p> <p>@cli.command() def create_dataset():     \"\"\"Create a new dataset.\"\"\"     pass</p> <p>if name == 'main':     cli()</p>"},{"location":"_build/html/_sources/installation/","title":"Installation Guide","text":"<p>This guide provides detailed instructions for installing the <code>segger</code> package. Whether you are installing for CPU or GPU, the instructions below will guide you through the process.</p> <pre><code>Ensure you are using `Python &gt;= 3.10` before starting the installation process.\n</code></pre>"},{"location":"_build/html/_sources/installation/#install-the-package","title":"Install the Package","text":""},{"location":"_build/html/_sources/installation/#from-source","title":"From Source","text":"<p>To install <code>segger</code> from the source code:</p> <ol> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/EliHei2/segger_dev.git\n</code></pre></p> </li> <li> <p>Navigate to the project directory:     <pre><code>cd segger_dev\n</code></pre></p> </li> <li> <p>Install the package:     <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"_build/html/_sources/installation/#cpu-and-gpu-installation-from-pypi","title":"CPU and GPU Installation from PyPI","text":"<p><pre><code>```{tab-item} CPU Installation\nIf you only need CPU support, use the following command:\n\n```bash\npip install segger\n</code></pre> This will install the package without any GPU-related dependencies.</p> <pre><code>This is ideal for environments where GPU support is not required or available.\n</code></pre> <p>```{tab-item} GPU Installation For installations with GPU support, use the following command:</p> <pre><code>pip install segger[gpu]\n</code></pre> <p>This includes the necessary dependencies for CUDA-enabled GPUs.</p> <pre><code>Ensure your machine has the appropriate CUDA drivers and NVIDIA libraries installed.\n</code></pre> <pre><code>\n</code></pre> <pre><code>## Optional Dependencies\n\nThe following sections describe optional dependencies you can install for specific features.\n\n```{tab-set}\n```{tab-item} Torch Geometric\n\nTo install `torch-geometric` related dependencies, run:\n\n```bash\npip install segger[torch-geometric]\n</code></pre> <p>Follow the additional steps on the PyTorch Geometric installation page to ensure proper setup.</p> <pre><code>Ensure you install `torch-geometric` with the correct CUDA version for GPU support.\n</code></pre> <p>```{tab-item} Multiprocessing</p> <p>To install <code>segger</code> with multiprocessing support, use the following command:</p> <pre><code>pip install segger[multiprocessing]\n</code></pre> <p>This will enable multi-core parallel processing features.</p> <pre><code>## Platform-Specific Installations\n\nBelow are instructions for installing `segger` on different operating systems.\n\n```{tab-set}\n```{tab-item} Linux\n\nOn Linux, use the following command to install the package:\n\n```bash\npip install segger\n</code></pre> <p>For GPU support on Linux, ensure you have the necessary CUDA drivers installed:</p> <pre><code>pip install segger[gpu]\n</code></pre> <p>```{tab-item} macOS</p> <p>To install on macOS, use the following command:</p> <pre><code>pip install segger\n</code></pre> <p>Note that macOS does not natively support CUDA, so GPU support is not available.</p> <pre><code>If you require GPU support, we recommend using Linux or Windows.\n</code></pre> <p>```{tab-item} Windows</p> <p>To install on Windows, use the following command:</p> <pre><code>pip install segger\n</code></pre> <p>For GPU support on Windows:</p> <pre><code>pip install segger[gpu]\n</code></pre> <pre><code>Ensure your CUDA drivers are installed on Windows by using `nvidia-smi`.\n</code></pre> <pre><code>## Installation for Developers\n\nFor developers looking to contribute or work with `segger` in a development environment, you can install the package with development dependencies.\n\n1. Clone the repository:\n    ```bash\n    git clone https://github.com/EliHei2/segger_dev.git\n    ```\n\n2. Navigate to the project directory:\n    ```bash\n    cd segger_dev\n    ```\n\n3. Install the package with development dependencies:\n    ```bash\n    pip install -e .[dev]\n    ```\n\nThis will install additional dependencies such as `pytest`, `black`, `flake8`, and more.\n\n## Common Installation Issues\n\n```{tip}\nIf you encounter installation issues, ensure that you are using the correct Python version (`&gt;= 3.10`) and that you have the necessary permissions to install packages on your system.\n</code></pre> <p>Some common errors include:</p> <ul> <li>Missing Python version: Ensure you are using <code>Python &gt;= 3.10</code>.</li> <li>Insufficient permissions: Use <code>pip install --user</code> if you do not have admin permissions.</li> <li>Conflicting CUDA drivers: Ensure you have compatible CUDA versions installed if using GPU support.</li> </ul> <p>For further troubleshooting, please refer to the official documentation.</p> <p>For more information, visit the official GitHub repository.</p> <p>```</p>"},{"location":"api/","title":"API Reference","text":"<p>This page contains auto-generated API reference documentation [^1].</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>CLI</li> <li>Data</li> <li>Validation</li> <li>Predict</li> <li>Train Model</li> </ul>"},{"location":"api/data/","title":"segger.data","text":"<p>The <code>segger.data</code> module in Segger is a comprehensive data processing and management system designed specifically for spatial transcriptomics datasets. It provides a unified, scalable interface for handling large-scale spatial transcriptomics data from technologies such as Xenium and Merscope, with a focus on preparing data for graph-based deep learning models.</p> <ul> <li>Sample: Core sample handling, tiling, and data management classes</li> <li>PyG Dataset: PyTorch Geometric dataset integration and utilities</li> <li>Utils: Core utility functions for data processing, filtering, and analysis</li> <li>Transcript Embedding: Transcript feature encoding and embedding utilities</li> <li>NDTree: Spatial partitioning and load balancing utilities</li> </ul>"},{"location":"api/data/#module-overview","title":"Module Overview","text":"<p>The <code>segger.data</code> package is organized into several key modules, each serving a specific purpose in the spatial transcriptomics data processing pipeline.</p>"},{"location":"api/data/#sample-module","title":"Sample Module","text":"<p>The main data processing module containing the core classes for handling spatial transcriptomics data.</p> <p>Main Functions:</p> <ul> <li>Data loading and validation</li> <li>Spatial tiling and partitioning</li> <li>Graph construction and feature engineering</li> <li>Parallel processing coordination</li> </ul> <p>Key Classes:</p> <ul> <li><code>STSampleParquet</code>: Main orchestrator for data loading and processing</li> <li><code>STInMemoryDataset</code>: In-memory dataset with spatial indexing</li> <li><code>STTile</code>: Individual spatial tile processing</li> </ul>"},{"location":"api/data/#utils-module","title":"Utils Module","text":"<p>Core utility functions for data processing, filtering, and analysis.</p> <p>Key Functions:</p> <ul> <li><code>get_xy_extents()</code>: Extract spatial extents from parquet files</li> <li><code>read_parquet_region()</code>: Read specific spatial regions from parquet files</li> <li><code>filter_transcripts()</code>: Filter transcripts by quality and gene type</li> <li><code>filter_boundaries()</code>: Filter boundaries by spatial criteria</li> <li><code>load_settings()</code>: Load technology-specific configurations</li> <li><code>find_markers()</code>: Identify marker genes for cell types</li> </ul>"},{"location":"api/data/#pyg-dataset-module","title":"PyG Dataset Module","text":"<p>PyTorch Geometric dataset integration for machine learning workflows.</p> <ul> <li>Automatic tile discovery and loading</li> <li>PyTorch Lightning integration</li> <li>Built-in data validation</li> <li>Efficient memory management</li> </ul> <p>Key Classes:</p> <ul> <li><code>STPyGDataset</code>: PyTorch Geometric dataset wrapper</li> </ul>"},{"location":"api/data/#transcript-embedding-module","title":"Transcript Embedding Module","text":"<p>Utilities for encoding transcript features into numerical representations.</p>"},{"location":"api/data/#ndtree-module","title":"NDTree Module","text":"<p>Spatial partitioning and load balancing utilities.</p> <p>Key Classes:</p> <ul> <li><code>NDTree</code>: N-dimensional spatial partitioning</li> <li><code>innernode</code>: Internal tree node structure</li> </ul> <p>Features:</p> <ul> <li>Efficient spatial data partitioning</li> <li>Load balancing for parallel processing</li> <li>Memory-optimized data structures</li> <li>Configurable region sizing</li> </ul>"},{"location":"api/data/#configuration-and-settings","title":"Configuration and Settings","text":""},{"location":"api/data/#settings-directory","title":"Settings Directory","text":"<p>Technology-specific configuration files for different spatial transcriptomics platforms.</p> <p>Available Platforms:</p> <ul> <li>Xenium</li> <li>Merscope</li> <li>CosMx</li> <li>Xenium v2 (segmentation kit)</li> </ul> <p>Configuration Options:</p> <ul> <li>Column mappings</li> <li>Quality thresholds</li> <li>Spatial parameters</li> <li>Platform-specific defaults</li> </ul>"},{"location":"api/data/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>Raw Data (Parquet) \u2192 STSampleParquet \u2192 Spatial Tiling \u2192 STInMemoryDataset \u2192 STTile \u2192 PyG Graph\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nMetadata Extraction    Region Division   Data Filtering   Tile Creation   Graph Construction\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nSettings Loading      Load Balancing    Spatial Indexing   Feature Comp.   Edge Generation\n</code></pre>"},{"location":"api/data/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>STSampleParquet (Main Orchestrator)\n\u251c\u2500\u2500 STInMemoryDataset (Region Processing)\n\u2502   \u2514\u2500\u2500 STTile (Individual Tile Processing)\n\u251c\u2500\u2500 TranscriptEmbedding (Feature Encoding)\n\u2514\u2500\u2500 NDTree (Spatial Partitioning)\n\nSTPyGDataset (ML Integration)\n\u2514\u2500\u2500 PyTorch Geometric Integration\n\nBackendHandler (Experimental)\n\u2514\u2500\u2500 Multi-backend Support\n</code></pre>"},{"location":"api/data/#usage","title":"Usage","text":"<pre><code>from segger.data.sample import STSampleParquet\n\n# Load and process data\nsample = STSampleParquet(base_dir=\"path/to/data\", n_workers=4)\nsample.save(data_dir=\"./processed\", tile_size=1000)\n</code></pre>"},{"location":"api/data/_utils/","title":"segger.data._utils","text":"<p>The <code>_utils</code> module provides core utility functions for data processing, filtering, and management in the Segger framework. This module contains essential functions for handling spatial transcriptomics data, including coordinate processing, data filtering, and settings management.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    transform: Callable = None,\n    pre_transform: Callable = None,\n    pre_filter: Callable = None,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(\n        os.path.join(self.processed_dir, self.processed_file_names[idx])\n    )\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.add_transcript_ids","title":"add_transcript_ids","text":"<pre><code>add_transcript_ids(transcripts_df, x_col, y_col, id_col='transcript_id', precision=1000)\n</code></pre> <p>Add unique transcript IDs to a DataFrame based on x,y coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>DataFrame containing transcript data with x,y coordinates.</p> required <code>x_col</code> <code>str</code> <p>Name of the x-coordinate column.</p> required <code>y_col</code> <code>str</code> <p>Name of the y-coordinate column.</p> required <code>id_col</code> <code>str</code> <p>Name of the column to store the transcript IDs. Defaults to \"transcript_id\".</p> <code>'transcript_id'</code> <code>precision</code> <code>int</code> <p>Precision multiplier for coordinate values to handle floating point precision. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with added transcript_id column.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def add_transcript_ids(\n    transcripts_df: pd.DataFrame,\n    x_col: str,\n    y_col: str,\n    id_col: str = \"transcript_id\",\n    precision: int = 1000,\n) -&gt; pd.DataFrame:\n    \"\"\"Add unique transcript IDs to a DataFrame based on x,y coordinates.\n\n    Args:\n        transcripts_df: DataFrame containing transcript data with x,y coordinates.\n        x_col: Name of the x-coordinate column.\n        y_col: Name of the y-coordinate column.\n        id_col: Name of the column to store the transcript IDs. Defaults to \"transcript_id\".\n        precision: Precision multiplier for coordinate values to handle floating point precision.\n            Defaults to 1000.\n\n    Returns:\n        pd.DataFrame: DataFrame with added transcript_id column.\n    \"\"\"\n    # Create coordinate strings with specified precision\n    x_coords = np.round(transcripts_df[x_col] * precision).astype(int).astype(str)\n    y_coords = np.round(transcripts_df[y_col] * precision).astype(int).astype(str)\n    coords_str = x_coords + \"_\" + y_coords\n\n    # Generate unique IDs using a deterministic hash function\n    def hash_coords(s):\n        \"\"\"Generate a deterministic hash for coordinate strings.\n\n        Args:\n            s: Coordinate string to hash.\n\n        Returns:\n            int: 8-digit integer hash value.\n        \"\"\"\n        # Use a fixed seed for reproducibility\n        seed = 1996\n        # Combine string with seed and take modulo to get an 8-digit integer\n        return abs(hash(s + str(seed))) % 100000000\n\n    tx_ids = np.array([hash_coords(s) for s in coords_str], dtype=np.int32)\n\n    # Add IDs to DataFrame\n    transcripts_df = transcripts_df.copy()\n    transcripts_df[id_col] = tx_ids\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(\n    adata: ad.AnnData, celltype_column: str\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(\n        gene_expression_binary, index=adata.obs_names, columns=adata.var_names\n    )\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.compute_nuclear_transcripts","title":"compute_nuclear_transcripts","text":"<pre><code>compute_nuclear_transcripts(polygons, transcripts, x_col, y_col, nuclear_column=None, nuclear_value=None)\n</code></pre> <p>Compute which transcripts are nuclear based on their coordinates and the nuclear polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>GeoSeries</code> <p>The nuclear polygons.</p> required <code>transcripts</code> <code>DataFrame</code> <p>The transcripts DataFrame.</p> required <code>x_col</code> <code>str</code> <p>The x-coordinate column name.</p> required <code>y_col</code> <code>str</code> <p>The y-coordinate column name.</p> required <code>nuclear_column</code> <code>str</code> <p>The column name that indicates if a transcript is nuclear. Defaults to None.</p> <code>None</code> <code>nuclear_value</code> <code>str</code> <p>The value in nuclear_column that indicates a nuclear transcript. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A boolean series indicating which transcripts are nuclear.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def compute_nuclear_transcripts(\n    polygons: gpd.GeoSeries,\n    transcripts: pd.DataFrame,\n    x_col: str,\n    y_col: str,\n    nuclear_column: str = None,\n    nuclear_value: str = None,\n) -&gt; pd.Series:\n    \"\"\"Compute which transcripts are nuclear based on their coordinates and the\n    nuclear polygons.\n\n    Args:\n        polygons: The nuclear polygons.\n        transcripts: The transcripts DataFrame.\n        x_col: The x-coordinate column name.\n        y_col: The y-coordinate column name.\n        nuclear_column: The column name that indicates if a transcript is nuclear. Defaults to None.\n        nuclear_value: The value in nuclear_column that indicates a nuclear transcript. Defaults to None.\n\n    Returns:\n        pd.Series: A boolean series indicating which transcripts are nuclear.\n    \"\"\"\n    # If nuclear_column and nuclear_value are provided, use them\n    if nuclear_column is not None and nuclear_value is not None:\n        if nuclear_column in transcripts.columns:\n            return transcripts[nuclear_column].eq(nuclear_value)\n\n    # Otherwise compute based on coordinates\n    points = gpd.GeoSeries(gpd.points_from_xy(transcripts[x_col], transcripts[y_col]))\n    return points.apply(lambda p: any(p.within(poly) for poly in polygons))\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[\n        assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_cytoplasmic = (\n        len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    )\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[\n        non_assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_non_assigned_cytoplasmic = (\n        len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    )\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (\n        gene_group_assigned.size() / (gene_group_all.size() + 1) * 100\n    ).reset_index(names=\"percent_assigned\")\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (\n        cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100\n    ).reset_index(name=\"percent_cytoplasmic\")\n    gene_metrics = pd.merge(\n        gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\"\n    ).fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(\n        columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"}\n    )[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(\n            cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\"\n        )\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = (\n        df[df[cell_id_col].astype(str) == \"UNASSIGNED\"]\n        .groupby(\"feature_name\")[\"feature_name\"]\n        .count()\n    )\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = (\n        var_df.index.map(unassigned_counts).fillna(0).astype(int)\n    )\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(\n        adata.obs,\n        cell_summary.loc[adata.obs_names, :],\n        left_index=True,\n        right_index=True,\n    )\n\n    return adata\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.ensure_transcript_ids","title":"ensure_transcript_ids","text":"<pre><code>ensure_transcript_ids(parquet_path, x_col, y_col, id_col='transcript_id', precision=1000)\n</code></pre> <p>Ensure that a parquet file has transcript IDs by adding them if missing.</p> <p>Parameters:</p> Name Type Description Default <code>parquet_path</code> <code>PathLike</code> <p>Path to the parquet file.</p> required <code>x_col</code> <code>str</code> <p>Name of the x-coordinate column.</p> required <code>y_col</code> <code>str</code> <p>Name of the y-coordinate column.</p> required <code>id_col</code> <code>str</code> <p>Name of the column to store the transcript IDs. Defaults to \"transcript_id\".</p> <code>'transcript_id'</code> <code>precision</code> <code>int</code> <p>Precision multiplier for coordinate values to handle floating point precision. Defaults to 1000.</p> <code>1000</code> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def ensure_transcript_ids(\n    parquet_path: os.PathLike,\n    x_col: str,\n    y_col: str,\n    id_col: str = \"transcript_id\",\n    precision: int = 1000,\n) -&gt; None:\n    \"\"\"Ensure that a parquet file has transcript IDs by adding them if missing.\n\n    Args:\n        parquet_path: Path to the parquet file.\n        x_col: Name of the x-coordinate column.\n        y_col: Name of the y-coordinate column.\n        id_col: Name of the column to store the transcript IDs. Defaults to \"transcript_id\".\n        precision: Precision multiplier for coordinate values to handle floating point precision.\n            Defaults to 1000.\n    \"\"\"\n    # First check metadata to see if column exists\n    metadata = pq.read_metadata(parquet_path)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Only proceed if the column doesn't exist\n    if id_col not in schema_idx:\n        # Read the parquet file\n        df = pd.read_parquet(parquet_path)\n\n        # Add transcript IDs\n        df = add_transcript_ids(df, x_col, y_col, id_col, precision)\n\n        # Convert DataFrame to Arrow table\n        table = pa.Table.from_pandas(df)\n\n        # Write back to parquet\n        pq.write_table(\n            table,\n            parquet_path,\n            version=\"2.6\",  # Use latest stable version\n            write_statistics=True,  # Ensure statistics are written\n            compression=\"snappy\",  # Use snappy compression for better performance\n        )\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.filter_boundaries","title":"filter_boundaries","text":"<pre><code>filter_boundaries(boundaries, inset, outset, x, y, label)\n</code></pre> <p>Filter boundary polygons based on their overlap with specified inset and outset regions.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries</code> <code>DataFrame</code> <p>A DataFrame containing the boundary data with x and y coordinates and identifiers.</p> required <code>inset</code> <code>Polygon</code> <p>A polygon representing the inner region to filter the boundaries.</p> required <code>outset</code> <code>Polygon</code> <p>A polygon representing the outer region to filter the boundaries.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <code>label</code> <code>str</code> <p>The name of the column representing the cell or nucleus label.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the filtered boundary polygons.</p> Note <p>The function determines overlaps of boundary polygons with the specified inset and outset regions. It creates boolean masks for overlaps with the top, left, right, and bottom sides of the outset region, as well as the center region defined by the inset polygon. The filtering logic includes polygons that: - Are completely within the center region. - Overlap with the center and the left side, but not the bottom side. - Overlap with the center and the top side, but not the right side.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def filter_boundaries(\n    boundaries: pd.DataFrame,\n    inset: shapely.Polygon,\n    outset: shapely.Polygon,\n    x: str,\n    y: str,\n    label: str,\n):\n    \"\"\"Filter boundary polygons based on their overlap with specified inset and\n    outset regions.\n\n    Args:\n        boundaries: A DataFrame containing the boundary data with x and y coordinates and\n            identifiers.\n        inset: A polygon representing the inner region to filter the boundaries.\n        outset: A polygon representing the outer region to filter the boundaries.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n        label: The name of the column representing the cell or nucleus label.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered boundary polygons.\n\n    Note:\n        The function determines overlaps of boundary polygons with the specified\n        inset and outset regions. It creates boolean masks for overlaps with the\n        top, left, right, and bottom sides of the outset region, as well as the\n        center region defined by the inset polygon. The filtering logic includes\n        polygons that:\n        - Are completely within the center region.\n        - Overlap with the center and the left side, but not the bottom side.\n        - Overlap with the center and the top side, but not the right side.\n    \"\"\"\n\n    # Determine overlaps of boundary polygons\n    def in_region(region):\n        \"\"\"Check if boundaries are within a specified region.\n\n        Args:\n            region: Shapely polygon defining the region to check.\n\n        Returns:\n            pd.Series: Boolean mask indicating which boundaries are in the region.\n        \"\"\"\n        in_x = boundaries[x].between(region.bounds[0], region.bounds[2])\n        in_y = boundaries[y].between(region.bounds[1], region.bounds[3])\n        return in_x &amp; in_y\n\n    x1, y1, x4, y4 = outset.bounds\n    x2, y2, x3, y3 = inset.bounds\n    boundaries[\"top\"] = in_region(shapely.box(x1, y1, x4, y2))\n    boundaries[\"left\"] = in_region(shapely.box(x1, y1, x2, y4))\n    boundaries[\"right\"] = in_region(shapely.box(x3, y1, x4, y4))\n    boundaries[\"bottom\"] = in_region(shapely.box(x1, y3, x4, y4))\n    boundaries[\"center\"] = in_region(inset)\n\n    # Filter boundary polygons\n    # Include overlaps with top and left, not bottom and right\n    gb = boundaries.groupby(label, sort=False)\n    total = gb[\"center\"].transform(\"size\")\n    in_top = gb[\"top\"].transform(\"sum\")\n    in_left = gb[\"left\"].transform(\"sum\")\n    in_right = gb[\"right\"].transform(\"sum\")\n    in_bottom = gb[\"bottom\"].transform(\"sum\")\n    in_center = gb[\"center\"].transform(\"sum\")\n    keep = in_center == total\n    keep |= (in_center &gt; 0) &amp; (in_left &gt; 0) &amp; (in_bottom == 0)\n    keep |= (in_center &gt; 0) &amp; (in_top &gt; 0) &amp; (in_right == 0)\n    inset_boundaries = boundaries.loc[keep]\n    return inset_boundaries\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, label=None, filter_substrings=None, qv_column=None, min_qv=None)\n</code></pre> <p>Filter transcripts based on quality value and remove unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>label</code> <code>Optional[str]</code> <p>The label of transcript features. Defaults to None.</p> <code>None</code> <code>filter_substrings</code> <code>Optional[List[str]]</code> <p>The list of feature substrings to remove. Defaults to None.</p> <code>None</code> <code>qv_column</code> <code>Optional[str]</code> <p>The name of the column representing the quality value. Defaults to None.</p> <code>None</code> <code>min_qv</code> <code>Optional[float]</code> <p>The minimum quality value threshold for filtering transcripts. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def filter_transcripts(\n    transcripts_df: pd.DataFrame,\n    label: Optional[str] = None,\n    filter_substrings: Optional[List[str]] = None,\n    qv_column: Optional[str] = None,\n    min_qv: Optional[float] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter transcripts based on quality value and remove unwanted transcripts.\n\n    Args:\n        transcripts_df: The dataframe containing transcript data.\n        label: The label of transcript features. Defaults to None.\n        filter_substrings: The list of feature substrings to remove. Defaults to None.\n        qv_column: The name of the column representing the quality value. Defaults to None.\n        min_qv: The minimum quality value threshold for filtering transcripts. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    mask = pd.Series(True, index=transcripts_df.index)\n    if filter_substrings is not None and label is not None:\n        mask &amp;= ~transcripts_df[label].str.startswith(tuple(filter_substrings))\n    if min_qv is not None and qv_column is not None:\n        mask &amp;= transcripts_df[qv_column].ge(min_qv)\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.find_markers","title":"find_markers","text":"<pre><code>find_markers(adata, cell_type_column, pos_percentile=5, neg_percentile=10, percentage=50)\n</code></pre> <p>Identify positive and negative marker genes for each cell type in an AnnData object. Positive markers are top-ranked genes that are expressed in at least <code>percentage</code> percent of cells in the given cell type. Args:     adata: Annotated data object containing gene expression data and cell type annotations.     cell_type_column: Name of the column in <code>adata.obs</code> specifying cell type identity for each cell.     pos_percentile: Percentile threshold for selecting top highly expressed genes as positive markers. Defaults to 5.     neg_percentile: Percentile threshold for selecting lowest expressed genes as negative markers. Defaults to 10.     percentage: Minimum percent of cells (0-100) in a cell type expressing a gene for it to be a marker. Defaults to 50. Returns:     dict: Dictionary mapping cell type names to:         {             'positive': [list of positive marker gene names],             'negative': [list of negative marker gene names]         }</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def find_markers(\n    adata: ad.AnnData,\n    cell_type_column: str,\n    pos_percentile: float = 5,\n    neg_percentile: float = 10,\n    percentage: float = 50,\n) -&gt; Dict[str, Dict[str, List[str]]]:\n    \"\"\"Identify positive and negative marker genes for each cell type in an AnnData object.\n    Positive markers are top-ranked genes that are expressed in at least\n    `percentage` percent of cells in the given cell type.\n    Args:\n        adata: Annotated data object containing gene expression data and cell type annotations.\n        cell_type_column: Name of the column in `adata.obs` specifying cell type identity for each cell.\n        pos_percentile: Percentile threshold for selecting top highly expressed genes as positive markers. Defaults to 5.\n        neg_percentile: Percentile threshold for selecting lowest expressed genes as negative markers. Defaults to 10.\n        percentage: Minimum percent of cells (0-100) in a cell type expressing a gene for it to be a marker. Defaults to 50.\n    Returns:\n        dict: Dictionary mapping cell type names to:\n            {\n                'positive': [list of positive marker gene names],\n                'negative': [list of negative marker gene names]\n            }\n    \"\"\"\n    markers = {}\n    sc.tl.rank_genes_groups(adata, groupby=cell_type_column)\n    genes = np.array(adata.var_names)\n    n_genes = adata.shape[1]\n    # Work with a dense matrix for expression fraction calculation\n    # (convert sparse to dense if needed)\n    if not isinstance(adata.X, np.ndarray):\n        expr_matrix = adata.X.toarray()\n    else:\n        expr_matrix = adata.X\n    for cell_type in adata.obs[cell_type_column].unique():\n        mask = (adata.obs[cell_type_column] == cell_type).values\n        gene_names = np.array(adata.uns['rank_genes_groups']['names'][cell_type])\n        n_pos = max(1, int(n_genes * pos_percentile // 100))\n        n_neg = max(1, int(n_genes * neg_percentile // 100))\n        # Calculate percent of cells in this cell type expressing each gene\n        expr_frac = (expr_matrix[mask] &gt; 0).mean(axis=0) * 100  # as percent\n        # Filter positive markers by expression fraction\n        pos_indices = []\n        for idx in range(n_pos):\n            gene = gene_names[idx]\n            gene_idx = np.where(genes == gene)[0][0]\n            if expr_frac[gene_idx] &gt;= percentage:\n                pos_indices.append(idx)\n        positive_markers = list(gene_names[pos_indices])\n        # Negative markers are the lowest-ranked\n        negative_markers = list(gene_names[-n_neg:])\n        markers[cell_type] = {\n            \"positive\": positive_markers,\n            \"negative\": negative_markers\n        }\n    return markers\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.find_mutually_exclusive_genes","title":"find_mutually_exclusive_genes","text":"<pre><code>find_mutually_exclusive_genes(adata, markers, cell_type_column)\n</code></pre> <p>Identify mutually exclusive genes based on expression criteria.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>Annotated data object containing gene expression data.</p> required <code>markers</code> <code>Dict[str, Dict[str, List[str]]]</code> <p>Dictionary where keys are cell types and values are dictionaries containing: 'positive': list of top x% highly expressed genes 'negative': list of top x% lowly expressed genes.</p> required <code>cell_type_column</code> <code>str</code> <p>Column name in <code>adata.obs</code> that specifies cell types.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[Tuple[str, str]]</code> <p>List of mutually exclusive gene pairs.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def find_mutually_exclusive_genes(\n    adata: ad.AnnData, markers: Dict[str, Dict[str, List[str]]], cell_type_column: str\n) -&gt; List[Tuple[str, str]]:\n    \"\"\"Identify mutually exclusive genes based on expression criteria.\n\n    Args:\n        adata: Annotated data object containing gene expression data.\n        markers: Dictionary where keys are cell types and values are dictionaries containing:\n            'positive': list of top x% highly expressed genes\n            'negative': list of top x% lowly expressed genes.\n        cell_type_column: Column name in `adata.obs` that specifies cell types.\n\n    Returns:\n        list: List of mutually exclusive gene pairs.\n    \"\"\"\n    exclusive_genes = {}\n    all_exclusive = []\n    gene_expression = adata.to_df()\n    for cell_type, marker_sets in markers.items():\n        positive_markers = marker_sets[\"positive\"]\n        exclusive_genes[cell_type] = []\n        for gene in positive_markers:\n            gene_expr = adata[:, gene].X\n            cell_type_mask = adata.obs[cell_type_column] == cell_type\n            non_cell_type_mask = ~cell_type_mask\n            if (gene_expr[cell_type_mask] &gt; 0).mean() &gt; 0.2 and (\n                gene_expr[non_cell_type_mask] &gt; 0\n            ).mean() &lt; 0.05:\n                exclusive_genes[cell_type].append(gene)\n                all_exclusive.append(gene)\n    unique_genes = list(\n        {\n            gene\n            for i in exclusive_genes.keys()\n            for gene in exclusive_genes[i]\n            if gene in all_exclusive\n        }\n    )\n    filtered_exclusive_genes = {\n        i: [gene for gene in exclusive_genes[i] if gene in unique_genes]\n        for i in exclusive_genes.keys()\n    }\n    mutually_exclusive_gene_pairs = [\n        tuple(sorted((gene1, gene2)))\n        for key1, key2 in combinations(filtered_exclusive_genes.keys(), 2)\n        if key1 != key2\n        for gene1 in filtered_exclusive_genes[key1]\n        for gene2 in filtered_exclusive_genes[key2]\n    ]\n    return set(mutually_exclusive_gene_pairs)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(\n            coords_1, coords_2, k=k, dist=dist, workers=workers\n        )\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(\n        coords_2, k=k, distance_upper_bound=dist, workers=workers\n    )\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(\n                np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T\n            )\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.get_polygons_from_xy","title":"get_polygons_from_xy","text":"<pre><code>get_polygons_from_xy(boundaries, x, y, label, scale_factor=1.0)\n</code></pre> <p>Convert boundary coordinates from a DataFrame to a GeoSeries of polygons.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries</code> <code>DataFrame</code> <p>A DataFrame containing the boundary data with x and y coordinates and identifiers.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <code>label</code> <code>str</code> <p>The name of the column representing the cell or nucleus label.</p> required <code>scale_factor</code> <code>float</code> <p>A ratio to scale the polygons. A value of 1.0 means no change, greater than 1.0 expands the polygons, and less than 1.0 shrinks the polygons. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>GeoSeries</code> <p>gpd.GeoSeries: A GeoSeries containing the polygons created from the boundary coordinates.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get_polygons_from_xy(\n    boundaries: pd.DataFrame,\n    x: str,\n    y: str,\n    label: str,\n    scale_factor: float = 1.0,\n) -&gt; gpd.GeoSeries:\n    \"\"\"Convert boundary coordinates from a DataFrame to a GeoSeries of polygons.\n\n    Args:\n        boundaries: A DataFrame containing the boundary data with x and y coordinates\n            and identifiers.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n        label: The name of the column representing the cell or nucleus label.\n        scale_factor: A ratio to scale the polygons. A value of 1.0 means no change,\n            greater than 1.0 expands the polygons, and less than 1.0 shrinks the polygons.\n            Defaults to 1.0.\n\n    Returns:\n        gpd.GeoSeries: A GeoSeries containing the polygons created from the boundary\n            coordinates.\n    \"\"\"\n    # Polygon offsets in coords\n    ids = boundaries[label].values\n    splits = np.where(ids[:-1] != ids[1:])[0] + 1\n    geometry_offset = np.hstack([0, splits, len(ids)])\n    part_offset = np.arange(len(np.unique(ids)) + 1)\n\n    # Convert to GeoSeries of polygons\n    polygons = shapely.from_ragged_array(\n        shapely.GeometryType.POLYGON,\n        coords=boundaries[[x, y]].values.copy(order=\"C\"),\n        offsets=(geometry_offset, part_offset),\n    )\n    gs = gpd.GeoSeries(polygons, index=np.unique(ids))\n\n    # print(gs)\n\n    if scale_factor != 1.0:\n        # Scale polygons around their centroid\n        gs = gpd.GeoSeries(\n            [\n                scale(geom, xfact=scale_factor, yfact=scale_factor, origin='centroid')\n                for geom in gs\n            ],\n            index=gs.index,\n        )\n        # print(gs)\n\n    return gs\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/_utils/#src.segger.data._utils.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.load_settings","title":"load_settings","text":"<pre><code>load_settings(sample_type)\n</code></pre> <p>Load a matching YAML file from the _settings/ directory and convert its contents into a SimpleNamespace.</p> <p>Parameters:</p> Name Type Description Default <code>sample_type</code> <code>str</code> <p>Name of the sample type to load (case-insensitive).</p> required <p>Returns:</p> Name Type Description <code>SimpleNamespace</code> <code>SimpleNamespace</code> <p>The settings loaded from the YAML file as a SimpleNamespace.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>sample_type</code> does not match any filenames.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def load_settings(sample_type: str) -&gt; SimpleNamespace:\n    \"\"\"Load a matching YAML file from the _settings/ directory and convert its\n    contents into a SimpleNamespace.\n\n    Args:\n        sample_type: Name of the sample type to load (case-insensitive).\n\n    Returns:\n        SimpleNamespace: The settings loaded from the YAML file as a SimpleNamespace.\n\n    Raises:\n        FileNotFoundError: If `sample_type` does not match any filenames.\n    \"\"\"\n    settings_dir = Path(__file__).parent.resolve() / \"_settings\"\n    # Get a list of YAML filenames (without extensions) in the _settings dir\n    filenames = [file.stem for file in settings_dir.glob(\"*.yaml\")]\n    # Convert sample_type to lowercase and check if it matches any filename\n    sample_type = sample_type.lower()\n    if sample_type not in filenames:\n        msg = (\n            f\"Sample type '{sample_type}' not found in settings. \"\n            f\"Available options: {', '.join(filenames)}\"\n        )\n        raise FileNotFoundError(msg)\n    # Load the matching YAML file\n    yaml_file_path = settings_dir / f\"{sample_type}.yaml\"\n    with yaml_file_path.open(\"r\") as file:\n        data = yaml.safe_load(file)\n\n    # Convert the YAML data into a SimpleNamespace recursively\n    return _dict_to_namespace(data)\n</code></pre>"},{"location":"api/data/_utils/#src.segger.data._utils.read_parquet_region","title":"read_parquet_region","text":"<pre><code>read_parquet_region(filepath, x, y, bounds=None, extra_columns=[], extra_filters=[], row_group_chunksize=None)\n</code></pre> <p>Read a region from a Parquet file based on x and y coordinates and optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <p>The path to the Parquet file.</p> required <code>x</code> <code>str</code> <p>The name of the column representing the x-coordinate.</p> required <code>y</code> <code>str</code> <p>The name of the column representing the y-coordinate.</p> required <code>bounds</code> <code>Polygon</code> <p>A polygon representing the bounding box to filter the data. If None, no bounding box filter is applied. Defaults to None.</p> <code>None</code> <code>extra_columns</code> <code>list[str]</code> <p>A list of additional columns to include in the output DataFrame. Defaults to [].</p> <code>[]</code> <code>extra_filters</code> <code>list[str]</code> <p>A list of additional filters to apply to the data. Defaults to [].</p> <code>[]</code> <code>row_group_chunksize</code> <code>Optional[int]</code> <p>Chunk size for row group processing. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the filtered data from the Parquet file.</p> Source code in <code>src/segger/data/_utils.py</code> <pre><code>def read_parquet_region(\n    filepath,\n    x: str,\n    y: str,\n    bounds: shapely.Polygon = None,\n    extra_columns: list[str] = [],\n    extra_filters: list[str] = [],\n    row_group_chunksize: Optional[int] = None,\n):\n    \"\"\"Read a region from a Parquet file based on x and y coordinates and optional\n    filters.\n\n    Args:\n        filepath: The path to the Parquet file.\n        x: The name of the column representing the x-coordinate.\n        y: The name of the column representing the y-coordinate.\n        bounds: A polygon representing the bounding box to filter the data. If None,\n            no bounding box filter is applied. Defaults to None.\n        extra_columns: A list of additional columns to include in the output DataFrame. Defaults to [].\n        extra_filters: A list of additional filters to apply to the data. Defaults to [].\n        row_group_chunksize: Chunk size for row group processing. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered data from the Parquet file.\n    \"\"\"\n    # Check backend and load dependencies if not already loaded\n\n    # Find bounds of full file if not supplied\n    if bounds is None:\n        bounds = get_xy_extents(filepath, x, y)\n\n    # Load pre-filtered data from Parquet file\n    filters = [\n        [\n            (x, \"&gt;\", bounds.bounds[0]),\n            (y, \"&gt;\", bounds.bounds[1]),\n            (x, \"&lt;\", bounds.bounds[2]),\n            (y, \"&lt;\", bounds.bounds[3]),\n        ]\n        + extra_filters\n    ]\n\n    columns = list({x, y} | set(extra_columns))\n\n    # Check if 'Geometry', 'geometry', 'polygon', or 'Polygon' is in the columns\n    if any(col in columns for col in ['Geometry', 'geometry', 'polygon', 'Polygon']):\n        import geopandas as gpd\n        # If geometry columns are present, read with geopandas\n        region = gpd.read_parquet(\n            filepath,\n            filters=filters,\n            columns=columns,\n        )\n    else:\n        # Otherwise, read with pandas\n        region = pd.read_parquet(\n            filepath,\n            filters=filters,\n            columns=columns,\n        )\n    return region\n</code></pre>"},{"location":"api/data/api_reference/","title":"Data Module API Reference","text":"<p>This page provides a comprehensive reference to all the classes, functions, and modules in the <code>segger.data</code> package.</p>"},{"location":"api/data/api_reference/#module-overview","title":"Module Overview","text":"<p>The <code>segger.data</code> package is organized into several key modules, each serving a specific purpose in the spatial transcriptomics data processing pipeline.</p>"},{"location":"api/data/api_reference/#sample-module","title":"Sample Module","text":"<p>The main data processing module containing the core classes for handling spatial transcriptomics data.</p> <p>Main Functions: - Data loading and validation - Spatial tiling and partitioning - Graph construction and feature engineering - Parallel processing coordination</p> <p>Key Classes: - <code>STSampleParquet</code>: Main orchestrator for data loading and processing - <code>STInMemoryDataset</code>: In-memory dataset with spatial indexing - <code>STTile</code>: Individual spatial tile processing</p>"},{"location":"api/data/api_reference/#utils-module","title":"Utils Module","text":"<p>Core utility functions for data processing, filtering, and analysis.</p> <p>Key Functions: - <code>get_xy_extents()</code>: Extract spatial extents from parquet files - <code>read_parquet_region()</code>: Read specific spatial regions from parquet files - <code>filter_transcripts()</code>: Filter transcripts by quality and gene type - <code>filter_boundaries()</code>: Filter boundaries by spatial criteria - <code>load_settings()</code>: Load technology-specific configurations - <code>find_markers()</code>: Identify marker genes for cell types</p>"},{"location":"api/data/api_reference/#pyg-dataset-module","title":"PyG Dataset Module","text":"<p>PyTorch Geometric dataset integration for machine learning workflows.</p> <ul> <li>Automatic tile discovery and loading</li> <li>PyTorch Lightning integration</li> <li>Built-in data validation</li> <li>Efficient memory management</li> </ul> <p>Key Classes: - <code>STPyGDataset</code>: PyTorch Geometric dataset wrapper</p>"},{"location":"api/data/api_reference/#transcript-embedding-module","title":"Transcript Embedding Module","text":"<p>Utilities for encoding transcript features into numerical representations.</p>"},{"location":"api/data/api_reference/#ndtree-module","title":"NDTree Module","text":"<p>Spatial partitioning and load balancing utilities.</p> <p>Key Classes: - <code>NDTree</code>: N-dimensional spatial partitioning - <code>innernode</code>: Internal tree node structure</p> <p>Features: - Efficient spatial data partitioning - Load balancing for parallel processing - Memory-optimized data structures - Configurable region sizing</p>"},{"location":"api/data/api_reference/#configuration-and-settings","title":"Configuration and Settings","text":""},{"location":"api/data/api_reference/#settings-directory","title":"Settings Directory","text":"<p>Technology-specific configuration files for different spatial transcriptomics platforms.</p> <p>Available Platforms: - Xenium - Merscope - CosMx - Xenium v2 (segmentation kit)</p> <p>Configuration Options: - Column mappings - Quality thresholds - Spatial parameters - Platform-specific defaults</p>"},{"location":"api/data/api_reference/#data-flow-architecture","title":"Data Flow Architecture","text":"<pre><code>Raw Data (Parquet) \u2192 STSampleParquet \u2192 Spatial Tiling \u2192 STInMemoryDataset \u2192 STTile \u2192 PyG Graph\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nMetadata Extraction    Region Division   Data Filtering   Tile Creation   Graph Construction\n     \u2193                      \u2193              \u2193                \u2193              \u2193\nSettings Loading      Load Balancing    Spatial Indexing   Feature Comp.   Edge Generation\n</code></pre>"},{"location":"api/data/api_reference/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>STSampleParquet (Main Orchestrator)\n\u251c\u2500\u2500 STInMemoryDataset (Region Processing)\n\u2502   \u2514\u2500\u2500 STTile (Individual Tile Processing)\n\u251c\u2500\u2500 TranscriptEmbedding (Feature Encoding)\n\u2514\u2500\u2500 NDTree (Spatial Partitioning)\n\nSTPyGDataset (ML Integration)\n\u2514\u2500\u2500 PyTorch Geometric Integration\n\nBackendHandler (Experimental)\n\u2514\u2500\u2500 Multi-backend Support\n</code></pre>"},{"location":"api/data/api_reference/#usage","title":"Usage","text":"<pre><code>from segger.data.sample import STSampleParquet\n\n# Load and process data\nsample = STSampleParquet(base_dir=\"path/to/data\", n_workers=4)\nsample.save(data_dir=\"./processed\", tile_size=1000)\n</code></pre>"},{"location":"api/data/ndtree/","title":"segger.data.ndtree","text":"<p>The <code>ndtree</code> module provides a specialized data structure for spatially partitioning multi-dimensional data. This module implements an NDTree (N-dimensional tree) that efficiently splits spatial data into balanced regions for parallel processing.</p>"},{"location":"api/data/ndtree/#src.segger.data._ndtree","title":"_ndtree","text":""},{"location":"api/data/ndtree/#src.segger.data._ndtree.NDTree","title":"NDTree","text":"<p>NDTree is a data structure for recursively splitting multi-dimensional data into smaller regions until each leaf node contains less than or equal to a specified number of points. It stores these regions in a balanced binary tree.</p> <p>Attributes:</p> Name Type Description <code>data</code> <p>The input data to be partitioned.</p> <code>n</code> <p>The maximum number of points allowed in a leaf node.</p> <code>idx</code> <p>The indices of the input data points.</p> <code>boxes</code> <p>A list to store the bounding boxes (as shapely polygons) of each region in the tree.</p> <code>rect</code> <p>The bounding box of the entire input data space.</p> <code>tree</code> <p>The root of the NDTree.</p> Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>class NDTree:\n    \"\"\"NDTree is a data structure for recursively splitting multi-dimensional data\n    into smaller regions until each leaf node contains less than or equal to a\n    specified number of points. It stores these regions in a balanced binary\n    tree.\n\n    Attributes:\n        data: The input data to be partitioned.\n        n: The maximum number of points allowed in a leaf node.\n        idx: The indices of the input data points.\n        boxes: A list to store the bounding boxes (as shapely polygons) of each region\n            in the tree.\n        rect: The bounding box of the entire input data space.\n        tree: The root of the NDTree.\n    \"\"\"\n\n    def __init__(self, data, n):\n        \"\"\"Initialize the NDTree with the given data and maximum points per leaf\n        node.\n\n        Args:\n            data: The input data to be partitioned.\n            n: The maximum number of points allowed in a leaf node.\n        \"\"\"\n        self.data = np.asarray(data)\n        self.n = n\n        self.idx = np.arange(data.shape[0])\n        self.boxes = []\n        self.rect = Rectangle(data.min(0), data.max(0))\n        self.tree = innernode(self.n, self.idx, self.rect, self)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.NDTree.__init__","title":"__init__","text":"<pre><code>__init__(data, n)\n</code></pre> <p>Initialize the NDTree with the given data and maximum points per leaf node.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The input data to be partitioned.</p> required <code>n</code> <p>The maximum number of points allowed in a leaf node.</p> required Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>def __init__(self, data, n):\n    \"\"\"Initialize the NDTree with the given data and maximum points per leaf\n    node.\n\n    Args:\n        data: The input data to be partitioned.\n        n: The maximum number of points allowed in a leaf node.\n    \"\"\"\n    self.data = np.asarray(data)\n    self.n = n\n    self.idx = np.arange(data.shape[0])\n    self.boxes = []\n    self.rect = Rectangle(data.min(0), data.max(0))\n    self.tree = innernode(self.n, self.idx, self.rect, self)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.innernode","title":"innernode","text":"<p>Represent a node in the NDTree. Each node either stores a bounding box for the data it contains (leaf nodes) or splits the data into two child nodes.</p> <p>Attributes:</p> Name Type Description <code>n</code> <p>The maximum number of points allowed in a leaf node for this subtree.</p> <code>idx</code> <p>The indices of the data points in this node.</p> <code>tree</code> <p>The reference to the main NDTree that holds the data and bounding boxes.</p> <code>rect</code> <p>The bounding box of the data points in this node.</p> <code>split_dim</code> <p>The dimension along which the node splits the data.</p> <code>split_point</code> <p>The value along the split dimension used to divide the data.</p> <code>less</code> <p>The child node containing data points less than or equal to the split point.</p> <code>greater</code> <p>The child node containing data points greater than the split point.</p> Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>class innernode:\n    \"\"\"Represent a node in the NDTree. Each node either stores a bounding box for\n    the data it contains (leaf nodes) or splits the data into two child nodes.\n\n    Attributes:\n        n: The maximum number of points allowed in a leaf node for this subtree.\n        idx: The indices of the data points in this node.\n        tree: The reference to the main NDTree that holds the data and bounding boxes.\n        rect: The bounding box of the data points in this node.\n        split_dim: The dimension along which the node splits the data.\n        split_point: The value along the split dimension used to divide the data.\n        less: The child node containing data points less than or equal to the split\n            point.\n        greater: The child node containing data points greater than the split point.\n    \"\"\"\n\n    def __init__(self, n, idx, rect, tree):\n        \"\"\"Initialize the innernode and split the data if necessary.\n\n        Args:\n            n: The maximum number of points allowed in a leaf node.\n            idx: The indices of the data points in this node.\n            rect: The bounding box of the data points in this node.\n            tree: The reference to the main NDTree.\n        \"\"\"\n        self.n = n\n        self.idx = idx\n        self.tree = tree\n        self.rect = rect\n        if not n == 1:\n            self.split()\n        else:\n            box = shapely.box(*self.rect.mins, *self.rect.maxes)\n            self.tree.boxes.append(box)\n\n    def split(self):\n        \"\"\"Recursively split the node's data into two child nodes along the\n        dimension with the largest spread.\n        \"\"\"\n        less = math.floor(self.n // 2)\n        greater = self.n - less\n        data = self.tree.data[self.idx]\n        self.split_dim = np.argmax(self.rect.maxes - self.rect.mins)\n        data = data[:, self.split_dim]\n        self.split_point = np.quantile(data, less / (less + greater))\n        mask = data &lt;= self.split_point\n        less_rect, greater_rect = self.rect.split(self.split_dim, self.split_point)\n        self.less = innernode(less, self.idx[mask], less_rect, self.tree)\n        self.greater = innernode(greater, self.idx[~mask], greater_rect, self.tree)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.innernode.__init__","title":"__init__","text":"<pre><code>__init__(n, idx, rect, tree)\n</code></pre> <p>Initialize the innernode and split the data if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>The maximum number of points allowed in a leaf node.</p> required <code>idx</code> <p>The indices of the data points in this node.</p> required <code>rect</code> <p>The bounding box of the data points in this node.</p> required <code>tree</code> <p>The reference to the main NDTree.</p> required Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>def __init__(self, n, idx, rect, tree):\n    \"\"\"Initialize the innernode and split the data if necessary.\n\n    Args:\n        n: The maximum number of points allowed in a leaf node.\n        idx: The indices of the data points in this node.\n        rect: The bounding box of the data points in this node.\n        tree: The reference to the main NDTree.\n    \"\"\"\n    self.n = n\n    self.idx = idx\n    self.tree = tree\n    self.rect = rect\n    if not n == 1:\n        self.split()\n    else:\n        box = shapely.box(*self.rect.mins, *self.rect.maxes)\n        self.tree.boxes.append(box)\n</code></pre>"},{"location":"api/data/ndtree/#src.segger.data._ndtree.innernode.split","title":"split","text":"<pre><code>split()\n</code></pre> <p>Recursively split the node's data into two child nodes along the dimension with the largest spread.</p> Source code in <code>src/segger/data/_ndtree.py</code> <pre><code>def split(self):\n    \"\"\"Recursively split the node's data into two child nodes along the\n    dimension with the largest spread.\n    \"\"\"\n    less = math.floor(self.n // 2)\n    greater = self.n - less\n    data = self.tree.data[self.idx]\n    self.split_dim = np.argmax(self.rect.maxes - self.rect.mins)\n    data = data[:, self.split_dim]\n    self.split_point = np.quantile(data, less / (less + greater))\n    mask = data &lt;= self.split_point\n    less_rect, greater_rect = self.rect.split(self.split_dim, self.split_point)\n    self.less = innernode(less, self.idx[mask], less_rect, self.tree)\n    self.greater = innernode(greater, self.idx[~mask], greater_rect, self.tree)\n</code></pre>"},{"location":"api/data/pyg_dataset/","title":"segger.data.pyg_dataset","text":"<p>The <code>pyg_dataset</code> module provides PyTorch Geometric (PyG) dataset integration for spatial transcriptomics data. This module enables seamless integration between Segger's spatial data processing and PyTorch-based machine learning workflows.</p>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset","title":"pyg_dataset","text":""},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset","title":"STPyGDataset","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>An in-memory dataset class for handling training using spatial transcriptomics data.</p> Source code in <code>src/segger/data/pyg_dataset.py</code> <pre><code>class STPyGDataset(InMemoryDataset):\n    \"\"\"An in-memory dataset class for handling training using spatial\n    transcriptomics data.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        transform: Optional[Callable] = None,\n        pre_transform: Optional[Callable] = None,\n        pre_filter: Optional[Callable] = None,\n    ):\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n    @property\n    def raw_file_names(self) -&gt; List[str]:\n        \"\"\"Return a list of raw file names in the raw directory.\n\n        Returns:\n            List[str]: List of raw file names.\n        \"\"\"\n        return os.listdir(self.raw_dir)\n\n    @property\n    def processed_file_names(self) -&gt; List[str]:\n        \"\"\"Return a list of processed file names in the processed directory.\n\n        Returns:\n            List[str]: List of processed file names.\n        \"\"\"\n        paths = glob.glob(f\"{self.processed_dir}/tiles_x*_y*_*_*.pt\")\n        # paths = paths.append(paths = glob.glob(f'{self.processed_dir}/tiles_x*_y*_*_*.pt'))\n        file_names = list(map(os.path.basename, paths))\n        return file_names\n\n    def len(self) -&gt; int:\n        \"\"\"Return the number of processed files.\n\n        Returns:\n            int: Number of processed files.\n        \"\"\"\n        return len(self.processed_file_names)\n\n    def get(self, idx: int) -&gt; Data:\n        \"\"\"Get a processed data object.\n\n        Args:\n            idx: Index of the data object to retrieve.\n\n        Returns:\n            Data: The processed data object.\n        \"\"\"\n        filepath = Path(self.processed_dir) / self.processed_file_names[idx]\n        data = torch.load(filepath)\n        # this is an issue in PyG's RandomLinkSplit, dimensions are not consistent if there is only one edge in the graph\n        if hasattr(data[\"tx\", \"belongs\", \"bd\"], \"edge_label_index\"):\n            if data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 1:\n                data[\"tx\", \"belongs\", \"bd\"].edge_label_index = data[\n                    \"tx\", \"belongs\", \"bd\"\n                ].edge_label_index.unsqueeze(1)\n                data[\"tx\", \"belongs\", \"bd\"].edge_label = data[\n                    \"tx\", \"belongs\", \"bd\"\n                ].edge_label.unsqueeze(0)\n            assert data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 2\n        return data\n</code></pre>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>src/segger/data/pyg_dataset.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx: Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    filepath = Path(self.processed_dir) / self.processed_file_names[idx]\n    data = torch.load(filepath)\n    # this is an issue in PyG's RandomLinkSplit, dimensions are not consistent if there is only one edge in the graph\n    if hasattr(data[\"tx\", \"belongs\", \"bd\"], \"edge_label_index\"):\n        if data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 1:\n            data[\"tx\", \"belongs\", \"bd\"].edge_label_index = data[\n                \"tx\", \"belongs\", \"bd\"\n            ].edge_label_index.unsqueeze(1)\n            data[\"tx\", \"belongs\", \"bd\"].edge_label = data[\n                \"tx\", \"belongs\", \"bd\"\n            ].edge_label.unsqueeze(0)\n        assert data[\"tx\", \"belongs\", \"bd\"].edge_label_index.dim() == 2\n    return data\n</code></pre>"},{"location":"api/data/pyg_dataset/#src.segger.data.pyg_dataset.STPyGDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>src/segger/data/pyg_dataset.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/pyg_dataset/#overview","title":"Overview","text":"<p>The <code>STPyGDataset</code> class extends PyTorch Geometric's <code>InMemoryDataset</code> to provide a standardized interface for loading and managing spatial transcriptomics data in machine learning pipelines. It handles the conversion of processed tiles into PyTorch Geometric format and provides utilities for training and validation.</p>"},{"location":"api/data/sample/","title":"segger.data.sample","text":"<p>The <code>sample</code> module is the core of the Segger data processing framework, providing comprehensive classes for handling spatial transcriptomics data. This module contains the main classes that orchestrate the entire data processing pipeline from raw data to machine learning-ready graphs.</p>"},{"location":"api/data/sample/#src.segger.data.sample","title":"sample","text":""},{"location":"api/data/sample/#src.segger.data.sample.STInMemoryDataset","title":"STInMemoryDataset","text":"<pre><code>STInMemoryDataset(sample, extents, margin=10)\n</code></pre> <p>A class for handling in-memory representations of ST data.</p> <p>This class is used to load and manage ST sample data from parquet files, filter boundaries and transcripts, and provide spatial tiling for further analysis. The class also pre-loads KDTrees for efficient spatial queries.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>STSampleParquet</code> <p>The ST sample containing paths to the data files.</p> required <code>extents</code> <code>Polygon</code> <p>The polygon defining the spatial extents for the dataset.</p> required <code>margin</code> <code>int</code> <p>The margin to buffer around the extents when filtering data. Defaults to 10.</p> <code>10</code> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>STSampleParquet</code> <p>The ST sample from which the data is loaded.</p> required <code>extents</code> <code>Polygon</code> <p>The spatial extents of the dataset.</p> required <code>margin</code> <code>int</code> <p>The buffer margin around the extents for filtering.</p> <code>10</code> <code>transcripts</code> <p>The filtered transcripts within the dataset extents.</p> required <code>boundaries</code> <p>The filtered boundaries within the dataset extents.</p> required <code>kdtree_tx</code> <p>The KDTree for fast spatial queries on the transcripts.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transcripts or boundaries could not be loaded or filtered.</p> <p>Initialize the STInMemoryDataset instance by loading transcripts and boundaries from parquet files and pre-loading a KDTree for fast spatial queries.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>STSampleParquet</code> <p>The ST sample containing paths to the data files.</p> required <code>extents</code> <code>Polygon</code> <p>The polygon defining the spatial extents for the dataset.</p> required <code>margin</code> <code>int</code> <p>The margin to buffer around the extents when filtering data. Defaults to 10.</p> <code>10</code> Source code in <code>src/segger/data/sample.py</code> <pre><code>def __init__(\n    self,\n    sample: STSampleParquet,\n    extents: shapely.Polygon,\n    margin: int = 10,\n):\n    \"\"\"Initialize the STInMemoryDataset instance by loading transcripts\n    and boundaries from parquet files and pre-loading a KDTree for fast\n    spatial queries.\n\n    Args:\n        sample: The ST sample containing paths to the data files.\n        extents: The polygon defining the spatial extents for the dataset.\n        margin: The margin to buffer around the extents when filtering data. Defaults to 10.\n    \"\"\"\n    # Set properties\n    self.sample = sample\n    self.extents = extents\n    self.margin = margin\n    self.settings = self.sample.settings\n\n    # Load data from parquet files\n    self._load_transcripts(self.sample._transcripts_filepath)\n    self._load_boundaries(self.sample._boundaries_filepath)\n\n    # Pre-load KDTrees\n    self.kdtree_tx = KDTree(\n        self.transcripts[self.settings.transcripts.xy], leafsize=100\n    )\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet","title":"STSampleParquet","text":"<pre><code>STSampleParquet(base_dir, n_workers=1, scale_factor=1.0, sample_type=None, weights=None)\n</code></pre> <p>A class to manage spatial transcriptomics data stored in parquet files.</p> <p>This class provides methods for loading, processing, and saving data related to ST samples. It supports parallel processing and efficient handling of transcript and boundary data.</p> <p>Initialize the STSampleParquet instance.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>PathLike</code> <p>The base directory containing the ST data.</p> required <code>n_workers</code> <code>Optional[int]</code> <p>The number of workers for parallel processing. Defaults to 1.</p> <code>1</code> <code>sample_type</code> <code>str</code> <p>The sample type of the raw data, e.g., 'xenium' or 'merscope'. Defaults to None.</p> <code>None</code> <code>weights</code> <code>DataFrame</code> <p>DataFrame containing weights for transcript embedding. Defaults to None.</p> <code>None</code> <code>scale_factor</code> <code>Optional[float]</code> <p>The scale factor to be used for expanding the boundary extents during spatial queries. If not provided, the default from settings will be used. Defaults to None.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the base directory does not exist or the required files are missing.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def __init__(\n    self,\n    base_dir: os.PathLike,\n    n_workers: Optional[int] = 1,\n    scale_factor: Optional[float] = 1.0,\n    sample_type: str = None,\n    weights: pd.DataFrame = None,\n):\n    \"\"\"Initialize the STSampleParquet instance.\n\n    Args:\n        base_dir: The base directory containing the ST data.\n        n_workers: The number of workers for parallel processing. Defaults to 1.\n        sample_type: The sample type of the raw data, e.g., 'xenium' or 'merscope'. Defaults to None.\n        weights: DataFrame containing weights for transcript embedding. Defaults to None.\n        scale_factor: The scale factor to be used for expanding the boundary extents\n            during spatial queries. If not provided, the default from settings\n            will be used. Defaults to None.\n\n    Raises:\n        FileNotFoundError: If the base directory does not exist or the required files are\n            missing.\n    \"\"\"\n    # Setup paths and resource constraints\n    self._base_dir = Path(base_dir)\n    self.settings = utils.load_settings(sample_type)\n    transcripts_fn = self.settings.transcripts.filename\n    self._transcripts_filepath = self._base_dir / transcripts_fn\n    boundaries_fn = self.settings.boundaries.filename\n    self._boundaries_filepath = self._base_dir / boundaries_fn\n    self.n_workers = n_workers\n    self.settings.boundaries.scale_factor = 1\n    nuclear_column = getattr(self.settings.transcripts, \"nuclear_column\", None)\n    if nuclear_column is None or self.settings.boundaries.scale_factor != 1.0:\n        print(\n            \"Boundary-transcript overlap information has not been pre-computed. It will be calculated during tile generation.\"\n        )\n    # Set scale factor if provided\n    if scale_factor != 1.0:\n        self.settings.boundaries.scale_factor = scale_factor\n\n    # Ensure transcript IDs exist\n    utils.ensure_transcript_ids(\n        self._transcripts_filepath,\n        self.settings.transcripts.x,\n        self.settings.transcripts.y,\n        self.settings.transcripts.id,\n    )\n\n    # Setup logging\n    logging.basicConfig(level=logging.INFO)\n    self.logger = logging.Logger(f\"STSample@{base_dir}\")\n\n    # Internal caches\n    self._extents = None\n    self._transcripts_metadata = None\n    self._boundaries_metadata = None\n\n    # Setup default embedding for transcripts\n    self._emb_genes = None\n    if weights is not None:\n        self._emb_genes = weights.index.to_list()\n    classes = self.transcripts_metadata[\"feature_names\"]\n    self._transcript_embedding = TranscriptEmbedding(np.array(classes), weights)\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.boundaries_metadata","title":"boundaries_metadata  <code>cached</code> <code>property</code>","text":"<pre><code>boundaries_metadata\n</code></pre> <p>Retrieve metadata for the boundaries stored in the sample.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Metadata dictionary for boundaries including column sizes.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the boundaries parquet file does not exist.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.extents","title":"extents  <code>cached</code> <code>property</code>","text":"<pre><code>extents\n</code></pre> <p>Get the combined extents (bounding box) of the transcripts and boundaries.</p> <p>Returns:</p> Type Description <code>Polygon</code> <p>shapely.Polygon: The bounding box covering all transcripts and boundaries.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.n_transcripts","title":"n_transcripts  <code>property</code>","text":"<pre><code>n_transcripts\n</code></pre> <p>Get the total number of transcripts in the sample.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of transcripts.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.transcripts_metadata","title":"transcripts_metadata  <code>cached</code> <code>property</code>","text":"<pre><code>transcripts_metadata\n</code></pre> <p>Retrieve metadata for the transcripts stored in the sample.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Metadata dictionary for transcripts including column sizes and</p> <code>dict</code> <p>feature names.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the transcript parquet file does not exist.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.save","title":"save","text":"<pre><code>save(data_dir, k_bd=3, dist_bd=15.0, k_tx=3, dist_tx=5.0, k_tx_ex=100, dist_tx_ex=20, tile_size=None, tile_width=None, tile_height=None, neg_sampling_ratio=5.0, frac=1.0, val_prob=0.1, test_prob=0.2, mutually_exclusive_genes=None)\n</code></pre> <p>Save the tiles of the sample as PyTorch geometric datasets.</p> <p>See documentation for 'STTile' for more information on dataset contents.</p> <p>Note: This function requires either 'tile_size' OR both 'tile_width' and 'tile_height' to be provided.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>PathLike</code> <p>The directory where the dataset should be saved.</p> required <code>k_bd</code> <code>int</code> <p>Number of nearest neighbors for boundary nodes. Defaults to 3.</p> <code>3</code> <code>dist_bd</code> <code>float</code> <p>Maximum distance for boundary neighbors. Defaults to 15.0.</p> <code>15.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for transcript nodes. Defaults to 3.</p> <code>3</code> <code>dist_tx</code> <code>float</code> <p>Maximum distance for transcript neighbors. Defaults to 5.0.</p> <code>5.0</code> <code>tile_size</code> <code>Optional[int]</code> <p>If provided, specifies the size of the tile. Overrides <code>tile_width</code> and <code>tile_height</code>. Defaults to None.</p> <code>None</code> <code>tile_width</code> <code>Optional[int]</code> <p>Width of the tiles in pixels. Ignored if <code>tile_size</code> is provided. Defaults to None.</p> <code>None</code> <code>tile_height</code> <code>Optional[int]</code> <p>Height of the tiles in pixels. Ignored if <code>tile_size</code> is provided. Defaults to None.</p> <code>None</code> <code>neg_sampling_ratio</code> <code>float</code> <p>Ratio of negative samples. Defaults to 5.0.</p> <code>5.0</code> <code>frac</code> <code>float</code> <p>Fraction of the dataset to process. Defaults to 1.0.</p> <code>1.0</code> <code>val_prob</code> <code>float</code> <p>Proportion of data for use for validation split. Defaults to 0.1.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Proportion of data for use for test split. Defaults to 0.2.</p> <code>0.2</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'frac' parameter is greater than 1.0 or if the calculated number of tiles is zero.</p> <code>AssertionError</code> <p>If the specified directory structure is not properly set up.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def save(\n    self,\n    data_dir: os.PathLike,\n    k_bd: int = 3,\n    dist_bd: float = 15.0,\n    k_tx: int = 3,\n    dist_tx: float = 5.0,\n    k_tx_ex: int = 100,\n    dist_tx_ex: float = 20,\n    tile_size: Optional[int] = None,\n    tile_width: Optional[int] = None,\n    tile_height: Optional[int] = None,\n    neg_sampling_ratio: float = 5.0,\n    frac: float = 1.0,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n    mutually_exclusive_genes: Optional[List] = None,\n):\n    \"\"\"Save the tiles of the sample as PyTorch geometric datasets.\n\n    See documentation for 'STTile' for more information on dataset contents.\n\n    Note: This function requires either 'tile_size' OR both 'tile_width' and\n    'tile_height' to be provided.\n\n    Args:\n        data_dir: The directory where the dataset should be saved.\n        k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n        dist_bd: Maximum distance for boundary neighbors. Defaults to 15.0.\n        k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n        dist_tx: Maximum distance for transcript neighbors. Defaults to 5.0.\n        tile_size: If provided, specifies the size of the tile. Overrides `tile_width`\n            and `tile_height`. Defaults to None.\n        tile_width: Width of the tiles in pixels. Ignored if `tile_size` is provided. Defaults to None.\n        tile_height: Height of the tiles in pixels. Ignored if `tile_size` is provided. Defaults to None.\n        neg_sampling_ratio: Ratio of negative samples. Defaults to 5.0.\n        frac: Fraction of the dataset to process. Defaults to 1.0.\n        val_prob: Proportion of data for use for validation split. Defaults to 0.1.\n        test_prob: Proportion of data for use for test split. Defaults to 0.2.\n\n    Raises:\n        ValueError: If the 'frac' parameter is greater than 1.0 or if the calculated\n            number of tiles is zero.\n        AssertionError: If the specified directory structure is not properly set up.\n    \"\"\"\n    # Check inputs\n    try:\n        if frac &gt; 1:\n            msg = f\"Arg 'frac' should be &lt;= 1.0, but got {frac}.\"\n            raise ValueError(msg)\n        if tile_size is not None:\n            n_tiles = self.n_transcripts / tile_size / self.n_workers * frac\n            if int(n_tiles) == 0:\n                msg = f\"Sampling parameters would yield 0 total tiles.\"\n                raise ValueError(msg)\n    # Propagate errors to logging\n    except Exception as e:\n        self.logger.error(str(e), exc_info=True)\n        raise e\n\n    # Setup directory structure to save tiles\n    data_dir = Path(data_dir)\n    STSampleParquet._setup_directory(data_dir)\n\n    # Function to parallelize over workers\n    def func(region):\n        xm = STInMemoryDataset(sample=self, extents=region)\n        tiles = xm._tile(tile_width, tile_height, tile_size)\n        # print(tiles)\n        if frac &lt; 1:\n            tiles = random.sample(tiles, int(len(tiles) * frac))\n        for tile in tiles:\n            # Choose training, test, or validation datasets\n            data_type = np.random.choice(\n                a=[\"train_tiles\", \"test_tiles\", \"val_tiles\"],\n                p=[1 - (test_prob + val_prob), test_prob, val_prob],\n            )\n            xt = STTile(dataset=xm, extents=tile)\n            pyg_data = xt.to_pyg_dataset(\n                k_bd=k_bd,\n                dist_bd=dist_bd,\n                k_tx=k_tx,\n                dist_tx=dist_tx,\n                k_tx_ex=k_tx_ex,\n                dist_tx_ex=dist_tx_ex,\n                neg_sampling_ratio=neg_sampling_ratio,\n                mutually_exclusive_genes = mutually_exclusive_genes\n            )\n            if pyg_data is not None:\n                if pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index.numel() == 0:\n                    # this tile is only for testing\n                    data_type = \"test_tiles\"\n                filepath = data_dir / data_type / \"processed\" / f\"{xt.uid}.pt\"\n                torch.save(pyg_data, filepath)\n\n    # TODO: Add Dask backend\n    regions = self._get_balanced_regions()\n    outs = []\n    for region in regions:\n        outs.append(func(region))\n    return outs\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.save_debug","title":"save_debug","text":"<pre><code>save_debug(data_dir, k_bd=3, dist_bd=15.0, k_tx=3, dist_tx=5.0, k_tx_ex=100, dist_tx_ex=20, tile_width=None, tile_height=None, neg_sampling_ratio=5.0, frac=1.0, val_prob=0.1, test_prob=0.2)\n</code></pre> <p>Debug version of save method that processes tiles sequentially and prints detailed information about the process.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>PathLike</code> <p>The directory where the dataset should be saved.</p> required <code>k_bd</code> <code>int</code> <p>Number of nearest neighbors for boundary nodes. Defaults to 3.</p> <code>3</code> <code>dist_bd</code> <code>float</code> <p>Maximum distance for boundary neighbors. Defaults to 15.0.</p> <code>15.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for transcript nodes. Defaults to 3.</p> <code>3</code> <code>dist_tx</code> <code>float</code> <p>Maximum distance for transcript neighbors. Defaults to 5.0.</p> <code>5.0</code> <code>k_tx_ex</code> <code>int</code> <p>Number of nearest neighbors for transcript exclusion. Defaults to 100.</p> <code>100</code> <code>dist_tx_ex</code> <code>float</code> <p>Maximum distance for transcript exclusion. Defaults to 20.</p> <code>20</code> <code>tile_width</code> <code>Optional[float]</code> <p>Width of the tiles in pixels. Defaults to None.</p> <code>None</code> <code>tile_height</code> <code>Optional[float]</code> <p>Height of the tiles in pixels. Defaults to None.</p> <code>None</code> <code>neg_sampling_ratio</code> <code>float</code> <p>Ratio of negative samples. Defaults to 5.0.</p> <code>5.0</code> <code>frac</code> <code>float</code> <p>Fraction of the dataset to process. Defaults to 1.0.</p> <code>1.0</code> <code>val_prob</code> <code>float</code> <p>Proportion of data for use for validation split. Defaults to 0.1.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Proportion of data for use for test split. Defaults to 0.2.</p> <code>0.2</code> Source code in <code>src/segger/data/sample.py</code> <pre><code>def save_debug(\n    self,\n    data_dir: os.PathLike,\n    k_bd: int = 3,\n    dist_bd: float = 15.0,\n    k_tx: int = 3,\n    dist_tx: float = 5.0,\n    k_tx_ex: int = 100,\n    dist_tx_ex: float = 20,\n    tile_width: Optional[float] = None,\n    tile_height: Optional[float] = None,\n    neg_sampling_ratio: float = 5.0,\n    frac: float = 1.0,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n):\n    \"\"\"Debug version of save method that processes tiles sequentially and prints\n    detailed information about the process.\n\n    Args:\n        data_dir: The directory where the dataset should be saved.\n        k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n        dist_bd: Maximum distance for boundary neighbors. Defaults to 15.0.\n        k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n        dist_tx: Maximum distance for transcript neighbors. Defaults to 5.0.\n        k_tx_ex: Number of nearest neighbors for transcript exclusion. Defaults to 100.\n        dist_tx_ex: Maximum distance for transcript exclusion. Defaults to 20.\n        tile_width: Width of the tiles in pixels. Defaults to None.\n        tile_height: Height of the tiles in pixels. Defaults to None.\n        neg_sampling_ratio: Ratio of negative samples. Defaults to 5.0.\n        frac: Fraction of the dataset to process. Defaults to 1.0.\n        val_prob: Proportion of data for use for validation split. Defaults to 0.1.\n        test_prob: Proportion of data for use for test split. Defaults to 0.2.\n    \"\"\"\n    print(\"\\n=== Starting Debug Tile Generation ===\")\n    print(f\"Parameters:\")\n    print(f\"- k_bd: {k_bd} (boundary neighbors)\")\n    print(f\"- dist_bd: {dist_bd} (boundary distance)\")\n    print(f\"- k_tx: {k_tx} (transcript neighbors)\")\n    print(f\"- dist_tx: {dist_tx} (transcript distance)\")\n    print(f\"- tile_width: {tile_width}\")\n    print(f\"- tile_height: {tile_height}\")\n    print(f\"- frac: {frac}\")\n    print(f\"- val_prob: {val_prob}\")\n    print(f\"- test_prob: {test_prob}\")\n\n    # Setup directory structure to save tiles\n    data_dir = Path(data_dir)\n    STSampleParquet._setup_directory(data_dir)\n    print(f\"\\nOutput directory: {data_dir}\")\n\n    # Get regions to process\n    regions = self._get_balanced_regions()\n    print(f\"\\nTotal regions to process: {len(regions)}\")\n    print(\"Region bounds:\")\n    for i, region in enumerate(regions):\n        print(f\"Region {i+1}: {region.bounds}\")\n\n    # Process each region sequentially\n    for region_idx, region in enumerate(regions):\n        print(f\"\\n=== Processing Region {region_idx + 1}/{len(regions)} ===\")\n        print(f\"Region bounds: {region.bounds}\")\n\n        xm = STInMemoryDataset(sample=self, extents=region)\n        tiles = xm._tile(tile_width, tile_height, None)\n        print(f\"Generated {len(tiles)} tiles for this region\")\n\n        if frac &lt; 1:\n            tiles = random.sample(tiles, int(len(tiles) * frac))\n            print(f\"After sampling: {len(tiles)} tiles\")\n\n        # Process each tile\n        for tile_idx, tile in enumerate(tiles):\n            print(f\"\\n--- Processing Tile {tile_idx + 1}/{len(tiles)} ---\")\n            print(f\"Tile bounds: {tile.bounds}\")\n\n            # Choose training, test, or validation datasets\n            data_type = np.random.choice(\n                a=[\"train_tiles\", \"test_tiles\", \"val_tiles\"],\n                p=[1 - (test_prob + val_prob), test_prob, val_prob],\n            )\n            print(f\"Assigned to: {data_type}\")\n\n            xt = STTile(dataset=xm, extents=tile)\n            print(f\"Tile UID: {xt.uid}\")\n\n            pyg_data = xt.to_pyg_dataset(\n                k_bd=k_bd,\n                dist_bd=dist_bd,\n                k_tx=k_tx,\n                dist_tx=dist_tx,\n                k_tx_ex=k_tx_ex,\n                dist_tx_ex=dist_tx_ex,\n                neg_sampling_ratio=neg_sampling_ratio,\n            )\n\n            if pyg_data is not None:\n                if pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index.numel() == 0:\n                    data_type = \"test_tiles\"\n                    print(\"No tx-belongs-bd edges found, reassigning to test_tiles\")\n\n                filepath = data_dir / data_type / \"processed\" / f\"{xt.uid}.pt\"\n                torch.save(pyg_data, filepath)\n                print(f\"Saved to: {filepath}\")\n\n                # Print some statistics about the generated data\n                print(f\"Data statistics:\")\n                print(f\"- Number of transcripts: {pyg_data['tx'].num_nodes}\")\n                print(f\"- Number of boundaries: {pyg_data['bd'].num_nodes}\")\n                print(\n                    f\"- Number of tx-tx edges: {pyg_data['tx', 'neighbors', 'tx'].edge_index.shape[1]}\"\n                )\n                print(\n                    f\"- Number of tx-bd edges: {pyg_data['tx', 'neighbors', 'bd'].edge_index.shape[1]}\"\n                )\n                # print(f\"- Number of tx-belongs-bd edges: {pyg_data['tx', 'belongs', 'bd'].edge_index.shape[1]}\")\n            else:\n                print(\"Skipping tile - no valid data generated\")\n\n    print(\"\\n=== Debug Tile Generation Completed ===\")\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STSampleParquet.set_transcript_embedding","title":"set_transcript_embedding","text":"<pre><code>set_transcript_embedding(weights)\n</code></pre> <p>Set the transcript embedding for the sample.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>DataFrame</code> <p>A DataFrame containing the weights for each transcript.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided weights do not match the number of transcript features.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def set_transcript_embedding(self, weights: pd.DataFrame):\n    \"\"\"Set the transcript embedding for the sample.\n\n    Args:\n        weights: A DataFrame containing the weights for each transcript.\n\n    Raises:\n        ValueError: If the provided weights do not match the number of transcript\n            features.\n    \"\"\"\n    classes = self._transcripts_metadata[\"feature_names\"]\n    self._transcript_embedding = TranscriptEmbedding(classes, weights)\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile","title":"STTile","text":"<pre><code>STTile(dataset, extents)\n</code></pre> <p>A class representing a tile of a ST sample.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>STInMemoryDataset</code> <p>The ST dataset containing data.</p> required <code>extents</code> <code>Polygon</code> <p>The extents of the tile in the sample.</p> required <code>boundaries</code> <p>Filtered boundaries within the tile extents.</p> required <code>transcripts</code> <p>Filtered transcripts within the tile extents.</p> required <p>Initialize a STTile instance.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>STInMemoryDataset</code> <p>The ST dataset containing data.</p> required <code>extents</code> <code>Polygon</code> <p>The extents of the tile in the sample.</p> required Note <p>The <code>boundaries</code> and <code>transcripts</code> attributes are cached to avoid the overhead of filtering when tiles are instantiated. This is particularly useful in multiprocessing settings where generating tiles in parallel could lead to high overhead.</p> Internal Args <p>_boundaries: Cached DataFrame of filtered boundaries. Initially set to None. _transcripts: Cached DataFrame of filtered transcripts. Initially set to None.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def __init__(\n    self,\n    dataset: STInMemoryDataset,\n    extents: shapely.Polygon,\n):\n    \"\"\"Initialize a STTile instance.\n\n    Args:\n        dataset: The ST dataset containing data.\n        extents: The extents of the tile in the sample.\n\n    Note:\n        The `boundaries` and `transcripts` attributes are cached to avoid the\n        overhead of filtering when tiles are instantiated. This is particularly\n        useful in multiprocessing settings where generating tiles in parallel\n        could lead to high overhead.\n\n    Internal Args:\n        _boundaries: Cached DataFrame of filtered boundaries. Initially set to None.\n        _transcripts: Cached DataFrame of filtered transcripts. Initially set to None.\n    \"\"\"\n    self.dataset = dataset\n    self.extents = extents\n    self.margin = dataset.margin\n    self.settings = self.dataset.settings\n\n    # Internal caches for filtered data\n    self._boundaries = None\n    self._transcripts = None\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.boundaries","title":"boundaries  <code>cached</code> <code>property</code>","text":"<pre><code>boundaries\n</code></pre> <p>Return the filtered boundaries within the tile extents, cached for efficiency.</p> <p>The boundaries are computed only once and cached. If the boundaries have not been computed yet, they are computed using <code>get_filtered_boundaries()</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered boundaries within the tile extents.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.transcripts","title":"transcripts  <code>cached</code> <code>property</code>","text":"<pre><code>transcripts\n</code></pre> <p>Return the filtered transcripts within the tile extents, cached for efficiency.</p> <p>The transcripts are computed only once and cached. If the transcripts have not been computed yet, they are computed using <code>get_filtered_transcripts()</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered transcripts within the tile extents.</p>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.uid","title":"uid  <code>property</code>","text":"<pre><code>uid\n</code></pre> <p>Generate a unique identifier for the tile based on its extents.</p> <p>This UID is particularly useful for saving or indexing tiles in distributed processing environments.</p> <p>The UID is constructed using the minimum and maximum x and y coordinates of the tile's bounding box, representing its position and size in the sample.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique identifier string in the format 'x=_y=_w=_h=' where: - <code>&lt;x_min&gt;</code>: Minimum x-coordinate of the tile's extents. - <code>&lt;y_min&gt;</code>: Minimum y-coordinate of the tile's extents. - <code>&lt;width&gt;</code>: Width of the tile. - <code>&lt;height&gt;</code>: Height of the tile. Example <p>If the tile's extents are bounded by (x_min, y_min) = (100, 200) and (x_max, y_max) = (150, 250), the generated UID would be: 'x=100_y=200_w=50_h=50'</p>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.canonical_edges","title":"canonical_edges","text":"<pre><code>canonical_edges(edge_index)\n</code></pre> <p>Sort edge indices to ensure canonical ordering.</p> <p>Parameters:</p> Name Type Description Default <code>edge_index</code> <p>The edge index tensor to sort.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The sorted edge index tensor.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def canonical_edges(edge_index):\n    \"\"\"Sort edge indices to ensure canonical ordering.\n\n    Args:\n        edge_index: The edge index tensor to sort.\n\n    Returns:\n        torch.Tensor: The sorted edge index tensor.\n    \"\"\"\n    return torch.sort(edge_index, dim=0)[0]\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_boundary_props","title":"get_boundary_props","text":"<pre><code>get_boundary_props(area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Compute geometric properties of boundary polygons.</p> <p>Parameters:</p> Name Type Description Default <code>area</code> <code>bool</code> <p>If True, compute the area of each boundary polygon. Defaults to True.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>If True, compute the convexity of each boundary polygon. Defaults to True.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>If True, compute the elongation of each boundary polygon. Defaults to True.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>If True, compute the circularity of each boundary polygon. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the computed properties for each boundary polygon.</p> Note <p>The intention is for this function to simplify testing new strategies for 'bd' node representations. You can just change the function body to return another torch.Tensor without worrying about changes to the rest of the code.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_boundary_props(\n    self,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Compute geometric properties of boundary polygons.\n\n    Args:\n        area: If True, compute the area of each boundary polygon. Defaults to True.\n        convexity: If True, compute the convexity of each boundary polygon. Defaults to True.\n        elongation: If True, compute the elongation of each boundary polygon. Defaults to True.\n        circularity: If True, compute the circularity of each boundary polygon. Defaults to True.\n\n    Returns:\n        torch.Tensor: A tensor containing the computed properties for each boundary\n            polygon.\n\n    Note:\n        The intention is for this function to simplify testing new strategies\n        for 'bd' node representations. You can just change the function body to\n        return another torch.Tensor without worrying about changes to the rest\n        of the code.\n    \"\"\"\n    # Get polygons from coordinates\n    # Use getattr to check for the geometry column\n    geometry_column = getattr(self.settings.boundaries, 'geometry', None)\n    if geometry_column and geometry_column in self.boundaries.columns:\n        polygons = self.boundaries[geometry_column]\n    else:\n        polygons = self.boundaries['geometry']  # Assign None if the geometry column does not exist\n    # Geometric properties of polygons\n    props = self.get_polygon_props(polygons)\n    props = torch.as_tensor(props.values).float()\n\n    return props\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_filtered_boundaries","title":"get_filtered_boundaries","text":"<pre><code>get_filtered_boundaries()\n</code></pre> <p>Filter the boundaries in the sample to include only those within the specified tile extents.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered boundaries within the tile extents.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_filtered_boundaries(self) -&gt; pd.DataFrame:\n    \"\"\"Filter the boundaries in the sample to include only those within\n    the specified tile extents.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered boundaries within the tile\n            extents.\n    \"\"\"\n    filtered_boundaries = utils.filter_boundaries(\n        boundaries=self.dataset.boundaries,\n        inset=self.extents,\n        outset=self.extents.buffer(self.margin, join_style=\"mitre\"),\n        x=self.settings.boundaries.x,\n        y=self.settings.boundaries.y,\n        label=self.settings.boundaries.label,\n    )\n    return filtered_boundaries\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_filtered_transcripts","title":"get_filtered_transcripts","text":"<pre><code>get_filtered_transcripts()\n</code></pre> <p>Filter the transcripts in the sample to include only those within the specified tile extents.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the filtered transcripts within the tile extents.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_filtered_transcripts(self) -&gt; pd.DataFrame:\n    \"\"\"Filter the transcripts in the sample to include only those within\n    the specified tile extents.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the filtered transcripts within the tile\n            extents.\n    \"\"\"\n\n    # Buffer tile bounds to include transcripts around boundary\n    outset = self.extents.buffer(self.margin, join_style=\"mitre\")\n    xmin, ymin, xmax, ymax = outset.bounds\n\n    # Get transcripts inside buffered region\n    x, y = self.settings.transcripts.xy\n    mask = self.dataset.transcripts[x].between(xmin, xmax)\n    mask &amp;= self.dataset.transcripts[y].between(ymin, ymax)\n    filtered_transcripts = self.dataset.transcripts[mask]\n\n    return filtered_transcripts\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_kdtree_edge_index","title":"get_kdtree_edge_index  <code>staticmethod</code>","text":"<pre><code>get_kdtree_edge_index(index_coords, query_coords, k, max_distance)\n</code></pre> <p>Compute the k-nearest neighbor edge indices using a KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>index_coords</code> <code>ndarray</code> <p>An array of shape (n_samples, n_features) representing the coordinates of the points to be indexed.</p> required <code>query_coords</code> <code>ndarray</code> <p>An array of shape (m_samples, n_features) representing the coordinates of the query points.</p> required <code>k</code> <code>int</code> <p>The number of nearest neighbors to find for each query point.</p> required <code>max_distance</code> <code>float</code> <p>The maximum distance to consider for neighbors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: An array of shape (2, n_edges) containing the edge indices. Each column represents an edge between two points, where the first row contains the source indices and the second row contains the target indices.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>@staticmethod\ndef get_kdtree_edge_index(\n    index_coords: np.ndarray,\n    query_coords: np.ndarray,\n    k: int,\n    max_distance: float,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the k-nearest neighbor edge indices using a KDTree.\n\n    Args:\n        index_coords: An array of shape (n_samples, n_features) representing the\n            coordinates of the points to be indexed.\n        query_coords: An array of shape (m_samples, n_features) representing the\n            coordinates of the query points.\n        k: The number of nearest neighbors to find for each query point.\n        max_distance: The maximum distance to consider for neighbors.\n\n    Returns:\n        torch.Tensor: An array of shape (2, n_edges) containing the edge indices. Each\n            column represents an edge between two points, where the first row\n            contains the source indices and the second row contains the target\n            indices.\n    \"\"\"\n    # KDTree search\n    tree = KDTree(index_coords)\n    dist, idx = tree.query(query_coords, k, max_distance)\n\n    # To sparse adjacency\n    edge_index = np.argwhere(dist != np.inf).T\n    edge_index[1] = idx[dist != np.inf]\n    edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n\n    return edge_index\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_polygon_props","title":"get_polygon_props  <code>staticmethod</code>","text":"<pre><code>get_polygon_props(polygons, area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Compute geometric properties of polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>GeoSeries</code> <p>A GeoSeries containing polygon geometries.</p> required <code>area</code> <code>bool</code> <p>If True, compute the area of each polygon. Defaults to True.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>If True, compute the convexity of each polygon. Defaults to True.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>If True, compute the elongation of each polygon. Defaults to True.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>If True, compute the circularity of each polygon. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the computed properties for each polygon.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>@staticmethod\ndef get_polygon_props(\n    polygons: gpd.GeoSeries,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute geometric properties of polygons.\n\n    Args:\n        polygons: A GeoSeries containing polygon geometries.\n        area: If True, compute the area of each polygon. Defaults to True.\n        convexity: If True, compute the convexity of each polygon. Defaults to True.\n        elongation: If True, compute the elongation of each polygon. Defaults to True.\n        circularity: If True, compute the circularity of each polygon. Defaults to True.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the computed properties for each polygon.\n    \"\"\"\n    props = pd.DataFrame(index=polygons.index, dtype=float)\n    if area:\n        props[\"area\"] = polygons.area\n    if convexity:\n        props[\"convexity\"] = polygons.convex_hull.area / polygons.area\n    if elongation:\n        rects = polygons.minimum_rotated_rectangle()\n        props[\"elongation\"] = rects.area / polygons.envelope.area\n    if circularity:\n        r = polygons.minimum_bounding_radius()\n        props[\"circularity\"] = polygons.area / r**2\n\n    return props\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.get_transcript_props","title":"get_transcript_props","text":"<pre><code>get_transcript_props()\n</code></pre> <p>Encode transcript features in a sparse format.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A sparse tensor containing the encoded transcript features.</p> Note <p>The intention is for this function to simplify testing new strategies for 'tx' node representations. For example, the encoder can be any type of encoder that transforms the transcript labels into a numerical matrix (in sparse format).</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def get_transcript_props(self) -&gt; torch.Tensor:\n    \"\"\"Encode transcript features in a sparse format.\n\n    Returns:\n        torch.Tensor: A sparse tensor containing the encoded transcript features.\n\n    Note:\n        The intention is for this function to simplify testing new strategies\n        for 'tx' node representations. For example, the encoder can be any type\n        of encoder that transforms the transcript labels into a numerical\n        matrix (in sparse format).\n    \"\"\"\n    # Encode transcript features in sparse format\n    embedding = self.dataset.sample._transcript_embedding\n    label = self.settings.transcripts.label\n    props = embedding.embed(self.transcripts[label])\n\n    return props\n</code></pre>"},{"location":"api/data/sample/#src.segger.data.sample.STTile.to_pyg_dataset","title":"to_pyg_dataset","text":"<pre><code>to_pyg_dataset(neg_sampling_ratio=10, k_bd=3, dist_bd=15, k_tx=3, dist_tx=5, k_tx_ex=100, dist_tx_ex=20, area=True, convexity=True, elongation=True, circularity=True, mutually_exclusive_genes=None)\n</code></pre> <p>Convert the sample data to a PyG HeteroData object.</p> <p>Parameters:</p> Name Type Description Default <code>neg_sampling_ratio</code> <code>float</code> <p>Ratio of negative samples. Defaults to 10.</p> <code>10</code> <code>k_bd</code> <code>int</code> <p>Number of nearest neighbors for boundary nodes. Defaults to 3.</p> <code>3</code> <code>dist_bd</code> <code>float</code> <p>Maximum distance for boundary neighbors. Defaults to 15.</p> <code>15</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for transcript nodes. Defaults to 3.</p> <code>3</code> <code>dist_tx</code> <code>float</code> <p>Maximum distance for transcript neighbors. Defaults to 5.</p> <code>5</code> <code>k_tx_ex</code> <code>int</code> <p>Number of nearest neighbors for transcript exclusion. Defaults to 100.</p> <code>100</code> <code>dist_tx_ex</code> <code>float</code> <p>Maximum distance for transcript exclusion. Defaults to 20.</p> <code>20</code> <code>area</code> <code>bool</code> <p>If True, compute area of boundary polygons. Defaults to True.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>If True, compute convexity of boundary polygons. Defaults to True.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>If True, compute elongation of boundary polygons. Defaults to True.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>If True, compute circularity of boundary polygons. Defaults to True.</p> <code>True</code> <code>mutually_exclusive_genes</code> <code>Optional[List]</code> <p>List of mutually exclusive gene pairs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HeteroData</code> <code>HeteroData</code> <p>A PyTorch Geometric HeteroData object containing the sample data.</p> Source code in <code>src/segger/data/sample.py</code> <pre><code>def to_pyg_dataset(\n    self,\n    # train: bool,\n    neg_sampling_ratio: float = 10,\n    k_bd: int = 3,\n    dist_bd: float = 15,\n    k_tx: int = 3,\n    dist_tx: float = 5,\n    k_tx_ex: int = 100,\n    dist_tx_ex: float = 20,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n    mutually_exclusive_genes: Optional[List] = None,\n) -&gt; HeteroData:\n    \"\"\"Convert the sample data to a PyG HeteroData object.\n\n    Args:\n        neg_sampling_ratio: Ratio of negative samples. Defaults to 10.\n        k_bd: Number of nearest neighbors for boundary nodes. Defaults to 3.\n        dist_bd: Maximum distance for boundary neighbors. Defaults to 15.\n        k_tx: Number of nearest neighbors for transcript nodes. Defaults to 3.\n        dist_tx: Maximum distance for transcript neighbors. Defaults to 5.\n        k_tx_ex: Number of nearest neighbors for transcript exclusion. Defaults to 100.\n        dist_tx_ex: Maximum distance for transcript exclusion. Defaults to 20.\n        area: If True, compute area of boundary polygons. Defaults to True.\n        convexity: If True, compute convexity of boundary polygons. Defaults to True.\n        elongation: If True, compute elongation of boundary polygons. Defaults to True.\n        circularity: If True, compute circularity of boundary polygons. Defaults to True.\n        mutually_exclusive_genes: List of mutually exclusive gene pairs. Defaults to None.\n\n    Returns:\n        HeteroData: A PyTorch Geometric HeteroData object containing the sample data.\n    \"\"\"\n    # Initialize an empty HeteroData object\n    pyg_data = HeteroData()\n\n    # Set up Transcript nodes\n    # Get transcript IDs - use getattr to safely check for id attribute\n    transcript_id_column = getattr(self.settings.transcripts, \"id\", None)\n    if transcript_id_column is None:\n        raise ValueError(\n            \"Transcript IDs not found in DataFrame. Please run add_transcript_ids() \"\n            \"as a preprocessing step before creating the dataset.\"\n        )\n\n    # Assign IDs to PyG data\n    pyg_data[\"tx\"].id = torch.tensor(\n        self.transcripts[transcript_id_column].values, dtype=torch.long\n    )\n    pyg_data[\"tx\"].pos = torch.tensor(\n        self.transcripts[self.settings.transcripts.xyz].values,\n        dtype=torch.float32,\n    )\n    pyg_data[\"tx\"].x = self.get_transcript_props()\n\n\n\n    # Set up Transcript-Transcript neighbor edges\n    nbrs_edge_idx = self.get_kdtree_edge_index(\n        self.transcripts[self.settings.transcripts.xyz],\n        self.transcripts[self.settings.transcripts.xyz],\n        k=k_tx,\n        max_distance=dist_tx,\n    )\n\n    # If there are no tx-neighbors-tx edges, skip saving tile\n    if nbrs_edge_idx.shape[1] == 0:\n        return None\n\n    pyg_data[\"tx\", \"neighbors\", \"tx\"].edge_index = nbrs_edge_idx\n\n\n    if mutually_exclusive_genes is not None:\n        # Get potential repulsive edges (k-nearest neighbors within distance)\n        # --- Step 1: Get repulsive edges (mutually exclusive genes) ---\n        repels_edge_idx = self.get_kdtree_edge_index(\n            self.transcripts[self.settings.transcripts.xyz],\n            self.transcripts[self.settings.transcripts.xyz],\n            k=k_tx_ex,\n            max_distance=dist_tx_ex,\n        )\n        gene_ids = self.transcripts[self.settings.transcripts.label].tolist()\n\n        # Filter repels_edge_idx to only keep mutually exclusive gene pairs\n        src_genes = [gene_ids[i] for i in repels_edge_idx[0].tolist()]\n        dst_genes = [gene_ids[i] for i in repels_edge_idx[1].tolist()]\n        mask = [\n            tuple(sorted((a, b))) in mutually_exclusive_genes if a != b else False\n            for a, b in zip(src_genes, dst_genes)\n        ]\n        repels_edge_idx = repels_edge_idx[:, torch.tensor(mask)]\n\n        # --- Step 2: Get attractive edges (same gene, at least one node in repels) ---\n        # Nodes involved in repels (for filtering nbrs_edge_idx)\n        repels_nodes = torch.cat([repels_edge_idx[0], repels_edge_idx[1]]).unique()\n\n        # Filter nbrs_edge_idx: keep edges where (1) same gene AND (2) at least one node in repels\n        attractive_mask = torch.zeros(nbrs_edge_idx.shape[1], dtype=torch.bool)\n        for i, (src, dst) in enumerate(nbrs_edge_idx.t().tolist()):\n            if (src != dst) and (gene_ids[src] == gene_ids[dst]) and (src in repels_nodes or dst in repels_nodes):\n                attractive_mask[i] = True\n        attractive_edge_idx = nbrs_edge_idx[:, attractive_mask]\n\n        # --- Step 3: Combine repels (label=0) and attractive (label=1) edges ---\n        edge_label_index = torch.cat([repels_edge_idx, attractive_edge_idx], dim=1)\n        edge_label = torch.cat([\n            torch.zeros(repels_edge_idx.shape[1], dtype=torch.long),  # 0 for repels\n            torch.ones(attractive_edge_idx.shape[1], dtype=torch.long)  # 1 for attracts\n        ])\n\n        # --- Step 4: Store in PyG data object ---\n        pyg_data[\"tx\", \"attracts\", \"tx\"].edge_label_index = edge_label_index\n        pyg_data[\"tx\", \"attracts\", \"tx\"].edge_label = edge_label\n\n\n    # Set up Boundary nodes\n    # Check if boundaries have geometries\n    geometry_column = getattr(self.settings.boundaries, 'geometry', None)\n    if geometry_column and geometry_column in self.boundaries.columns:\n        polygons = gpd.GeoSeries(self.boundaries[geometry_column], index=self.boundaries.index)\n    else:\n        # Fallback: compute polygons\n        polygons = utils.get_polygons_from_xy(\n            self.boundaries,\n            x=self.settings.boundaries.x,\n            y=self.settings.boundaries.y,\n            label=self.settings.boundaries.label,\n            scale_factor=self.settings.boundaries.scale_factor,\n        )\n\n    # Ensure self.boundaries is a GeoDataFrame with correct geometry\n    self.boundaries = gpd.GeoDataFrame(self.boundaries.copy(), geometry=polygons)\n    centroids = polygons.centroid.get_coordinates()\n    pyg_data[\"bd\"].id = polygons.index.to_numpy()\n    pyg_data[\"bd\"].pos = torch.tensor(centroids.values, dtype=torch.float32)\n    pyg_data[\"bd\"].x = self.get_boundary_props(\n        area, convexity, elongation, circularity\n    )\n\n    # Set up Boundary-Transcript neighbor edges\n    dist = np.sqrt(polygons.area.max()) * 10  # heuristic distance\n    nbrs_edge_idx = self.get_kdtree_edge_index(\n        centroids,\n        self.transcripts[self.settings.transcripts.xy],\n        k=k_bd,\n        max_distance=dist,\n    )\n    pyg_data[\"tx\", \"neighbors\", \"bd\"].edge_index = nbrs_edge_idx\n\n    # If there are no tx-neighbors-bd edges, we put the tile automatically in test set\n    if nbrs_edge_idx.numel() == 0:\n        # logging.warning(f\"No tx-neighbors-bd edges found in tile {self.uid}.\")\n        pyg_data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.tensor(\n            [], dtype=torch.long\n        )\n        return pyg_data\n\n    # Now we identify and split the tx-belongs-bd edges\n    edge_type = (\"tx\", \"belongs\", \"bd\")\n\n    # Find nuclear transcripts\n    tx_cell_ids = self.transcripts[self.settings.boundaries.id]\n    cell_ids_map = {idx: i for (i, idx) in enumerate(polygons.index)}\n\n    # Get nuclear column and value from settings\n    nuclear_column = getattr(self.settings.transcripts, \"nuclear_column\", None)\n    nuclear_value = getattr(self.settings.transcripts, \"nuclear_value\", None)\n\n    if nuclear_column is None or self.settings.boundaries.scale_factor != 1.0:\n        is_nuclear = utils.compute_nuclear_transcripts(\n            polygons=polygons,\n            transcripts=self.transcripts,\n            x_col=self.settings.transcripts.x,\n            y_col=self.settings.transcripts.y,\n            nuclear_column=nuclear_column,\n            nuclear_value=nuclear_value,\n        )\n    else:\n        is_nuclear = self.transcripts[nuclear_column].eq(nuclear_value)\n    is_nuclear &amp;= tx_cell_ids.isin(polygons.index)\n\n    # # Set up overlap edges\n    # row_idx = np.where(is_nuclear)[0]\n    # col_idx = tx_cell_ids.iloc[row_idx].map(cell_ids_map)\n    # blng_edge_idx = torch.tensor(np.stack([row_idx, col_idx])).long()\n    # pyg_data[edge_type].edge_index = blng_edge_idx\n\n    # # If there are no tx-belongs-bd edges, flag tile as test only (cannot be used for training)\n    # if blng_edge_idx.numel() == 0:\n    #     return pyg_data\n\n    #         # If there are tx-bd edges, add negative edges for training\n    # transform = RandomLinkSplit(\n    #     num_val=0,\n    #     num_test=0,\n    #     is_undirected=True,\n    #     edge_types=[edge_type],\n    #     neg_sampling_ratio=neg_sampling_ratio,\n    # )\n    # pyg_data, _, _ = transform(pyg_data)\n\n    # # Refilter negative edges to include only transcripts in the\n    # # original positive edges (still need a memory-efficient solution)\n    # edges = pyg_data[edge_type]\n    # mask = edges.edge_label_index[0].unsqueeze(1) == edges.edge_index[0].unsqueeze(\n    #     0\n    # )\n    # mask = torch.nonzero(torch.any(mask, 1)).squeeze()\n    # edges.edge_label_index = edges.edge_label_index[:, mask]\n    # edges.edge_label = edges.edge_label[mask]\n\n    # return pyg_data\n\n\n    # Set up overlap edges\n    row_idx = np.where(is_nuclear)[0]\n    col_idx = tx_cell_ids.iloc[row_idx].map(cell_ids_map)\n    blng_edge_idx = torch.tensor(np.stack([row_idx, col_idx])).long()\n    pyg_data[edge_type].edge_index = blng_edge_idx\n\n    # If there are no tx-belongs-bd edges, flag tile as test only (cannot be used for training)\n    if blng_edge_idx.numel() == 0:\n        return pyg_data\n\n    # If there are tx-bd edges, add negative edges for training\n    pos_edges = blng_edge_idx  # shape (2, num_pos)\n    num_pos = pos_edges.shape[1]\n\n    # Negative edges (tx-neighbors-bd) - EXCLUDE positives\n    neg_candidates = nbrs_edge_idx  # shape (2, num_candidates)\n\n    # --- Fast Negative Filtering (PyTorch-only) ---\n    # Reshape edges for broadcasting: (2, num_pos) vs (2, num_candidates, 1)\n    pos_expanded = pos_edges.unsqueeze(2)  # shape (2, num_pos, 1)\n    neg_expanded = neg_candidates.unsqueeze(1)  # shape (2, 1, num_candidates)\n\n    # Compare all edges in one go (broadcasting)\n    matches = (pos_expanded == neg_expanded).all(dim=0)  # shape (num_pos, num_candidates)\n    is_negative = ~matches.any(dim=0)  # shape (num_candidates,)\n\n    # Filter negatives\n    neg_edges = neg_candidates[:, is_negative]  # shape (2, num_filtered_neg)\n    num_neg = neg_edges.shape[1]\n\n    # --- Combine and label ---\n    edge_label_index = torch.cat([neg_edges, pos_edges], dim=1)\n    edge_label = torch.cat([\n        torch.zeros(num_neg, dtype=torch.float),\n        torch.ones(num_pos, dtype=torch.float)\n    ])\n\n    mask = edge_label_index[0].unsqueeze(1) == blng_edge_idx[0].unsqueeze(0)\n    mask = torch.nonzero(torch.any(mask, 1)).squeeze()\n    edge_label_index = edge_label_index[:, mask]\n    edge_label = edge_label[mask]\n\n    pyg_data[edge_type].edge_label_index = edge_label_index\n    pyg_data[edge_type].edge_label = edge_label\n\n    return pyg_data\n</code></pre>"},{"location":"api/data/sample/#usage-examples","title":"Usage Examples","text":""},{"location":"api/data/sample/#basic-data-loading","title":"Basic Data Loading","text":"<pre><code>from segger.data.sample import STSampleParquet\n\n# Load a spatial transcriptomics sample\nsample = STSampleParquet(\n    base_dir=\"/path/to/xenium/data\",\n    n_workers=4,\n    sample_type=\"xenium\"\n)\n\n# Get sample information\nprint(f\"Transcripts: {sample.n_transcripts}\")\nprint(f\"Spatial extents: {sample.extents}\")\nprint(f\"Feature names: {sample.transcripts_metadata['feature_names'][:5]}\")\n</code></pre>"},{"location":"api/data/sample/#spatial-tiling-and-processing","title":"Spatial Tiling and Processing","text":"<pre><code># Save processed tiles\nsample.save(\n    data_dir=\"./processed_data\",\n    tile_size=1000,  # 1000 transcripts per tile\n    k_bd=3,          # 3 boundary neighbors\n    k_tx=5,          # 5 transcript neighbors\n    dist_bd=15.0,    # 15 pixel boundary distance\n    dist_tx=5.0,     # 5 pixel transcript distance\n    frac=0.8,        # Process 80% of data\n    val_prob=0.1,    # 10% validation\n    test_prob=0.2    # 20% test\n)\n</code></pre>"},{"location":"api/data/sample/#in-memory-dataset-processing","title":"In-Memory Dataset Processing","text":"<pre><code>from segger.data.sample import STInMemoryDataset\n\n# Create dataset for a specific region\ndataset = STInMemoryDataset(\n    sample=sample,\n    extents=region_polygon,\n    margin=10\n)\n\n# Generate tiles\ntiles = dataset._tile(\n    width=100,    # 100 pixel width\n    height=100    # 100 pixel height\n)\n\nprint(f\"Generated {len(tiles)} tiles\")\n</code></pre>"},{"location":"api/data/sample/#individual-tile-processing","title":"Individual Tile Processing","text":"<pre><code>from segger.data.sample import STTile\n\n# Process individual tile\ntile = STTile(dataset=dataset, extents=tile_polygon)\n\n# Get tile data\ntranscripts = tile.transcripts\nboundaries = tile.boundaries\n\n# Convert to PyG format\npyg_data = tile.to_pyg_dataset(\n    k_bd=3,\n    dist_bd=15,\n    k_tx=5,\n    dist_tx=5,\n    area=True,\n    convexity=True,\n    elongation=True,\n    circularity=True\n)\n\nprint(f\"Tile UID: {tile.uid}\")\nprint(f\"Transcripts: {len(transcripts)}\")\nprint(f\"Boundaries: {len(boundaries)}\")\n</code></pre>"},{"location":"api/data/transcript_embedding/","title":"segger.data.transcript_embedding","text":"<p>The <code>transcript_embedding</code> module provides utilities for encoding transcript features into numerical representations suitable for machine learning models. This module handles the conversion of gene names and transcript labels into embeddings that can be used in graph neural networks.</p>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding","title":"TranscriptEmbedding","text":"<p>               Bases: <code>Module</code></p> <p>Utility class to handle transcript embeddings in PyTorch so that they are optionally learnable in the future.</p> <p>Default behavior is to use the index of gene names.</p> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>class TranscriptEmbedding(torch.nn.Module):\n    \"\"\"Utility class to handle transcript embeddings in PyTorch so that they are\n    optionally learnable in the future.\n\n    Default behavior is to use the index of gene names.\n    \"\"\"\n\n    # TODO: Add documentation\n    @staticmethod\n    def _check_inputs(\n        classes: ArrayLike,\n        weights: Union[pd.DataFrame, None],\n    ):\n        \"\"\"Check input arguments for validity.\n\n        Args:\n            classes: A 1D array of unique class names.\n            weights: Optional DataFrame containing weights for each class. Defaults to None.\n\n        Raises:\n            ValueError: If classes is not 1D, contains duplicates, or if weights DataFrame\n                is missing entries for some classes.\n        \"\"\"\n        # Classes is a 1D array\n        if len(classes.shape) &gt; 1:\n            msg = (\n                \"'classes' should be a 1D array, got an array of shape \"\n                f\"{classes.shape} instead.\"\n            )\n            raise ValueError(msg)\n        # Items appear exactly once\n        if len(classes) != len(set(classes)):\n            msg = (\n                \"All embedding classes must be unique. One or more items in \"\n                \"'classes' appears twice.\"\n            )\n            raise ValueError(msg)\n        # All classes have an entry in weights\n        elif weights is not None:\n            missing = set(classes).difference(weights.index)\n            if len(missing) &gt; 0:\n                msg = (\n                    f\"Index of 'weights' DataFrame is missing {len(missing)} \"\n                    \"entries compared to classes.\"\n                )\n                raise ValueError(msg)\n\n    # TODO: Add documentation\n    def __init__(\n        self,\n        classes: ArrayLike,\n        weights: Optional[pd.DataFrame] = None,\n    ):\n        \"\"\"Initialize the TranscriptEmbedding module.\n\n        Args:\n            classes: A 1D array of unique class names.\n            weights: Optional DataFrame containing weights for each class. Defaults to None.\n        \"\"\"\n        # check input arguments\n        TranscriptEmbedding._check_inputs(classes, weights)\n        # Setup as PyTorch module\n        super(TranscriptEmbedding, self).__init__()\n        self._encoder = LabelEncoder().fit(classes)\n        if weights is None:\n            self._weights = None\n        else:\n            self._weights = Tensor(weights.loc[classes].values)\n\n    # TODO: Add documentation\n    def embed(self, classes: ArrayLike):\n        \"\"\"Embed transcript classes into numerical representations.\n\n        Args:\n            classes: Array of class names to embed.\n\n        Returns:\n            Union[LongTensor, torch.Tensor]: If no weights provided, returns indices.\n                If weights provided, returns embedded representations.\n        \"\"\"\n        indices = LongTensor(self._encoder.transform(classes))\n        # Default, one-hot encoding\n        if self._weights is None:\n            return indices  # F.one_hot(indices, len(self._encoder.classes_))\n        else:\n            return F.embedding(indices, self._weights)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding._encoder","title":"_encoder  <code>instance-attribute</code>","text":"<pre><code>_encoder = fit(classes)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding._weights","title":"_weights  <code>instance-attribute</code>","text":"<pre><code>_weights = None\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(classes, weights=None)\n</code></pre> <p>Initialize the TranscriptEmbedding module.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>ArrayLike</code> <p>A 1D array of unique class names.</p> required <code>weights</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame containing weights for each class. Defaults to None.</p> <code>None</code> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>def __init__(\n    self,\n    classes: ArrayLike,\n    weights: Optional[pd.DataFrame] = None,\n):\n    \"\"\"Initialize the TranscriptEmbedding module.\n\n    Args:\n        classes: A 1D array of unique class names.\n        weights: Optional DataFrame containing weights for each class. Defaults to None.\n    \"\"\"\n    # check input arguments\n    TranscriptEmbedding._check_inputs(classes, weights)\n    # Setup as PyTorch module\n    super(TranscriptEmbedding, self).__init__()\n    self._encoder = LabelEncoder().fit(classes)\n    if weights is None:\n        self._weights = None\n    else:\n        self._weights = Tensor(weights.loc[classes].values)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding._check_inputs","title":"_check_inputs  <code>staticmethod</code>","text":"<pre><code>_check_inputs(classes, weights)\n</code></pre> <p>Check input arguments for validity.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>ArrayLike</code> <p>A 1D array of unique class names.</p> required <code>weights</code> <code>Union[DataFrame, None]</code> <p>Optional DataFrame containing weights for each class. Defaults to None.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If classes is not 1D, contains duplicates, or if weights DataFrame is missing entries for some classes.</p> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>@staticmethod\ndef _check_inputs(\n    classes: ArrayLike,\n    weights: Union[pd.DataFrame, None],\n):\n    \"\"\"Check input arguments for validity.\n\n    Args:\n        classes: A 1D array of unique class names.\n        weights: Optional DataFrame containing weights for each class. Defaults to None.\n\n    Raises:\n        ValueError: If classes is not 1D, contains duplicates, or if weights DataFrame\n            is missing entries for some classes.\n    \"\"\"\n    # Classes is a 1D array\n    if len(classes.shape) &gt; 1:\n        msg = (\n            \"'classes' should be a 1D array, got an array of shape \"\n            f\"{classes.shape} instead.\"\n        )\n        raise ValueError(msg)\n    # Items appear exactly once\n    if len(classes) != len(set(classes)):\n        msg = (\n            \"All embedding classes must be unique. One or more items in \"\n            \"'classes' appears twice.\"\n        )\n        raise ValueError(msg)\n    # All classes have an entry in weights\n    elif weights is not None:\n        missing = set(classes).difference(weights.index)\n        if len(missing) &gt; 0:\n            msg = (\n                f\"Index of 'weights' DataFrame is missing {len(missing)} \"\n                \"entries compared to classes.\"\n            )\n            raise ValueError(msg)\n</code></pre>"},{"location":"api/data/transcript_embedding/#src.segger.data.transcript_embedding.TranscriptEmbedding.embed","title":"embed","text":"<pre><code>embed(classes)\n</code></pre> <p>Embed transcript classes into numerical representations.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>ArrayLike</code> <p>Array of class names to embed.</p> required <p>Returns:</p> Type Description <p>Union[LongTensor, torch.Tensor]: If no weights provided, returns indices. If weights provided, returns embedded representations.</p> Source code in <code>src/segger/data/transcript_embedding.py</code> <pre><code>def embed(self, classes: ArrayLike):\n    \"\"\"Embed transcript classes into numerical representations.\n\n    Args:\n        classes: Array of class names to embed.\n\n    Returns:\n        Union[LongTensor, torch.Tensor]: If no weights provided, returns indices.\n            If weights provided, returns embedded representations.\n    \"\"\"\n    indices = LongTensor(self._encoder.transform(classes))\n    # Default, one-hot encoding\n    if self._weights is None:\n        return indices  # F.one_hot(indices, len(self._encoder.classes_))\n    else:\n        return F.embedding(indices, self._weights)\n</code></pre>"},{"location":"api/data/transcript_embedding/#overview","title":"Overview","text":"<p>The <code>TranscriptEmbedding</code> class is designed to handle transcript feature encoding in a flexible and extensible way. It supports both simple index-based encoding and weighted embeddings, making it suitable for various machine learning applications.</p>"},{"location":"api/data/transcript_embedding/#usage-examples","title":"Usage Examples","text":""},{"location":"api/data/transcript_embedding/#basic-index-based-encoding","title":"Basic Index-based Encoding","text":"<pre><code>from segger.data.transcript_embedding import TranscriptEmbedding\nimport pandas as pd\n\n# Create a list of gene names\ngene_names = [\"GENE1\", \"GENE2\", \"GENE3\", \"GENE4\"]\n\n# Initialize embedding without weights (index-based)\nembedding = TranscriptEmbedding(classes=gene_names)\n\n# Encode transcript labels\ntranscript_labels = [\"GENE1\", \"GENE3\", \"GENE2\"]\nencoded = embedding.embed(transcript_labels)\n# Returns: tensor([0, 2, 1])\n</code></pre>"},{"location":"api/data/transcript_embedding/#weighted-embeddings","title":"Weighted Embeddings","text":"<pre><code>import pandas as pd\n\n# Create weights DataFrame\nweights_df = pd.DataFrame({\n    'weight1': [0.1, 0.2, 0.3, 0.4],\n    'weight2': [0.5, 0.6, 0.7, 0.8]\n}, index=[\"GENE1\", \"GENE2\", \"GENE3\", \"GENE4\"])\n\n# Initialize embedding with weights\nembedding = TranscriptEmbedding(\n    classes=gene_names,\n    weights=weights_df\n)\n\n# Encode transcript labels\ntranscript_labels = [\"GENE1\", \"GENE3\"]\nencoded = embedding.embed(transcript_labels)\n# Returns: tensor([[0.1, 0.5], [0.3, 0.7]])\n</code></pre>"},{"location":"api/data/transcript_embedding/#integration-with-pytorch","title":"Integration with PyTorch","text":"<pre><code>import torch\nfrom segger.data.transcript_embedding import TranscriptEmbedding\n\n# Create embedding module\nembedding = TranscriptEmbedding(classes=gene_names)\n\n# Use in a neural network\nclass TranscriptEncoder(torch.nn.Module):\n    def __init__(self, gene_names):\n        super().__init__()\n        self.embedding = TranscriptEmbedding(gene_names)\n        self.projection = torch.nn.Linear(len(gene_names), 128)\n\n    def forward(self, transcript_labels):\n        encoded = self.embedding.embed(transcript_labels)\n        projected = self.projection(encoded)\n        return projected\n\n# Initialize and use\nencoder = TranscriptEncoder(gene_names)\noutput = encoder([\"GENE1\", \"GENE2\"])\n</code></pre>"},{"location":"api/data/constants/","title":"segger.data.constants","text":""},{"location":"api/data/constants/#segger.data.constants.MerscopeKeys","title":"MerscopeKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/constants/#segger.data.constants.SpatialDataKeys","title":"SpatialDataKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/constants/#segger.data.constants.SpatialTranscriptomicsKeys","title":"SpatialTranscriptomicsKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Unified keys for spatial transcriptomics data, supporting multiple platforms.</p>"},{"location":"api/data/constants/#segger.data.constants.XeniumKeys","title":"XeniumKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for 10X Genomics Xenium formatted dataset.</p>"},{"location":"api/data/io/","title":"segger.data.io","text":""},{"location":"api/data/io/#segger.data.io.MerscopeKeys","title":"MerscopeKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/io/#segger.data.io.MerscopeSample","title":"MerscopeSample","text":"<pre><code>MerscopeSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(\n        transcripts_df,\n        transcripts_radius,\n        boundaries_graph,\n        embedding_df,\n        MerscopeKeys,\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.MerscopeSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on specific criteria for Merscope using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <p>dd.DataFrame The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <p>float, optional The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame The filtered Dask DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def filter_transcripts(\n    self, transcripts_df: dd.DataFrame, min_qv: float = 20.0\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on specific criteria for Merscope using Dask.\n\n    Parameters:\n        transcripts_df : dd.DataFrame\n            The Dask DataFrame containing transcript data.\n        min_qv : float, optional\n            The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame\n            The filtered Dask DataFrame.\n    \"\"\"\n    # Add custom Merscope-specific filtering logic if needed\n    # For now, apply only the quality value filter\n    return transcripts_df[transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialDataKeys","title":"SpatialDataKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for MERSCOPE data (Vizgen platform).</p>"},{"location":"api/data/io/#segger.data.io.SpatialDataSample","title":"SpatialDataSample","text":"<pre><code>SpatialDataSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, feature_name=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    feature_name: str | None = None,\n    verbose: bool = True,\n):\n    if feature_name is not None:\n        # luca: just a quick hack for now, I propose to use dataclasses instead of enums to address this\n        SpatialDataKeys.FEATURE_NAME._value_ = feature_name\n    else:\n        raise ValueError(\n            \"the automatic determination of a feature_name from a SpatialData object is not enabled yet\"\n        )\n\n    super().__init__(\n        transcripts_df,\n        transcripts_radius,\n        boundaries_graph,\n        embedding_df,\n        SpatialDataKeys,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialDataSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def filter_transcripts(\n    self, transcripts_df: dd.DataFrame, min_qv: float = 20.0\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[\n            self.keys.FEATURE_NAME.value\n        ].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(\n        filter_codewords\n    )\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>segger/data/utils.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    transform: Callable = None,\n    pre_transform: Callable = None,\n    pre_filter: Callable = None,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(\n        os.path.join(self.processed_dir, self.processed_file_names[idx])\n    )\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsKeys","title":"SpatialTranscriptomicsKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Unified keys for spatial transcriptomics data, supporting multiple platforms.</p>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample","title":"SpatialTranscriptomicsSample","text":"<pre><code>SpatialTranscriptomicsSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, keys=None, verbose=True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Initialize the SpatialTranscriptomicsSample class.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>A DataFrame containing transcript data.</p> <code>None</code> <code>transcripts_radius</code> <code>int</code> <p>Radius for transcripts in the analysis.</p> <code>10</code> <code>boundaries_graph</code> <code>bool</code> <p>Whether to include boundaries (e.g., nucleus, cell) graph information.</p> <code>False</code> <code>keys</code> <code>Dict</code> <p>The enum class containing key mappings specific to the dataset.</p> <code>None</code> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: pd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    keys: Dict = None,\n    verbose: bool = True,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsSample class.\n\n    Args:\n        transcripts_df (pd.DataFrame, optional): A DataFrame containing transcript data.\n        transcripts_radius (int, optional): Radius for transcripts in the analysis.\n        boundaries_graph (bool, optional): Whether to include boundaries (e.g., nucleus, cell) graph information.\n        keys (Dict, optional): The enum class containing key mappings specific to the dataset.\n    \"\"\"\n    self.transcripts_df = transcripts_df\n    self.transcripts_radius = transcripts_radius\n    self.boundaries_graph = boundaries_graph\n    self.keys = keys\n    self.embedding_df = embedding_df\n    self.current_embedding = \"token\"\n    self.verbose = verbose\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.build_pyg_data_from_tile","title":"build_pyg_data_from_tile","text":"<pre><code>build_pyg_data_from_tile(boundaries_df, transcripts_df, r_tx=5.0, k_tx=3, method='kd_tree', gpu=False, workers=1, scale_boundaries=1.0)\n</code></pre> <p>Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundaries data (e.g., nucleus, cell).</p> required <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcripts data.</p> required <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5.0</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>HeteroData</code> <code>HeteroData</code> <p>PyG Heterogeneous Data object.</p> Source code in <code>segger/data/io.py</code> <pre><code>def build_pyg_data_from_tile(\n    self,\n    boundaries_df: dd.DataFrame,\n    transcripts_df: dd.DataFrame,\n    r_tx: float = 5.0,\n    k_tx: int = 3,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n    scale_boundaries: float = 1.0,\n) -&gt; HeteroData:\n    \"\"\"\n    Builds PyG data from a tile of boundaries and transcripts data using Dask utilities for efficient processing.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): Dask DataFrame containing boundaries data (e.g., nucleus, cell).\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcripts data.\n        r_tx (float): Radius for building the transcript-to-transcript graph.\n        k_tx (int): Number of nearest neighbors for the tx-tx graph.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        HeteroData: PyG Heterogeneous Data object.\n    \"\"\"\n    # Initialize the PyG HeteroData object\n    data = HeteroData()\n\n    # Lazily compute boundaries geometries using Dask\n    if self.verbose:\n        print(\"Computing boundaries geometries...\")\n    bd_gdf = self.compute_boundaries_geometries(\n        boundaries_df, scale_factor=scale_boundaries\n    )\n    bd_gdf = bd_gdf[bd_gdf[\"geometry\"].notnull()]\n\n    # Add boundary node data to PyG HeteroData lazily\n    data[\"bd\"].id = bd_gdf[self.keys.CELL_ID.value].values\n    data[\"bd\"].pos = torch.as_tensor(\n        bd_gdf[[\"centroid_x\", \"centroid_y\"]].values.astype(float)\n    )\n\n    if data[\"bd\"].pos.isnan().any():\n        raise ValueError(data[\"bd\"].id[data[\"bd\"].pos.isnan().any(1)])\n\n    bd_x = bd_gdf.iloc[:, 4:]\n    data[\"bd\"].x = torch.as_tensor(bd_x.to_numpy(), dtype=torch.float32)\n\n    # Extract the transcript coordinates lazily\n    if self.verbose:\n        print(\"Preparing transcript features and positions...\")\n    x_xyz = transcripts_df[\n        [self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]\n    ].to_numpy()\n    data[\"tx\"].id = torch.as_tensor(\n        transcripts_df[self.keys.TRANSCRIPTS_ID.value].values.astype(int)\n    )\n    data[\"tx\"].pos = torch.tensor(x_xyz, dtype=torch.float32)\n\n    # Lazily prepare transcript embeddings (if available)\n    if self.verbose:\n        print(\"Preparing transcript embeddings..\")\n    token_encoding = self.tx_encoder.transform(\n        transcripts_df[self.keys.FEATURE_NAME.value]\n    )\n    transcripts_df[\"token\"] = (\n        token_encoding  # Store the integer tokens in the 'token' column\n    )\n    data[\"tx\"].token = torch.as_tensor(token_encoding).int()\n    # Handle additional embeddings lazily as well\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        embeddings = delayed(\n            lambda df: self.embedding_df.loc[\n                df[self.keys.FEATURE_NAME.value].values\n            ].values\n        )(transcripts_df)\n    else:\n        embeddings = token_encoding\n    if hasattr(embeddings, \"compute\"):\n        embeddings = embeddings.compute()\n    x_features = torch.as_tensor(embeddings).int()\n    data[\"tx\"].x = x_features\n\n    # Check if the overlap column exists, if not, compute it lazily using Dask\n    if self.keys.OVERLAPS_BOUNDARY.value not in transcripts_df.columns:\n        if self.verbose:\n            print(f\"Computing overlaps for transcripts...\")\n        transcripts_df = self.compute_transcript_overlap_with_boundaries(\n            transcripts_df, polygons_gdf=bd_gdf, scale_factor=1.0\n        )\n\n    # Connect transcripts with their corresponding boundaries (e.g., nuclei, cells)\n    if self.verbose:\n        print(\"Connecting transcripts with boundaries...\")\n    overlaps = transcripts_df[self.keys.OVERLAPS_BOUNDARY.value].values\n    valid_cell_ids = bd_gdf[self.keys.CELL_ID.value].values\n    ind = np.where(\n        overlaps &amp; transcripts_df[self.keys.CELL_ID.value].isin(valid_cell_ids)\n    )[0]\n    tx_bd_edge_index = np.column_stack(\n        (\n            ind,\n            np.searchsorted(\n                valid_cell_ids, transcripts_df.iloc[ind][self.keys.CELL_ID.value]\n            ),\n        )\n    )\n\n    # Add transcript-boundary edge index to PyG HeteroData\n    data[\"tx\", \"belongs\", \"bd\"].edge_index = torch.as_tensor(\n        tx_bd_edge_index.T, dtype=torch.long\n    )\n\n    # Compute transcript-to-transcript (tx-tx) edges using Dask (lazy computation)\n    if self.verbose:\n        print(\"Computing tx-tx edges...\")\n    tx_positions = transcripts_df[\n        [self.keys.TRANSCRIPTS_X.value, self.keys.TRANSCRIPTS_Y.value]\n    ].values\n    delayed_tx_edge_index = delayed(get_edge_index)(\n        tx_positions,\n        tx_positions,\n        k=k_tx,\n        dist=r_tx,\n        method=method,\n        gpu=gpu,\n        workers=workers,\n    )\n    tx_edge_index = delayed_tx_edge_index.compute()\n\n    # Add the tx-tx edge index to the PyG HeteroData object\n    data[\"tx\", \"neighbors\", \"tx\"].edge_index = torch.as_tensor(\n        tx_edge_index.T, dtype=torch.long\n    )\n\n    if self.verbose:\n        print(\"Finished building PyG data for the tile.\")\n    return data\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.compute_boundaries_geometries","title":"compute_boundaries_geometries","text":"<pre><code>compute_boundaries_geometries(boundaries_df=None, polygons_gdf=None, scale_factor=1.0, area=True, convexity=True, elongation=True, circularity=True)\n</code></pre> <p>Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>The dataframe containing boundaries data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <code>area</code> <code>bool</code> <p>Whether to compute area.</p> <code>True</code> <code>convexity</code> <code>bool</code> <p>Whether to compute convexity.</p> <code>True</code> <code>elongation</code> <code>bool</code> <p>Whether to compute elongation.</p> <code>True</code> <code>circularity</code> <code>bool</code> <p>Whether to compute circularity.</p> <code>True</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.</p> Source code in <code>segger/data/io.py</code> <pre><code>def compute_boundaries_geometries(\n    self,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n    area: bool = True,\n    convexity: bool = True,\n    elongation: bool = True,\n    circularity: bool = True,\n) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Computes geometries for boundaries (e.g., nuclei, cells) from the dataframe using Dask.\n\n    Parameters:\n        boundaries_df (dd.DataFrame, optional): The dataframe containing boundaries data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n        area (bool, optional): Whether to compute area.\n        convexity (bool, optional): Whether to compute convexity.\n        elongation (bool, optional): Whether to compute elongation.\n        circularity (bool, optional): Whether to compute circularity.\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing computed geometries.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\n                \"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\"\n            )\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        if self.verbose:\n            print(\n                f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\"\n            )\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    # Check if the generated polygons_gdf is empty\n    if polygons_gdf.shape[0] == 0:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(\n                f\"Polygons are available. Proceeding with geometrical computations.\"\n            )\n\n    # Compute additional geometrical properties\n    polygons = polygons_gdf.geometry\n\n    # Compute additional geometrical properties\n    if area:\n        if self.verbose:\n            print(\"Computing area...\")\n        polygons_gdf[\"area\"] = polygons.area\n    if convexity:\n        if self.verbose:\n            print(\"Computing convexity...\")\n        polygons_gdf[\"convexity\"] = polygons.convex_hull.area / polygons.area\n    if elongation:\n        if self.verbose:\n            print(\"Computing elongation...\")\n        r = polygons.minimum_rotated_rectangle()\n        polygons_gdf[\"elongation\"] = (r.length * r.length) / r.area\n    if circularity:\n        if self.verbose:\n            print(\"Computing circularity...\")\n        r = polygons_gdf.minimum_bounding_radius()\n        polygons_gdf[\"circularity\"] = polygons.area / (r * r)\n\n    if self.verbose:\n        print(\"Geometrical computations completed.\")\n\n    return polygons_gdf.reset_index(drop=True)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.compute_transcript_overlap_with_boundaries","title":"compute_transcript_overlap_with_boundaries","text":"<pre><code>compute_transcript_overlap_with_boundaries(transcripts_df, boundaries_df=None, polygons_gdf=None, scale_factor=1.0)\n</code></pre> <p>Computes the overlap of transcript locations with scaled boundary polygons and assigns corresponding cell IDs to the transcripts using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>Dask DataFrame containing transcript data.</p> required <code>boundaries_df</code> <code>DataFrame</code> <p>Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.</p> <code>None</code> <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.</p> Source code in <code>segger/data/io.py</code> <pre><code>def compute_transcript_overlap_with_boundaries(\n    self,\n    transcripts_df: dd.DataFrame,\n    boundaries_df: dd.DataFrame = None,\n    polygons_gdf: dgpd.GeoDataFrame = None,\n    scale_factor: float = 1.0,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Computes the overlap of transcript locations with scaled boundary polygons\n    and assigns corresponding cell IDs to the transcripts using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): Dask DataFrame containing transcript data.\n        boundaries_df (dd.DataFrame, optional): Dask DataFrame containing boundary data. Required if polygons_gdf is not provided.\n        polygons_gdf (dgpd.GeoDataFrame, optional): Precomputed Dask GeoDataFrame containing boundary polygons. If None, will compute from boundaries_df.\n        scale_factor (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n\n    Returns:\n        dd.DataFrame: The updated DataFrame with overlap information and assigned cell IDs.\n    \"\"\"\n    # Check if polygons_gdf is provided, otherwise compute from boundaries_df\n    if polygons_gdf is None:\n        if boundaries_df is None:\n            raise ValueError(\n                \"Both boundaries_df and polygons_gdf cannot be None. Provide at least one.\"\n            )\n\n        # Generate polygons from boundaries_df if polygons_gdf is None\n        # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n        polygons_gdf = self.generate_and_scale_polygons(boundaries_df, scale_factor)\n\n    if polygons_gdf.empty:\n        raise ValueError(\"No valid polygons were generated from the boundaries.\")\n    else:\n        if self.verbose:\n            print(f\"Polygons are available. Proceeding with overlap computation.\")\n\n    # Create a delayed function to check if a point is within any polygon\n    def check_overlap(transcript, polygons_gdf):\n        x = transcript[self.keys.TRANSCRIPTS_X.value]\n        y = transcript[self.keys.TRANSCRIPTS_Y.value]\n        point = Point(x, y)\n\n        overlap = False\n        cell_id = None\n\n        # Check for point containment lazily within polygons\n        for _, polygon in polygons_gdf.iterrows():\n            if polygon.geometry.contains(point):\n                overlap = True\n                cell_id = polygon[self.keys.CELL_ID.value]\n                break\n\n        return overlap, cell_id\n\n    # Apply the check_overlap function in parallel to each row using Dask's map_partitions\n    if self.verbose:\n        print(\n            f\"Starting overlap computation for transcripts with the boundary polygons.\"\n        )\n    if isinstance(transcripts_df, pd.DataFrame):\n        # luca: I found this bug here\n        warnings.warn(\n            \"BUG! This function expects Dask DataFrames, not Pandas DataFrames.\"\n        )\n        # if we want to really have the below working in parallel, we need to add n_partitions&gt;1 here\n        transcripts_df = dd.from_pandas(transcripts_df, npartitions=1)\n        transcripts_df.compute().columns\n    transcripts_df = transcripts_df.map_partitions(\n        lambda df: df.assign(\n            **{\n                self.keys.OVERLAPS_BOUNDARY.value: df.apply(\n                    lambda row: delayed(check_overlap)(row, polygons_gdf)[0], axis=1\n                ),\n                self.keys.CELL_ID.value: df.apply(\n                    lambda row: delayed(check_overlap)(row, polygons_gdf)[1], axis=1\n                ),\n            }\n        )\n    )\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.create_scaled_polygon","title":"create_scaled_polygon  <code>staticmethod</code>","text":"<pre><code>create_scaled_polygon(group, scale_factor, keys)\n</code></pre> <p>Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>Group of boundary coordinates (for a specific cell).</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons.</p> required <code>keys</code> <code>Dict or Enum</code> <p>A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.</p> Source code in <code>segger/data/io.py</code> <pre><code>@staticmethod\ndef create_scaled_polygon(\n    group: pd.DataFrame, scale_factor: float, keys\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Static method to create and scale a polygon from boundary vertices and return a GeoDataFrame.\n\n    Parameters:\n        group (pd.DataFrame): Group of boundary coordinates (for a specific cell).\n        scale_factor (float): The factor by which to scale the polygons.\n        keys (Dict or Enum): A collection of keys to access column names for 'cell_id', 'vertex_x', and 'vertex_y'.\n\n    Returns:\n        gpd.GeoDataFrame: A GeoDataFrame containing the scaled Polygon and cell_id.\n    \"\"\"\n    # Extract coordinates and cell ID from the group using keys\n    x_coords = group[keys[\"vertex_x\"]]\n    y_coords = group[keys[\"vertex_y\"]]\n    cell_id = group[keys[\"cell_id\"]].iloc[0]\n\n    # Ensure there are at least 3 points to form a polygon\n    if len(x_coords) &gt;= 3:\n\n        polygon = Polygon(zip(x_coords, y_coords))\n        if polygon.is_valid and not polygon.is_empty:\n            # Scale the polygon by the provided factor\n            scaled_polygon = polygon.buffer(scale_factor)\n            if scaled_polygon.is_valid and not scaled_polygon.is_empty:\n                return gpd.GeoDataFrame(\n                    {\"geometry\": [scaled_polygon], keys[\"cell_id\"]: [cell_id]},\n                    geometry=\"geometry\",\n                    crs=\"EPSG:4326\",\n                )\n    # Return an empty GeoDataFrame if no valid polygon is created\n    return gpd.GeoDataFrame(\n        {\"geometry\": [None], keys[\"cell_id\"]: [cell_id]},\n        geometry=\"geometry\",\n        crs=\"EPSG:4326\",\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.filter_transcripts","title":"filter_transcripts  <code>abstractmethod</code>","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Abstract method to filter transcripts based on dataset-specific criteria.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>segger/data/io.py</code> <pre><code>@abstractmethod\ndef filter_transcripts(\n    self, transcripts_df: pd.DataFrame, min_qv: float = 20.0\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Abstract method to filter transcripts based on dataset-specific criteria.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.generate_and_scale_polygons","title":"generate_and_scale_polygons","text":"<pre><code>generate_and_scale_polygons(boundaries_df, scale_factor=1.0)\n</code></pre> <p>Generate and scale polygons from boundary coordinates using Dask. Keeps class structure intact by using static method for the core polygon generation.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries_df</code> <code>DataFrame</code> <p>DataFrame containing boundary coordinates.</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the polygons (default is 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.</p> Source code in <code>segger/data/io.py</code> <pre><code>def generate_and_scale_polygons(\n    self, boundaries_df: dd.DataFrame, scale_factor: float = 1.0\n) -&gt; dgpd.GeoDataFrame:\n    \"\"\"\n    Generate and scale polygons from boundary coordinates using Dask.\n    Keeps class structure intact by using static method for the core polygon generation.\n\n    Parameters:\n        boundaries_df (dd.DataFrame): DataFrame containing boundary coordinates.\n        scale_factor (float, optional): The factor by which to scale the polygons (default is 1.0).\n\n    Returns:\n        dgpd.GeoDataFrame: A GeoDataFrame containing scaled Polygon objects and their centroids.\n    \"\"\"\n    # if self.verbose: print(f\"No precomputed polygons provided. Computing polygons from boundaries with a scale factor of {scale_factor}.\")\n\n    # Extract required columns from self.keys\n    cell_id_column = self.keys.CELL_ID.value\n    vertex_x_column = self.keys.BOUNDARIES_VERTEX_X.value\n    vertex_y_column = self.keys.BOUNDARIES_VERTEX_Y.value\n\n    create_polygon = self.create_scaled_polygon\n    # Use a lambda to wrap the static method call and avoid passing the function object directly to Dask\n    polygons_ddf = boundaries_df.groupby(cell_id_column).apply(\n        lambda group: create_polygon(\n            group=group,\n            scale_factor=scale_factor,\n            keys={  # Pass keys as a dict for the lambda function\n                \"vertex_x\": vertex_x_column,\n                \"vertex_y\": vertex_y_column,\n                \"cell_id\": cell_id_column,\n            },\n        )\n    )\n\n    # Lazily compute centroids for each polygon\n    if self.verbose:\n        print(\"Adding centroids to the polygons...\")\n    polygons_ddf[\"centroid_x\"] = polygons_ddf.geometry.centroid.x\n    polygons_ddf[\"centroid_y\"] = polygons_ddf.geometry.centroid.y\n\n    polygons_ddf = polygons_ddf.drop_duplicates()\n    # polygons_ddf = polygons_ddf.to_crs(\"EPSG:3857\")\n\n    return polygons_ddf\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.load_boundaries","title":"load_boundaries","text":"<pre><code>load_boundaries(path, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load boundaries data lazily using Dask, filtering by the specified bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the boundaries file.</p> required <code>file_format</code> <code>str</code> <p>Format of the file to load. Only 'parquet' is supported in this refactor.</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered boundaries DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def load_boundaries(\n    self,\n    path: Path,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load boundaries data lazily using Dask, filtering by the specified bounding box.\n\n    Parameters:\n        path (Path): Path to the boundaries file.\n        file_format (str, optional): Format of the file to load. Only 'parquet' is supported in this refactor.\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered boundaries DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(f\"Unsupported file format: {file_format}\")\n\n    self.boundaries_path = path\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.BOUNDARIES_VERTEX_X.value,\n        self.keys.BOUNDARIES_VERTEX_Y.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&gt;=\", x_min),\n        (self.keys.BOUNDARIES_VERTEX_X.value, \"&lt;=\", x_max),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&gt;=\", y_min),\n        (self.keys.BOUNDARIES_VERTEX_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(path).columns)\n    if \"geometry\" in columns:\n        bbox = (x_min, y_min, x_max, y_max)\n        # TODO: check that SpatialData objects write the \"bbox covering metadata\" to the parquet file\n        gdf = dgpd.read_parquet(path, bbox=bbox)\n        id_col, x_col, y_col = (\n            self.keys.CELL_ID.value,\n            self.keys.BOUNDARIES_VERTEX_X.value,\n            self.keys.BOUNDARIES_VERTEX_Y.value,\n        )\n\n        # Function to expand each polygon into a list of vertices\n        def expand_polygon(row):\n            expanded_data = []\n            polygon = row[\"geometry\"]\n            if polygon.geom_type == \"Polygon\":\n                exterior_coords = polygon.exterior.coords\n                for x, y in exterior_coords:\n                    expanded_data.append({id_col: row.name, x_col: x, y_col: y})\n            else:\n                # Instead of expanding the gdf and then having code later to recreate it (when computing the pyg graph)\n                # we could directly have this function returning a Dask GeoDataFrame. This means that we don't need\n                # to implement this else black\n                raise ValueError(f\"Unsupported geometry type: {polygon.geom_type}\")\n            return expanded_data\n\n        # Apply the function to each partition and collect results\n        def process_partition(df):\n            expanded_data = [expand_polygon(row) for _, row in df.iterrows()]\n            # Flatten the list of lists\n            flattened_data = [item for sublist in expanded_data for item in sublist]\n            return pd.DataFrame(flattened_data)\n\n        # Use map_partitions to apply the function and convert it into a Dask DataFrame\n        boundaries_df = gdf.map_partitions(\n            process_partition, meta={id_col: str, x_col: float, y_col: float}\n        )\n    else:\n        boundaries_df = dd.read_parquet(\n            path, columns=columns_to_read, filters=filters\n        )\n\n        # Convert the cell IDs to strings lazily\n        boundaries_df[self.keys.CELL_ID.value] = boundaries_df[\n            self.keys.CELL_ID.value\n        ].apply(\n            lambda x: str(x) if pd.notnull(x) else None, meta=(\"cell_id\", \"object\")\n        )\n\n    if self.verbose:\n        print(\n            f\"Loaded boundaries from '{path}' within bounding box ({x_min}, {x_max}, {y_min}, {y_max}).\"\n        )\n\n    return boundaries_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.load_transcripts","title":"load_transcripts","text":"<pre><code>load_transcripts(base_path=None, sample=None, transcripts_filename=None, path=None, file_format='parquet', x_min=None, x_max=None, y_min=None, y_max=None)\n</code></pre> <p>Load transcripts from a Parquet file using Dask for efficient chunked processing, only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>The base directory path where samples are stored.</p> <code>None</code> <code>sample</code> <code>str</code> <p>The sample name or identifier.</p> <code>None</code> <code>transcripts_filename</code> <code>str</code> <p>The filename of the transcripts file (default is derived from the dataset keys).</p> <code>None</code> <code>path</code> <code>Path</code> <p>Specific path to the transcripts file.</p> <code>None</code> <code>file_format</code> <code>str</code> <p>Format of the file to load (default is 'parquet').</p> <code>'parquet'</code> <code>x_min</code> <code>float</code> <p>Minimum X-coordinate for the bounding box.</p> <code>None</code> <code>x_max</code> <code>float</code> <p>Maximum X-coordinate for the bounding box.</p> <code>None</code> <code>y_min</code> <code>float</code> <p>Minimum Y-coordinate for the bounding box.</p> <code>None</code> <code>y_max</code> <code>float</code> <p>Maximum Y-coordinate for the bounding box.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered transcripts DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def load_transcripts(\n    self,\n    base_path: Path = None,\n    sample: str = None,\n    transcripts_filename: str = None,\n    path: Path = None,\n    file_format: str = \"parquet\",\n    x_min: float = None,\n    x_max: float = None,\n    y_min: float = None,\n    y_max: float = None,\n    # additional_embeddings: Optional[Dict[str, pd.DataFrame]] = None,\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Load transcripts from a Parquet file using Dask for efficient chunked processing,\n    only within the specified bounding box, and return the filtered DataFrame with integer token embeddings.\n\n    Parameters:\n        base_path (Path, optional): The base directory path where samples are stored.\n        sample (str, optional): The sample name or identifier.\n        transcripts_filename (str, optional): The filename of the transcripts file (default is derived from the dataset keys).\n        path (Path, optional): Specific path to the transcripts file.\n        file_format (str, optional): Format of the file to load (default is 'parquet').\n        x_min (float, optional): Minimum X-coordinate for the bounding box.\n        x_max (float, optional): Maximum X-coordinate for the bounding box.\n        y_min (float, optional): Minimum Y-coordinate for the bounding box.\n        y_max (float, optional): Maximum Y-coordinate for the bounding box.\n\n    Returns:\n        dd.DataFrame: The filtered transcripts DataFrame.\n    \"\"\"\n    if file_format != \"parquet\":\n        raise ValueError(\"This version only supports parquet files with Dask.\")\n\n    # Set the file path for transcripts\n    transcripts_filename = transcripts_filename or self.keys.TRANSCRIPTS_FILE.value\n    file_path = path or (base_path / sample / transcripts_filename)\n    self.transcripts_path = file_path\n\n    # Set metadata\n    # self.set_metadata()\n\n    # Use bounding box values from set_metadata if not explicitly provided\n    x_min = x_min or self.x_min\n    x_max = x_max or self.x_max\n    y_min = y_min or self.y_min\n    y_max = y_max or self.y_max\n\n    # Check for available columns in the file's metadata (without loading the data)\n    parquet_metadata = dd.read_parquet(file_path, meta_only=True)\n    available_columns = parquet_metadata.columns\n\n    # Define the list of columns to read\n    columns_to_read = [\n        self.keys.TRANSCRIPTS_ID.value,\n        self.keys.TRANSCRIPTS_X.value,\n        self.keys.TRANSCRIPTS_Y.value,\n        self.keys.FEATURE_NAME.value,\n        self.keys.CELL_ID.value,\n    ]\n\n    # Check if the QUALITY_VALUE key exists in the dataset, and add it to the columns list if present\n    if self.keys.QUALITY_VALUE.value in available_columns:\n        columns_to_read.append(self.keys.QUALITY_VALUE.value)\n\n    if self.keys.OVERLAPS_BOUNDARY.value in available_columns:\n        columns_to_read.append(self.keys.OVERLAPS_BOUNDARY.value)\n\n    # Use filters to only load data within the specified bounding box (x_min, x_max, y_min, y_max)\n    filters = [\n        (self.keys.TRANSCRIPTS_X.value, \"&gt;=\", x_min),\n        (self.keys.TRANSCRIPTS_X.value, \"&lt;=\", x_max),\n        (self.keys.TRANSCRIPTS_Y.value, \"&gt;=\", y_min),\n        (self.keys.TRANSCRIPTS_Y.value, \"&lt;=\", y_max),\n    ]\n\n    # Load the dataset lazily with filters applied for the bounding box\n    columns = set(dd.read_parquet(file_path).columns)\n    transcripts_df = dd.read_parquet(\n        file_path, columns=columns_to_read, filters=filters\n    ).compute()\n\n    # Convert transcript and cell IDs to strings lazily\n    transcripts_df[self.keys.TRANSCRIPTS_ID.value] = transcripts_df[\n        self.keys.TRANSCRIPTS_ID.value\n    ].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n    transcripts_df[self.keys.CELL_ID.value] = transcripts_df[\n        self.keys.CELL_ID.value\n    ].apply(\n        lambda x: str(x) if pd.notnull(x) else None,\n    )\n\n    # Convert feature names from bytes to strings if necessary\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[\n            self.keys.FEATURE_NAME.value\n        ].astype(str)\n\n    # Apply dataset-specific filtering (e.g., quality filtering for Xenium)\n    transcripts_df = self.filter_transcripts(transcripts_df)\n\n    # Handle additional embeddings if provided\n    if self.embedding_df is not None and not self.embedding_df.empty:\n        valid_genes = self.embedding_df.index\n        # Lazily count the number of rows in the DataFrame before filtering\n        initial_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        # Filter the DataFrame lazily based on valid genes from embeddings\n        transcripts_df = transcripts_df[\n            transcripts_df[self.keys.FEATURE_NAME.value].isin(valid_genes)\n        ]\n        final_count = delayed(lambda df: df.shape[0])(transcripts_df)\n        if self.verbose:\n            print(\n                f\"Dropped {initial_count - final_count} transcripts not found in embedding.\"\n            )\n\n    # Ensure that the 'OVERLAPS_BOUNDARY' column is boolean if it exists\n    if self.keys.OVERLAPS_BOUNDARY.value in transcripts_df.columns:\n        transcripts_df[self.keys.OVERLAPS_BOUNDARY.value] = transcripts_df[\n            self.keys.OVERLAPS_BOUNDARY.value\n        ].astype(bool)\n\n    return transcripts_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.save_dataset_for_segger","title":"save_dataset_for_segger","text":"<pre><code>save_dataset_for_segger(processed_dir, x_size=1000, y_size=1000, d_x=900, d_y=900, margin_x=None, margin_y=None, compute_labels=True, r_tx=5, k_tx=3, val_prob=0.1, test_prob=0.2, neg_sampling_ratio_approx=5, sampling_rate=1, num_workers=1, scale_boundaries=1.0, method='kd_tree', gpu=False, workers=1)\n</code></pre> <p>Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.</p> <p>Parameters:</p> Name Type Description Default <code>processed_dir</code> <code>Path</code> <p>Directory to save the processed dataset.</p> required <code>x_size</code> <code>float</code> <p>Width of each tile.</p> <code>1000</code> <code>y_size</code> <code>float</code> <p>Height of each tile.</p> <code>1000</code> <code>d_x</code> <code>float</code> <p>Step size in the x direction for tiles.</p> <code>900</code> <code>d_y</code> <code>float</code> <p>Step size in the y direction for tiles.</p> <code>900</code> <code>margin_x</code> <code>float</code> <p>Margin in the x direction to include transcripts.</p> <code>None</code> <code>margin_y</code> <code>float</code> <p>Margin in the y direction to include transcripts.</p> <code>None</code> <code>compute_labels</code> <code>bool</code> <p>Whether to compute edge labels for tx_belongs_bd edges.</p> <code>True</code> <code>r_tx</code> <code>float</code> <p>Radius for building the transcript-to-transcript graph.</p> <code>5</code> <code>k_tx</code> <code>int</code> <p>Number of nearest neighbors for the tx-tx graph.</p> <code>3</code> <code>val_prob</code> <code>float</code> <p>Probability of assigning a tile to the validation set.</p> <code>0.1</code> <code>test_prob</code> <code>float</code> <p>Probability of assigning a tile to the test set.</p> <code>0.2</code> <code>neg_sampling_ratio_approx</code> <code>float</code> <p>Approximate ratio of negative samples.</p> <code>5</code> <code>sampling_rate</code> <code>float</code> <p>Rate of sampling tiles.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for parallel processing.</p> <code>1</code> <code>scale_boundaries</code> <code>float</code> <p>The factor by which to scale the boundary polygons. Default is 1.0.</p> <code>1.0</code> <code>method</code> <code>str</code> <p>Method for computing edge indices (e.g., 'kd_tree', 'faiss').</p> <code>'kd_tree'</code> <code>gpu</code> <code>bool</code> <p>Whether to use GPU acceleration for edge index computation.</p> <code>False</code> <code>workers</code> <code>int</code> <p>Number of workers to use to compute the neighborhood graph (per tile).</p> <code>1</code> Source code in <code>segger/data/io.py</code> <pre><code>def save_dataset_for_segger(\n    self,\n    processed_dir: Path,\n    x_size: float = 1000,\n    y_size: float = 1000,\n    d_x: float = 900,\n    d_y: float = 900,\n    margin_x: float = None,\n    margin_y: float = None,\n    compute_labels: bool = True,\n    r_tx: float = 5,\n    k_tx: int = 3,\n    val_prob: float = 0.1,\n    test_prob: float = 0.2,\n    neg_sampling_ratio_approx: float = 5,\n    sampling_rate: float = 1,\n    num_workers: int = 1,\n    scale_boundaries: float = 1.0,\n    method: str = \"kd_tree\",\n    gpu: bool = False,\n    workers: int = 1,\n) -&gt; None:\n    \"\"\"\n    Saves the dataset for Segger in a processed format using Dask for parallel and lazy processing.\n\n    Parameters:\n        processed_dir (Path): Directory to save the processed dataset.\n        x_size (float, optional): Width of each tile.\n        y_size (float, optional): Height of each tile.\n        d_x (float, optional): Step size in the x direction for tiles.\n        d_y (float, optional): Step size in the y direction for tiles.\n        margin_x (float, optional): Margin in the x direction to include transcripts.\n        margin_y (float, optional): Margin in the y direction to include transcripts.\n        compute_labels (bool, optional): Whether to compute edge labels for tx_belongs_bd edges.\n        r_tx (float, optional): Radius for building the transcript-to-transcript graph.\n        k_tx (int, optional): Number of nearest neighbors for the tx-tx graph.\n        val_prob (float, optional): Probability of assigning a tile to the validation set.\n        test_prob (float, optional): Probability of assigning a tile to the test set.\n        neg_sampling_ratio_approx (float, optional): Approximate ratio of negative samples.\n        sampling_rate (float, optional): Rate of sampling tiles.\n        num_workers (int, optional): Number of workers to use for parallel processing.\n        scale_boundaries (float, optional): The factor by which to scale the boundary polygons. Default is 1.0.\n        method (str, optional): Method for computing edge indices (e.g., 'kd_tree', 'faiss').\n        gpu (bool, optional): Whether to use GPU acceleration for edge index computation.\n        workers (int, optional): Number of workers to use to compute the neighborhood graph (per tile).\n\n    \"\"\"\n    # Prepare directories for storing processed tiles\n    self._prepare_directories(processed_dir)\n\n    # Get x and y coordinate ranges for tiling\n    x_range, y_range = self._get_ranges(d_x, d_y)\n\n    # Generate parameters for each tile\n    tile_params = self._generate_tile_params(\n        x_range,\n        y_range,\n        x_size,\n        y_size,\n        margin_x,\n        margin_y,\n        compute_labels,\n        r_tx,\n        k_tx,\n        val_prob,\n        test_prob,\n        neg_sampling_ratio_approx,\n        sampling_rate,\n        processed_dir,\n        scale_boundaries,\n        method,\n        gpu,\n        workers,\n    )\n\n    # Process each tile using Dask to parallelize the task\n    if self.verbose:\n        print(\"Starting tile processing...\")\n    tasks = [delayed(self._process_tile)(params) for params in tile_params]\n\n    with ProgressBar():\n        # Use Dask to process all tiles in parallel\n        dask.compute(*tasks, num_workers=num_workers)\n    if self.verbose:\n        print(\"Tile processing completed.\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_embedding","title":"set_embedding","text":"<pre><code>set_embedding(embedding_name)\n</code></pre> <p>Set the current embedding type for the transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_name</code> <p>str The name of the embedding to use.</p> required Source code in <code>segger/data/io.py</code> <pre><code>def set_embedding(self, embedding_name: str) -&gt; None:\n    \"\"\"\n    Set the current embedding type for the transcripts.\n\n    Parameters:\n        embedding_name : str\n            The name of the embedding to use.\n\n    \"\"\"\n    if embedding_name in self.embeddings_dict:\n        self.current_embedding = embedding_name\n    else:\n        raise ValueError(\n            f\"Embedding {embedding_name} not found in embeddings_dict.\"\n        )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_file_paths","title":"set_file_paths","text":"<pre><code>set_file_paths(transcripts_path, boundaries_path)\n</code></pre> <p>Set the paths for the transcript and boundary files.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_path</code> <code>Path</code> <p>Path to the Parquet file containing transcripts data.</p> required <code>boundaries_path</code> <code>Path</code> <p>Path to the Parquet file containing boundaries data.</p> required Source code in <code>segger/data/io.py</code> <pre><code>def set_file_paths(self, transcripts_path: Path, boundaries_path: Path) -&gt; None:\n    \"\"\"\n    Set the paths for the transcript and boundary files.\n\n    Parameters:\n        transcripts_path (Path): Path to the Parquet file containing transcripts data.\n        boundaries_path (Path): Path to the Parquet file containing boundaries data.\n    \"\"\"\n    self.transcripts_path = transcripts_path\n    self.boundaries_path = boundaries_path\n\n    if self.verbose:\n        print(f\"Set transcripts file path to {transcripts_path}\")\n    if self.verbose:\n        print(f\"Set boundaries file path to {boundaries_path}\")\n</code></pre>"},{"location":"api/data/io/#segger.data.io.SpatialTranscriptomicsSample.set_metadata","title":"set_metadata","text":"<pre><code>set_metadata()\n</code></pre> <p>Set metadata for the transcript dataset, including bounding box limits and unique gene names, without reading the entire Parquet file. Additionally, return integer tokens for unique gene names instead of one-hot encodings and store the lookup table for later mapping.</p> Source code in <code>segger/data/io.py</code> <pre><code>def set_metadata(self) -&gt; None:\n    \"\"\"\n    Set metadata for the transcript dataset, including bounding box limits and unique gene names,\n    without reading the entire Parquet file. Additionally, return integer tokens for unique gene names\n    instead of one-hot encodings and store the lookup table for later mapping.\n    \"\"\"\n    # Load the Parquet file metadata\n    parquet_file = pq.read_table(self.transcripts_path)\n\n    # Get the column names for X, Y, and feature names from the class's keys\n    x_col = self.keys.TRANSCRIPTS_X.value\n    y_col = self.keys.TRANSCRIPTS_Y.value\n    feature_col = self.keys.FEATURE_NAME.value\n\n    # Initialize variables to track min/max values for X and Y\n    x_min, x_max, y_min, y_max = (\n        float(\"inf\"),\n        float(\"-inf\"),\n        float(\"inf\"),\n        float(\"-inf\"),\n    )\n\n    # Extract unique gene names and ensure they're strings\n    gene_set = set()\n\n    # Define the filter for unwanted codewords\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    row_group_size = 4_000_000\n    start = 0\n    n = len(parquet_file)\n    while start &lt; n:\n        chunk = parquet_file.slice(start, start + row_group_size)\n        start += row_group_size\n\n        # Update the bounding box values (min/max)\n        x_values = chunk[x_col].to_pandas()\n        y_values = chunk[y_col].to_pandas()\n\n        x_min = min(x_min, x_values.min())\n        x_max = max(x_max, x_values.max())\n        y_min = min(y_min, y_values.min())\n        y_max = max(y_max, y_values.max())\n\n        # Convert feature values (gene names) to strings and filter out unwanted codewords\n        feature_values = (\n            chunk[feature_col]\n            .to_pandas()\n            .apply(\n                lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else str(x),\n            )\n        )\n\n        # Filter out unwanted codewords\n        filtered_genes = feature_values[\n            ~feature_values.str.startswith(filter_codewords)\n        ]\n\n        # Update the unique gene set\n        gene_set.update(filtered_genes.unique())\n\n    # Set bounding box limits\n    self.x_min = x_min\n    self.x_max = x_max\n    self.y_min = y_min\n    self.y_max = y_max\n\n    if self.verbose:\n        print(\n            f\"Bounding box limits set: x_min={self.x_min}, x_max={self.x_max}, y_min={self.y_min}, y_max={self.y_max}\"\n        )\n\n    # Convert the set of unique genes into a sorted list for consistent ordering\n    self.unique_genes = sorted(gene_set)\n    if self.verbose:\n        print(\n            f\"Extracted {len(self.unique_genes)} unique gene names for integer tokenization.\"\n        )\n\n    # Initialize a LabelEncoder to convert unique genes into integer tokens\n    self.tx_encoder = LabelEncoder()\n\n    # Fit the LabelEncoder on the unique genes\n    self.tx_encoder.fit(self.unique_genes)\n\n    # Store the integer tokens mapping to gene names\n    self.gene_to_token_map = dict(\n        zip(\n            self.tx_encoder.classes_,\n            self.tx_encoder.transform(self.tx_encoder.classes_),\n        )\n    )\n\n    if self.verbose:\n        print(\n            \"Integer tokens have been computed and stored based on unique gene names.\"\n        )\n\n    # Optional: Create a reverse mapping for lookup purposes (token to gene)\n    self.token_to_gene_map = {v: k for k, v in self.gene_to_token_map.items()}\n\n    if self.verbose:\n        print(\n            \"Lookup tables (gene_to_token_map and token_to_gene_map) have been created.\"\n        )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.XeniumKeys","title":"XeniumKeys","text":"<p>               Bases: <code>Enum</code></p> <p>Keys for 10X Genomics Xenium formatted dataset.</p>"},{"location":"api/data/io/#segger.data.io.XeniumSample","title":"XeniumSample","text":"<pre><code>XeniumSample(transcripts_df=None, transcripts_radius=10, boundaries_graph=False, embedding_df=None, verbose=True)\n</code></pre> <p>               Bases: <code>SpatialTranscriptomicsSample</code></p> Source code in <code>segger/data/io.py</code> <pre><code>def __init__(\n    self,\n    transcripts_df: dd.DataFrame = None,\n    transcripts_radius: int = 10,\n    boundaries_graph: bool = False,\n    embedding_df: pd.DataFrame = None,\n    verbose: bool = True,\n):\n    super().__init__(\n        transcripts_df,\n        transcripts_radius,\n        boundaries_graph,\n        embedding_df,\n        XeniumKeys,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/data/io/#segger.data.io.XeniumSample.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The Dask DataFrame containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dd.DataFrame: The filtered Dask DataFrame.</p> Source code in <code>segger/data/io.py</code> <pre><code>def filter_transcripts(\n    self, transcripts_df: dd.DataFrame, min_qv: float = 20.0\n) -&gt; dd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts for Xenium using Dask.\n\n    Parameters:\n        transcripts_df (dd.DataFrame): The Dask DataFrame containing transcript data.\n        min_qv (float, optional): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        dd.DataFrame: The filtered Dask DataFrame.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    # Ensure FEATURE_NAME is a string type for proper filtering (compatible with Dask)\n    # Handle potential bytes to string conversion for Dask DataFrame\n    if pd.api.types.is_object_dtype(transcripts_df[self.keys.FEATURE_NAME.value]):\n        transcripts_df[self.keys.FEATURE_NAME.value] = transcripts_df[\n            self.keys.FEATURE_NAME.value\n        ].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n\n    # Apply the quality value filter using Dask\n    mask_quality = transcripts_df[self.keys.QUALITY_VALUE.value] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[self.keys.FEATURE_NAME.value].str.startswith(\n        filter_codewords\n    )\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n\n    # Return the filtered DataFrame lazily\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(\n    adata: ad.AnnData, celltype_column: str\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(\n        gene_expression_binary, index=adata.obs_names, columns=adata.var_names\n    )\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/io/#segger.data.io.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[\n        assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_cytoplasmic = (\n        len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    )\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[\n        non_assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_non_assigned_cytoplasmic = (\n        len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    )\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (\n        gene_group_assigned.size() / (gene_group_all.size() + 1) * 100\n    ).reset_index(names=\"percent_assigned\")\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (\n        cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100\n    ).reset_index(name=\"percent_cytoplasmic\")\n    gene_metrics = pd.merge(\n        gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\"\n    ).fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/io/#segger.data.io.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(\n        columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"}\n    )[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(\n            cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\"\n        )\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = (\n        df[df[cell_id_col].astype(str) == \"UNASSIGNED\"]\n        .groupby(\"feature_name\")[\"feature_name\"]\n        .count()\n    )\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = (\n        var_df.index.map(unassigned_counts).fillna(0).astype(int)\n    )\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(\n        adata.obs,\n        cell_summary.loc[adata.obs_names, :],\n        left_index=True,\n        right_index=True,\n    )\n\n    return adata\n</code></pre>"},{"location":"api/data/io/#segger.data.io.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/io/#segger.data.io.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/io/#segger.data.io.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/io/#segger.data.io.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(\n            coords_1, coords_2, k=k, dist=dist, workers=workers\n        )\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(\n        coords_2, k=k, distance_upper_bound=dist, workers=workers\n    )\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(\n                np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T\n            )\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/io/#segger.data.io.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/io/#segger.data.io.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/io/#segger.data.io.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/data/utils/","title":"segger.data.utils","text":""},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset","title":"SpatialTranscriptomicsDataset","text":"<pre><code>SpatialTranscriptomicsDataset(root, transform=None, pre_transform=None, pre_filter=None)\n</code></pre> <p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for handling SpatialTranscriptomics spatial transcriptomics data.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>The root directory where the dataset is stored.</p> <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version.</p> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it.</p> <p>Initialize the SpatialTranscriptomicsDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in a Data object and returns a transformed version. Defaults to None.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.</p> <code>None</code> Source code in <code>segger/data/utils.py</code> <pre><code>def __init__(\n    self,\n    root: str,\n    transform: Callable = None,\n    pre_transform: Callable = None,\n    pre_filter: Callable = None,\n):\n    \"\"\"Initialize the SpatialTranscriptomicsDataset.\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_transform (callable, optional): A function/transform that takes in a Data object and returns a transformed version. Defaults to None.\n        pre_filter (callable, optional): A function that takes in a Data object and returns a boolean indicating whether to keep it. Defaults to None.\n    \"\"\"\n    super().__init__(root, transform, pre_transform, pre_filter)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names\n</code></pre> <p>Return a list of processed file names in the processed directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed file names.</p>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names\n</code></pre> <p>Return a list of raw file names in the raw directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of raw file names.</p>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download the raw data. This method should be overridden if you need to download the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def download(self) -&gt; None:\n    \"\"\"Download the raw data. This method should be overridden if you need to download the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.get","title":"get","text":"<pre><code>get(idx)\n</code></pre> <p>Get a processed data object.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the data object to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Data</code> <code>Data</code> <p>The processed data object.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get a processed data object.\n\n    Args:\n        idx (int): Index of the data object to retrieve.\n\n    Returns:\n        Data: The processed data object.\n    \"\"\"\n    data = torch.load(\n        os.path.join(self.processed_dir, self.processed_file_names[idx])\n    )\n    data[\"tx\"].x = data[\"tx\"].x.to_dense()\n    return data\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.len","title":"len","text":"<pre><code>len()\n</code></pre> <p>Return the number of processed files.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of processed files.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Return the number of processed files.\n\n    Returns:\n        int: Number of processed files.\n    \"\"\"\n    return len(self.processed_file_names)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.SpatialTranscriptomicsDataset.process","title":"process","text":"<pre><code>process()\n</code></pre> <p>Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def process(self) -&gt; None:\n    \"\"\"Process the raw data and save it to the processed directory. This method should be overridden if you need to process the data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.calculate_gene_celltype_abundance_embedding","title":"calculate_gene_celltype_abundance_embedding","text":"<pre><code>calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n</code></pre> <p>Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type that express the gene (non-zero expression).</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>An AnnData object containing gene expression data and cell type information.</p> required <code>celltype_column</code> <code>str</code> <p>The column name in <code>adata.obs</code> that contains the cell type information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing the fraction of cells in that cell type expressing the gene.</p> Example <p>adata = AnnData(...)  # Load your scRNA-seq AnnData object celltype_column = 'celltype_major' abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column) abundance_df.head()</p> Source code in <code>segger/data/utils.py</code> <pre><code>def calculate_gene_celltype_abundance_embedding(\n    adata: ad.AnnData, celltype_column: str\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the cell type abundance embedding for each gene based on the fraction of cells in each cell type\n    that express the gene (non-zero expression).\n\n    Parameters:\n        adata (ad.AnnData): An AnnData object containing gene expression data and cell type information.\n        celltype_column (str): The column name in `adata.obs` that contains the cell type information.\n\n    Returns:\n        pd.DataFrame: A DataFrame where rows are genes and columns are cell types, with each value representing\n            the fraction of cells in that cell type expressing the gene.\n\n    Example:\n        &gt;&gt;&gt; adata = AnnData(...)  # Load your scRNA-seq AnnData object\n        &gt;&gt;&gt; celltype_column = 'celltype_major'\n        &gt;&gt;&gt; abundance_df = calculate_gene_celltype_abundance_embedding(adata, celltype_column)\n        &gt;&gt;&gt; abundance_df.head()\n    \"\"\"\n    # Extract expression data (cells x genes) and cell type information (cells)\n    expression_data = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n    cell_types = adata.obs[celltype_column].values\n    # Create a binary matrix for gene expression (1 if non-zero, 0 otherwise)\n    gene_expression_binary = (expression_data &gt; 0).astype(int)\n    # Convert the binary matrix to a DataFrame\n    gene_expression_df = pd.DataFrame(\n        gene_expression_binary, index=adata.obs_names, columns=adata.var_names\n    )\n    # Perform one-hot encoding on the cell types\n    encoder = OneHotEncoder(sparse_output=False)\n    cell_type_encoded = encoder.fit_transform(cell_types.reshape(-1, 1))\n    # Calculate the fraction of cells expressing each gene per cell type\n    cell_type_abundance_list = []\n    for i in range(cell_type_encoded.shape[1]):\n        # Extract cells of the current cell type\n        cell_type_mask = cell_type_encoded[:, i] == 1\n        # Calculate the abundance: sum of non-zero expressions in this cell type / total cells in this cell type\n        abundance = gene_expression_df[cell_type_mask].mean(axis=0)\n        cell_type_abundance_list.append(abundance)\n    # Create a DataFrame for the cell type abundance with gene names as rows and cell types as columns\n    cell_type_abundance_df = pd.DataFrame(\n        cell_type_abundance_list, columns=adata.var_names, index=encoder.categories_[0]\n    ).T\n    return cell_type_abundance_df\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.compute_transcript_metrics","title":"compute_transcript_metrics","text":"<pre><code>compute_transcript_metrics(df, qv_threshold=30, cell_id_col='cell_id')\n</code></pre> <p>Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>cell_id_col</code> <code>str</code> <p>The name of the column representing the cell ID.</p> <code>'cell_id'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing various transcript metrics: - 'percent_assigned' (float): The percentage of assigned transcripts. - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts. - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts. - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts. - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def compute_transcript_metrics(\n    df: pd.DataFrame, qv_threshold: float = 30, cell_id_col: str = \"cell_id\"\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Computes various metrics for a given dataframe of transcript data filtered by quality value threshold.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing transcript data.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        cell_id_col (str): The name of the column representing the cell ID.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing various transcript metrics:\n            - 'percent_assigned' (float): The percentage of assigned transcripts.\n            - 'percent_cytoplasmic' (float): The percentage of cytoplasmic transcripts among assigned transcripts.\n            - 'percent_nucleus' (float): The percentage of nucleus transcripts among assigned transcripts.\n            - 'percent_non_assigned_cytoplasmic' (float): The percentage of non-assigned cytoplasmic transcripts.\n            - 'gene_metrics' (pd.DataFrame): A dataframe containing gene-level metrics.\n    \"\"\"\n    df_filtered = df[df[\"qv\"] &gt; qv_threshold]\n    total_transcripts = len(df_filtered)\n    assigned_transcripts = df_filtered[df_filtered[cell_id_col] != -1]\n    percent_assigned = len(assigned_transcripts) / (total_transcripts + 1) * 100\n    cytoplasmic_transcripts = assigned_transcripts[\n        assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_cytoplasmic = (\n        len(cytoplasmic_transcripts) / (len(assigned_transcripts) + 1) * 100\n    )\n    percent_nucleus = 100 - percent_cytoplasmic\n    non_assigned_transcripts = df_filtered[df_filtered[cell_id_col] == -1]\n    non_assigned_cytoplasmic = non_assigned_transcripts[\n        non_assigned_transcripts[\"overlaps_nucleus\"] != 1\n    ]\n    percent_non_assigned_cytoplasmic = (\n        len(non_assigned_cytoplasmic) / (len(non_assigned_transcripts) + 1) * 100\n    )\n    gene_group_assigned = assigned_transcripts.groupby(\"feature_name\")\n    gene_group_all = df_filtered.groupby(\"feature_name\")\n    gene_percent_assigned = (\n        gene_group_assigned.size() / (gene_group_all.size() + 1) * 100\n    ).reset_index(names=\"percent_assigned\")\n    cytoplasmic_gene_group = cytoplasmic_transcripts.groupby(\"feature_name\")\n    gene_percent_cytoplasmic = (\n        cytoplasmic_gene_group.size() / (len(cytoplasmic_transcripts) + 1) * 100\n    ).reset_index(name=\"percent_cytoplasmic\")\n    gene_metrics = pd.merge(\n        gene_percent_assigned, gene_percent_cytoplasmic, on=\"feature_name\", how=\"outer\"\n    ).fillna(0)\n    results = {\n        \"percent_assigned\": percent_assigned,\n        \"percent_cytoplasmic\": percent_cytoplasmic,\n        \"percent_nucleus\": percent_nucleus,\n        \"percent_non_assigned_cytoplasmic\": percent_non_assigned_cytoplasmic,\n        \"gene_metrics\": gene_metrics,\n    }\n    return results\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.create_anndata","title":"create_anndata","text":"<pre><code>create_anndata(df, panel_df=None, min_transcripts=5, cell_id_col='cell_id', qv_threshold=30, min_cell_area=10.0, max_cell_area=1000.0)\n</code></pre> <p>Generates an AnnData object from a dataframe of segmented transcriptomics data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing segmented transcriptomics data.</p> required <code>panel_df</code> <code>Optional[DataFrame]</code> <p>The dataframe containing panel information.</p> <code>None</code> <code>min_transcripts</code> <code>int</code> <p>The minimum number of transcripts required for a cell to be included.</p> <code>5</code> <code>cell_id_col</code> <code>str</code> <p>The column name representing the cell ID in the input dataframe.</p> <code>'cell_id'</code> <code>qv_threshold</code> <code>float</code> <p>The quality value threshold for filtering transcripts.</p> <code>30</code> <code>min_cell_area</code> <code>float</code> <p>The minimum cell area to include a cell.</p> <code>10.0</code> <code>max_cell_area</code> <code>float</code> <p>The maximum cell area to include a cell.</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>AnnData</code> <p>ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def create_anndata(\n    df: pd.DataFrame,\n    panel_df: Optional[pd.DataFrame] = None,\n    min_transcripts: int = 5,\n    cell_id_col: str = \"cell_id\",\n    qv_threshold: float = 30,\n    min_cell_area: float = 10.0,\n    max_cell_area: float = 1000.0,\n) -&gt; ad.AnnData:\n    \"\"\"\n    Generates an AnnData object from a dataframe of segmented transcriptomics data.\n\n    Parameters:\n        df (pd.DataFrame): The dataframe containing segmented transcriptomics data.\n        panel_df (Optional[pd.DataFrame]): The dataframe containing panel information.\n        min_transcripts (int): The minimum number of transcripts required for a cell to be included.\n        cell_id_col (str): The column name representing the cell ID in the input dataframe.\n        qv_threshold (float): The quality value threshold for filtering transcripts.\n        min_cell_area (float): The minimum cell area to include a cell.\n        max_cell_area (float): The maximum cell area to include a cell.\n\n    Returns:\n        ad.AnnData: The generated AnnData object containing the transcriptomics data and metadata.\n    \"\"\"\n    # Filter out unassigned cells\n    df_filtered = df[df[cell_id_col].astype(str) != \"UNASSIGNED\"]\n\n    # Create pivot table for gene expression counts per cell\n    pivot_df = df_filtered.rename(\n        columns={cell_id_col: \"cell\", \"feature_name\": \"gene\"}\n    )[[\"cell\", \"gene\"]].pivot_table(\n        index=\"cell\", columns=\"gene\", aggfunc=\"size\", fill_value=0\n    )\n    pivot_df = pivot_df[pivot_df.sum(axis=1) &gt;= min_transcripts]\n\n    # Summarize cell metrics\n    cell_summary = []\n    for cell_id, cell_data in df_filtered.groupby(cell_id_col):\n        if len(cell_data) &lt; min_transcripts:\n            continue\n        cell_convex_hull = ConvexHull(\n            cell_data[[\"x_location\", \"y_location\"]], qhull_options=\"QJ\"\n        )\n        cell_area = cell_convex_hull.area\n        if cell_area &lt; min_cell_area or cell_area &gt; max_cell_area:\n            continue\n        cell_summary.append(\n            {\n                \"cell\": cell_id,\n                \"cell_centroid_x\": cell_data[\"x_location\"].mean(),\n                \"cell_centroid_y\": cell_data[\"y_location\"].mean(),\n                \"cell_area\": cell_area,\n            }\n        )\n    cell_summary = pd.DataFrame(cell_summary).set_index(\"cell\")\n\n    # Add genes from panel_df (if provided) to the pivot table\n    if panel_df is not None:\n        panel_df = panel_df.sort_values(\"gene\")\n        genes = panel_df[\"gene\"].values\n        for gene in genes:\n            if gene not in pivot_df:\n                pivot_df[gene] = 0\n        pivot_df = pivot_df[genes.tolist()]\n\n    # Create var DataFrame\n    if panel_df is None:\n        var_df = pd.DataFrame(\n            [\n                {\"gene\": gene, \"feature_types\": \"Gene Expression\", \"genome\": \"Unknown\"}\n                for gene in np.unique(pivot_df.columns.values)\n            ]\n        ).set_index(\"gene\")\n    else:\n        var_df = panel_df[[\"gene\", \"ensembl\"]].rename(columns={\"ensembl\": \"gene_ids\"})\n        var_df[\"feature_types\"] = \"Gene Expression\"\n        var_df[\"genome\"] = \"Unknown\"\n        var_df = var_df.set_index(\"gene\")\n\n    # Compute total assigned and unassigned transcript counts for each gene\n    assigned_counts = df_filtered.groupby(\"feature_name\")[\"feature_name\"].count()\n    unassigned_counts = (\n        df[df[cell_id_col].astype(str) == \"UNASSIGNED\"]\n        .groupby(\"feature_name\")[\"feature_name\"]\n        .count()\n    )\n    var_df[\"total_assigned\"] = var_df.index.map(assigned_counts).fillna(0).astype(int)\n    var_df[\"total_unassigned\"] = (\n        var_df.index.map(unassigned_counts).fillna(0).astype(int)\n    )\n\n    # Filter cells and create the AnnData object\n    cells = list(set(pivot_df.index) &amp; set(cell_summary.index))\n    pivot_df = pivot_df.loc[cells, :]\n    cell_summary = cell_summary.loc[cells, :]\n    adata = ad.AnnData(pivot_df.values)\n    adata.var = var_df\n    adata.obs[\"transcripts\"] = pivot_df.sum(axis=1).values\n    adata.obs[\"unique_transcripts\"] = (pivot_df &gt; 0).sum(axis=1).values\n    adata.obs_names = pivot_df.index.values.tolist()\n    adata.obs = pd.merge(\n        adata.obs,\n        cell_summary.loc[adata.obs_names, :],\n        left_index=True,\n        right_index=True,\n    )\n\n    return adata\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.filter_transcripts","title":"filter_transcripts","text":"<pre><code>filter_transcripts(transcripts_df, min_qv=20.0)\n</code></pre> <p>Filters transcripts based on quality value and removes unwanted transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>transcripts_df</code> <code>DataFrame</code> <p>The dataframe containing transcript data.</p> required <code>min_qv</code> <code>float</code> <p>The minimum quality value threshold for filtering transcripts.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The filtered dataframe.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def filter_transcripts(  # ONLY FOR XENIUM\n    transcripts_df: pd.DataFrame,\n    min_qv: float = 20.0,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters transcripts based on quality value and removes unwanted transcripts.\n\n    Parameters:\n        transcripts_df (pd.DataFrame): The dataframe containing transcript data.\n        min_qv (float): The minimum quality value threshold for filtering transcripts.\n\n    Returns:\n        pd.DataFrame: The filtered dataframe.\n    \"\"\"\n    filter_codewords = (\n        \"NegControlProbe_\",\n        \"antisense_\",\n        \"NegControlCodeword_\",\n        \"BLANK_\",\n        \"DeprecatedCodeword_\",\n        \"UnassignedCodeword_\",\n    )\n\n    transcripts_df[\"feature_name\"] = transcripts_df[\"feature_name\"].apply(\n        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n    )\n    mask_quality = transcripts_df[\"qv\"] &gt;= min_qv\n\n    # Apply the filter for unwanted codewords using Dask string functions\n    mask_codewords = ~transcripts_df[\"feature_name\"].str.startswith(filter_codewords)\n\n    # Combine the filters and return the filtered Dask DataFrame\n    mask = mask_quality &amp; mask_codewords\n    return transcripts_df[mask]\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.format_time","title":"format_time","text":"<pre><code>format_time(elapsed)\n</code></pre> <p>Format elapsed time to hs.</p>"},{"location":"api/data/utils/#segger.data.utils.format_time--parameters","title":"Parameters:","text":"<p>elapsed : float     Elapsed time in seconds.</p>"},{"location":"api/data/utils/#segger.data.utils.format_time--returns","title":"Returns:","text":"<p>str     Formatted time in hs.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def format_time(elapsed: float) -&gt; str:\n    \"\"\"\n    Format elapsed time to h:m:s.\n\n    Parameters:\n    ----------\n    elapsed : float\n        Elapsed time in seconds.\n\n    Returns:\n    -------\n    str\n        Formatted time in h:m:s.\n    \"\"\"\n    return str(timedelta(seconds=int(elapsed)))\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_edge_index","title":"get_edge_index","text":"<pre><code>get_edge_index(coords_1, coords_2, k=5, dist=10, method='kd_tree', workers=1)\n</code></pre> <p>Computes edge indices using KD-Tree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <code>method</code> <code>str</code> <p>The method to use. Only 'kd_tree' is supported now.</p> <code>'kd_tree'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    method: str = \"kd_tree\",\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KD-Tree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n        method (str, optional): The method to use. Only 'kd_tree' is supported now.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if method == \"kd_tree\":\n        return get_edge_index_kdtree(\n            coords_1, coords_2, k=k, dist=dist, workers=workers\n        )\n    # elif method == \"cuda\":\n    #     return get_edge_index_cuda(coords_1, coords_2, k=k, dist=dist)\n    else:\n        msg = f\"Unknown method {method}. The only supported method is 'kd_tree' now.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_edge_index_kdtree","title":"get_edge_index_kdtree","text":"<pre><code>get_edge_index_kdtree(coords_1, coords_2, k=5, dist=10, workers=1)\n</code></pre> <p>Computes edge indices using KDTree.</p> <p>Parameters:</p> Name Type Description Default <code>coords_1</code> <code>ndarray</code> <p>First set of coordinates.</p> required <code>coords_2</code> <code>ndarray</code> <p>Second set of coordinates.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors.</p> <code>5</code> <code>dist</code> <code>int</code> <p>Distance threshold.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Edge indices.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_edge_index_kdtree(\n    coords_1: np.ndarray,\n    coords_2: np.ndarray,\n    k: int = 5,\n    dist: int = 10,\n    workers: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes edge indices using KDTree.\n\n    Parameters:\n        coords_1 (np.ndarray): First set of coordinates.\n        coords_2 (np.ndarray): Second set of coordinates.\n        k (int, optional): Number of nearest neighbors.\n        dist (int, optional): Distance threshold.\n\n    Returns:\n        torch.Tensor: Edge indices.\n    \"\"\"\n    if isinstance(coords_1, torch.Tensor):\n        coords_1 = coords_1.cpu().numpy()\n    if isinstance(coords_2, torch.Tensor):\n        coords_2 = coords_2.cpu().numpy()\n    tree = cKDTree(coords_1)\n    d_kdtree, idx_out = tree.query(\n        coords_2, k=k, distance_upper_bound=dist, workers=workers\n    )\n    valid_mask = d_kdtree &lt; dist\n    edges = []\n\n    for idx, valid in enumerate(valid_mask):\n        valid_indices = idx_out[idx][valid]\n        if valid_indices.size &gt; 0:\n            edges.append(\n                np.vstack((np.full(valid_indices.shape, idx), valid_indices)).T\n            )\n\n    edge_index = torch.tensor(np.vstack(edges), dtype=torch.long).contiguous()\n    return edge_index\n</code></pre>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents","title":"get_xy_extents","text":"<pre><code>get_xy_extents(filepath, x, y)\n</code></pre> <p>Get the bounding box of the x and y coordinates from a Parquet file.</p>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents--parameters","title":"Parameters","text":"<p>filepath : str     The path to the Parquet file. x : str     The name of the column representing the x-coordinate. y : str     The name of the column representing the y-coordinate.</p>"},{"location":"api/data/utils/#segger.data.utils.get_xy_extents--returns","title":"Returns","text":"<p>shapely.Polygon     A polygon representing the bounding box of the x and y coordinates.</p> Source code in <code>segger/data/utils.py</code> <pre><code>def get_xy_extents(\n    filepath,\n    x: str,\n    y: str,\n) -&gt; Tuple[int]:\n    \"\"\"\n    Get the bounding box of the x and y coordinates from a Parquet file.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the Parquet file.\n    x : str\n        The name of the column representing the x-coordinate.\n    y : str\n        The name of the column representing the y-coordinate.\n\n    Returns\n    -------\n    shapely.Polygon\n        A polygon representing the bounding box of the x and y coordinates.\n    \"\"\"\n    # Get index of columns of parquet file\n    metadata = pq.read_metadata(filepath)\n    schema_idx = dict(map(reversed, enumerate(metadata.schema.names)))\n\n    # Find min and max values across all row groups\n    x_max = -1\n    x_min = sys.maxsize\n    y_max = -1\n    y_min = sys.maxsize\n    for i in range(metadata.num_row_groups):\n        group = metadata.row_group(i)\n        x_min = min(x_min, group.column(schema_idx[x]).statistics.min)\n        x_max = max(x_max, group.column(schema_idx[x]).statistics.max)\n        y_min = min(y_min, group.column(schema_idx[y]).statistics.min)\n        y_max = max(y_max, group.column(schema_idx[y]).statistics.max)\n    return x_min, y_min, x_max, y_max\n</code></pre>"},{"location":"api/models/","title":"segger.models","text":"<p>The <code>segger.models</code> module provides the core machine learning models for the Segger framework, specifically designed for spatial transcriptomics data analysis using Graph Neural Networks (GNNs). This module implements attention-based convolutional architectures that can handle heterogeneous graphs with transcript and boundary nodes, formulating cell segmentation as a transcript-to-cell link prediction task.</p> <p>\ud83d\udcd6 Detailed Module Guide - For a comprehensive understanding of the models module, including architecture, examples, and best practices.</p> <p>\ud83d\udd0d Complete API Reference - For detailed API documentation of all classes, functions, and modules.</p>"},{"location":"api/models/#overview","title":"Overview","text":"<p>The models module serves as the machine learning engine for spatial transcriptomics analysis in Segger, offering:</p> <ul> <li>Graph Neural Network Architecture: Attention-based convolutional layers for spatial data analysis</li> <li>Heterogeneous Graph Support: Handles different node types (transcripts, boundaries) with specialized processing</li> <li>Spatial Relationship Learning: Learns complex spatial relationships between transcripts and cellular structures</li> <li>Link Prediction Framework: Formulates cell segmentation as a transcript-to-cell assignment problem</li> <li>PyTorch Integration: Seamless integration with PyTorch and PyTorch Geometric ecosystems</li> <li>Scalable Training: Support for both single and multi-GPU training workflows</li> </ul>"},{"location":"api/models/#core-architecture","title":"Core Architecture","text":"<p>The module is built around a sophisticated GNN architecture that processes spatial transcriptomics data:</p> <ul> <li><code>Segger</code>: Main GNN model with attention-based convolutional layers using GATv2Conv</li> <li>Heterogeneous Graph Processing: Handles different node and edge types with specialized attention mechanisms</li> <li>Attention Mechanisms: Multi-head Graph Attention Networks for learning node relationships</li> <li>Feature Engineering: Automatic feature transformation and embedding for transcripts and boundaries</li> </ul>"},{"location":"api/models/#key-features","title":"Key Features","text":""},{"location":"api/models/#advanced-gnn-architecture","title":"Advanced GNN Architecture","text":"<ul> <li>Graph Attention Networks: GATv2Conv layers for learning node relationships with flexible attention</li> <li>Heterogeneous Processing: Specialized handling for transcript and boundary nodes with different feature types</li> <li>Multi-head Attention: Parallel attention mechanisms for robust feature learning</li> <li>Residual Connections: Stabilized learning with configurable layer depth</li> </ul>"},{"location":"api/models/#spatial-data-optimization","title":"Spatial Data Optimization","text":"<ul> <li>Node Type Differentiation: Automatic detection and processing of different node types</li> <li>Spatial Relationship Learning: Captures complex spatial interactions through graph structure</li> <li>Feature Embedding: Efficient transformation of spatial and transcript features</li> <li>Memory Optimization: Optimized for large spatial transcriptomics datasets</li> </ul>"},{"location":"api/models/#training-deployment","title":"Training &amp; Deployment","text":"<ul> <li>PyTorch Integration: Native PyTorch module compatibility with full CUDA support</li> <li>PyTorch Geometric: Optimized for graph-based operations and heterogeneous graphs</li> <li>Multi-GPU Support: Scalable training across multiple devices with PyTorch Lightning</li> <li>Production Ready: Optimized inference and deployment capabilities</li> </ul>"},{"location":"api/models/#submodules","title":"Submodules","text":"<ul> <li>Segger Model: Core GNN architecture for spatial transcriptomics</li> <li>Architecture Details: Detailed model architecture and design principles</li> <li>Training Workflows: Training workflows and optimization strategies</li> <li>Inference &amp; Prediction: Model inference and prediction utilities</li> </ul>"},{"location":"api/models/#use-cases","title":"Use Cases","text":"<p>The models module is designed for:</p> <ul> <li>Research Scientists: Training GNNs on spatial transcriptomics data for cell segmentation</li> <li>ML Engineers: Building production-ready spatial analysis models with link prediction</li> <li>Bioinformaticians: Analyzing complex spatial gene expression patterns and cell relationships</li> <li>Software Developers: Integrating GNN models into spatial analysis pipelines</li> </ul>"},{"location":"api/models/#api-documentation","title":"API Documentation","text":""},{"location":"api/models/api_reference/","title":"Models Module API Reference","text":"<p>This page provides a comprehensive reference to all the classes, functions, and modules in the <code>segger.models</code> package.</p>"},{"location":"api/models/api_reference/#module-overview","title":"Module Overview","text":"<p>The <code>segger.models</code> package provides the machine learning core of the Segger framework, implementing Graph Neural Network architectures specifically designed for spatial transcriptomics data analysis.</p>"},{"location":"api/models/api_reference/#core-modules","title":"Core Modules","text":""},{"location":"api/models/api_reference/#segger-model","title":"Segger Model","text":"<p>The main GNN architecture module containing the core Segger model.</p> <p>Key Classes: - <code>Segger</code>: Main Graph Neural Network model for spatial transcriptomics</p> <p>Main Functions: - Graph attention network processing - Heterogeneous node type handling - Spatial relationship learning - PyTorch integration</p>"},{"location":"api/models/api_reference/#architecture-details","title":"Architecture Details","text":"<p>Comprehensive documentation of the Segger model architecture and design principles.</p> <p>Key Topics: - Heterogeneous graph representation - Link prediction framework - GATv2Conv implementation - Multi-head attention mechanisms - Performance characteristics</p>"},{"location":"api/models/api_reference/#training-workflows","title":"Training Workflows","text":"<p>Complete guide to training the Segger model with PyTorch Lightning.</p> <p>Key Topics: - PyTorch Lightning integration - Multi-GPU training strategies - Data preparation and validation - Performance optimization - Troubleshooting and best practices</p>"},{"location":"api/models/api_reference/#inference-prediction","title":"Inference &amp; Prediction","text":"<p>Comprehensive guide to using trained models for inference and cell segmentation.</p> <p>Key Topics: - Model loading and configuration - Inference pipeline workflow - Fragment detection - Performance optimization - Output formats and post-processing</p>"},{"location":"api/models/api_reference/#core-classes","title":"Core Classes","text":""},{"location":"api/models/api_reference/#segger","title":"Segger","text":"<p>               Bases: <code>Module</code></p> <p>Initializes the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>num_tx_tokens</code> <code>int)  </code> <p>Number of unique 'tx' tokens for embedding.</p> required <code>init_emb</code> <code>int)       </code> <p>Initial embedding size for both 'tx' and boundary (non-token) nodes.</p> <code>16</code> <code>hidden_channels</code> <code>int</code> <p>Number of hidden channels.</p> <code>32</code> <code>num_mid_layers</code> <code>int) </code> <p>Number of hidden layers (excluding first and last layers).</p> <code>3</code> <code>out_channels</code> <code>int)   </code> <p>Number of output channels.</p> <code>32</code> <code>heads</code> <code>int)          </code> <p>Number of attention heads.</p> <code>3</code> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def __init__(\n    self,\n    num_tx_tokens: int,\n    init_emb: int = 16,\n    hidden_channels: int = 32,\n    num_mid_layers: int = 3,\n    out_channels: int = 32,\n    heads: int = 3,\n):\n    \"\"\"\n    Initializes the Segger model.\n\n    Args:\n        num_tx_tokens (int)  : Number of unique 'tx' tokens for embedding.\n        init_emb (int)       : Initial embedding size for both 'tx' and boundary (non-token) nodes.\n        hidden_channels (int): Number of hidden channels.\n        num_mid_layers (int) : Number of hidden layers (excluding first and last layers).\n        out_channels (int)   : Number of output channels.\n        heads (int)          : Number of attention heads.\n    \"\"\"\n    super().__init__()\n\n    # Embedding for 'tx' (transcript) nodes\n    self.tx_embedding = Embedding(num_tx_tokens, init_emb)\n\n    # Linear layer for boundary (non-token) nodes\n    self.lin0 = Linear(-1, init_emb, bias=False)\n\n    # First GATv2Conv layer\n    self.conv_first = GATv2Conv(\n        (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n    )\n    # self.lin_first = Linear(-1, hidden_channels * heads)\n\n    # Middle GATv2Conv layers\n    self.num_mid_layers = num_mid_layers\n    if num_mid_layers &gt; 0:\n        self.conv_mid_layers = torch.nn.ModuleList()\n        # self.lin_mid_layers = torch.nn.ModuleList()\n        for _ in range(num_mid_layers):\n            self.conv_mid_layers.append(\n                GATv2Conv(\n                    (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n                )\n            )\n            # self.lin_mid_layers.append(Linear(-1, hidden_channels * heads))\n\n    # Last GATv2Conv layer\n    self.conv_last = GATv2Conv(\n        (-1, -1), out_channels, heads=heads, add_self_loops=False\n    )\n</code></pre>"},{"location":"api/models/api_reference/#src.segger.models.segger_model.Segger.decode","title":"decode","text":"<pre><code>decode(z, edge_index)\n</code></pre> <p>Decode the node embeddings to predict edge values.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Node embeddings.</p> required <code>edge_index</code> <code>EdgeIndex</code> <p>Edge label indices.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Predicted edge values.</p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def decode(self, z: Tensor, edge_index: Union[Tensor]) -&gt; Tensor:\n    \"\"\"\n    Decode the node embeddings to predict edge values.\n\n    Args:\n        z (Tensor): Node embeddings.\n        edge_index (EdgeIndex): Edge label indices.\n\n    Returns:\n        Tensor: Predicted edge values.\n    \"\"\"\n    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n</code></pre>"},{"location":"api/models/api_reference/#src.segger.models.segger_model.Segger.forward","title":"forward","text":"<pre><code>forward(x, edge_index)\n</code></pre> <p>Forward pass for the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Node features.</p> required <code>edge_index</code> <code>Tensor</code> <p>Edge indices.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Output node embeddings.</p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def forward(self, x: Tensor, edge_index: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass for the Segger model.\n\n    Args:\n        x (Tensor): Node features.\n        edge_index (Tensor): Edge indices.\n\n    Returns:\n        Tensor: Output node embeddings.\n    \"\"\"\n    x = torch.nan_to_num(x, nan=0)\n    is_one_dim = (x.ndim == 1) * 1\n    x = x[:, None]\n    x = self.tx_embedding(\n        ((x.sum(-1) * is_one_dim).int())\n    ) * is_one_dim + self.lin0(x.float()) * (1 - is_one_dim)\n    x = x.squeeze()\n    # First layer\n    x = x.relu()\n    x = self.conv_first(x, edge_index)  # + self.lin_first(x)\n    x = x.relu()\n\n    # Middle layers\n    if self.num_mid_layers &gt; 0:\n        for conv_mid in self.conv_mid_layers:\n            x = conv_mid(x, edge_index)  # + lin_mid(x)\n            x = x.relu()\n\n    # Last layer\n    x = self.conv_last(x, edge_index)  # + self.lin_last(x)\n\n    # x = F.normalize(x)\n\n    return x\n</code></pre>"},{"location":"api/models/api_reference/#model-architecture","title":"Model Architecture","text":"<p>The Segger model is built around a sophisticated Graph Neural Network architecture:</p>"},{"location":"api/models/api_reference/#input-processing","title":"Input Processing","text":"<p>The model automatically handles different input types:</p> <ul> <li>Transcript Nodes: 1D features processed through embedding layers</li> <li>Boundary Nodes: Multi-dimensional features processed through linear layers</li> <li>Mixed Features: Automatic routing based on feature dimensions</li> </ul>"},{"location":"api/models/api_reference/#graph-attention-layers","title":"Graph Attention Layers","text":"<p>Multiple GATv2Conv layers process the graph:</p> <ol> <li>Initial Layer: Transforms input features to hidden representations</li> <li>Middle Layers: Learn complex spatial relationships</li> <li>Final Layer: Produces output embeddings</li> </ol>"},{"location":"api/models/api_reference/#output-processing","title":"Output Processing","text":"<p>Final node embeddings can be used for:</p> <ul> <li>Node Classification: Predicting cell types or states</li> <li>Link Prediction: Predicting spatial relationships</li> <li>Graph Classification: Tissue-level analysis</li> <li>Downstream Tasks: Integration with other ML models</li> </ul>"},{"location":"api/models/api_reference/#data-flow","title":"Data Flow","text":"<p>The typical model workflow follows this pattern:</p> <pre><code>Spatial Data \u2192 Graph Construction \u2192 Segger Model \u2192 Node Embeddings \u2192 Downstream Tasks\n     \u2193              \u2193                \u2193              \u2193\nTranscripts    Spatial Edges    GNN Processing   Learned Features\nBoundaries     Node Features    Attention Mech.   Biological Insights\n</code></pre> <ol> <li>Data Preparation: Spatial data converted to graph format</li> <li>Model Initialization: Segger model configured for specific task</li> <li>Forward Pass: Graph processed through attention layers</li> <li>Feature Learning: Spatial relationships captured in embeddings</li> <li>Application: Embeddings used for biological analysis</li> </ol>"},{"location":"api/models/api_reference/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/api_reference/#basic-model-usage","title":"Basic Model Usage","text":"<pre><code>from segger.models.segger_model import Segger\n\n# Initialize model\nmodel = Segger(\n    num_tx_tokens=5000,\n    init_emb=32,\n    hidden_channels=64,\n    num_mid_layers=3,\n    out_channels=128,\n    heads=4\n)\n\n# Forward pass\noutput = model(x, edge_index)\n</code></pre>"},{"location":"api/models/api_reference/#training-configuration","title":"Training Configuration","text":"<pre><code>import torch.nn as nn\nimport torch.optim as optim\n\n# Model configuration\nmodel = Segger(\n    num_tx_tokens=10000,\n    hidden_channels=128,\n    out_channels=256\n)\n\n# Optimizer and loss\noptimizer = optim.AdamW(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    out = model(x, edge_index)\n    loss = criterion(out, labels)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"api/models/api_reference/#pytorch-lightning-integration","title":"PyTorch Lightning Integration","text":"<pre><code>import pytorch_lightning as pl\n\nclass SeggerModule(pl.LightningModule):\n    def __init__(self, num_tx_tokens, hidden_channels):\n        super().__init__()\n        self.model = Segger(\n            num_tx_tokens=num_tx_tokens,\n            hidden_channels=hidden_channels\n        )\n\n    def forward(self, x, edge_index):\n        return self.model(x, edge_index)\n\n    def training_step(self, batch, batch_idx):\n        x, edge_index, labels = batch\n        out = self(x, edge_index)\n        loss = F.cross_entropy(out, labels)\n        return loss\n\n# Training\ntrainer = pl.Trainer(max_epochs=100)\ntrainer.fit(model, train_loader)\n</code></pre>"},{"location":"api/models/api_reference/#model-parameters","title":"Model Parameters","text":""},{"location":"api/models/api_reference/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>num_tx_tokens</code> (int): Number of unique transcript types in your dataset</li> <li>This determines the size of the transcript embedding layer</li> <li>Should match the number of unique genes/transcripts in your data</li> </ul>"},{"location":"api/models/api_reference/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>init_emb</code> (int, default=16): Initial embedding dimension</li> <li>Used for both transcript embeddings and boundary feature transformation</li> <li> <p>Larger values provide more expressive features but increase memory usage</p> </li> <li> <p><code>hidden_channels</code> (int, default=32): Number of hidden channels</p> </li> <li>Size of intermediate layer representations</li> <li> <p>Affects model capacity and computational cost</p> </li> <li> <p><code>num_mid_layers</code> (int, default=3): Number of hidden GAT layers</p> </li> <li>More layers enable learning of more complex patterns</li> <li> <p>Balance between expressiveness and overfitting</p> </li> <li> <p><code>out_channels</code> (int, default=32): Output embedding dimension</p> </li> <li>Size of final node representations</li> <li> <p>Should match your downstream task requirements</p> </li> <li> <p><code>heads</code> (int, default=3): Number of attention heads</p> </li> <li>Multiple heads learn different types of relationships</li> <li>More heads generally improve performance but increase computation</li> </ul>"},{"location":"api/models/api_reference/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/models/api_reference/#scalability","title":"Scalability","text":"<ul> <li>Memory Usage: Scales with graph size and model complexity</li> <li>Training Time: Linear scaling with number of layers and attention heads</li> <li>Inference: Optimized for fast prediction on new data</li> </ul>"},{"location":"api/models/api_reference/#optimization-features","title":"Optimization Features","text":"<ul> <li>Attention Mechanisms: Efficient learning of spatial relationships</li> <li>Residual Connections: Stable training for deep architectures</li> <li>Multi-head Processing: Parallel attention for robust learning</li> <li>Graph Optimization: Optimized for PyTorch Geometric operations</li> </ul>"},{"location":"api/models/api_reference/#integration-with-segger-pipeline","title":"Integration with Segger Pipeline","text":"<p>The Segger model is designed to work seamlessly with the broader Segger framework:</p> <ol> <li>Data Processing: Use <code>segger.data</code> modules to prepare spatial data</li> <li>Graph Construction: Convert spatial data to PyTorch Geometric format</li> <li>Model Training: Train Segger model on prepared graphs</li> <li>Inference: Use trained model for predictions and analysis</li> </ol>"},{"location":"api/models/api_reference/#error-handling","title":"Error Handling","text":"<p>The module includes comprehensive error handling:</p> <ul> <li>Input Validation: Checks for valid input data and dimensions</li> <li>Memory Management: Handles out-of-memory situations gracefully</li> <li>Graph Validation: Ensures proper graph structure and connectivity</li> <li>User Feedback: Clear error messages for common issues</li> </ul>"},{"location":"api/models/api_reference/#best-practices","title":"Best Practices","text":""},{"location":"api/models/api_reference/#model-architecture-selection","title":"Model Architecture Selection","text":"<p>Choose appropriate architecture based on:</p> <ul> <li>Data Size: Larger datasets benefit from deeper models</li> <li>Task Complexity: Complex tasks need more attention heads</li> <li>Computational Resources: Balance model size with available resources</li> </ul>"},{"location":"api/models/api_reference/#training-strategy","title":"Training Strategy","text":"<ul> <li>Learning Rate: Start with 0.001 and adjust based on convergence</li> <li>Batch Size: Use largest batch size that fits in memory</li> <li>Regularization: Apply weight decay and consider dropout</li> <li>Early Stopping: Monitor validation performance to prevent overfitting</li> </ul>"},{"location":"api/models/api_reference/#data-preparation","title":"Data Preparation","text":"<ul> <li>Graph Construction: Ensure proper edge construction for spatial relationships</li> <li>Feature Engineering: Provide meaningful input features</li> <li>Normalization: Normalize features for stable training</li> <li>Validation Split: Use spatial-aware validation strategies</li> </ul>"},{"location":"api/models/api_reference/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api/models/api_reference/#research-applications","title":"Research Applications","text":"<ul> <li>Cell Type Identification: Process large tissue sections</li> <li>Spatial Gene Expression: Analyze gene expression patterns</li> <li>Tissue Architecture: Study spatial organization of cells</li> </ul>"},{"location":"api/models/api_reference/#machine-learning","title":"Machine Learning","text":"<ul> <li>Graph Neural Networks: Train GNNs on spatial transcriptomics data</li> <li>Transfer Learning: Adapt to new datasets and technologies</li> <li>Scalable Training: Process datasets too large for single machines</li> </ul>"},{"location":"api/models/api_reference/#pipeline-development","title":"Pipeline Development","text":"<ul> <li>Automated Processing: Build reproducible analysis pipelines</li> <li>Quality Control: Integrate model predictions and validation</li> <li>Multi-platform Support: Handle data from different technologies</li> </ul>"},{"location":"api/models/api_reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/models/api_reference/#common-issues","title":"Common Issues","text":"<ol> <li>Memory Errors: Reduce model size or batch size</li> <li>Training Instability: Lower learning rate or add regularization</li> <li>Poor Performance: Check data quality and feature engineering</li> <li>Slow Convergence: Adjust learning rate or model architecture</li> </ol>"},{"location":"api/models/api_reference/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use appropriate model size for your hardware</li> <li>Enable parallel processing for large datasets</li> <li>Cache frequently accessed data when possible</li> <li>Filter data early to reduce processing overhead</li> </ol>"},{"location":"api/models/api_reference/#future-developments","title":"Future Developments","text":"<p>The module is actively developed with plans for:</p> <ul> <li>Additional Architectures: Support for different GNN types</li> <li>Enhanced Attention: More sophisticated attention mechanisms</li> <li>Multi-modal Integration: Support for additional data types</li> <li>Cloud Computing: Support for distributed processing</li> </ul>"},{"location":"api/models/api_reference/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Areas for improvement include:</p> <ul> <li>New Architectures: Additional GNN architectures and attention mechanisms</li> <li>Performance Optimization: Better training and inference performance</li> <li>Documentation: Examples, tutorials, and best practices</li> <li>Testing: Comprehensive test coverage and validation</li> </ul>"},{"location":"api/models/api_reference/#dependencies","title":"Dependencies","text":""},{"location":"api/models/api_reference/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>PyTorch: Core neural network functionality</li> <li>PyTorch Geometric: Graph neural network support</li> <li>NumPy: Numerical operations</li> </ul>"},{"location":"api/models/api_reference/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>PyTorch Lightning: Training framework integration</li> <li>CUDA: GPU acceleration support</li> <li>TensorBoard: Training visualization and monitoring</li> </ul>"},{"location":"api/models/architecture/","title":"Segger Model Architecture","text":""},{"location":"api/models/architecture/#overview","title":"Overview","text":"<p>The Segger model implements a sophisticated Graph Neural Network architecture specifically designed for spatial transcriptomics data analysis. The model formulates cell segmentation as a transcript-to-cell link prediction task, leveraging heterogeneous graphs with specialized attention mechanisms to learn complex spatial relationships.</p>"},{"location":"api/models/architecture/#core-design-principles","title":"Core Design Principles","text":""},{"location":"api/models/architecture/#1-heterogeneous-graph-representation","title":"1. Heterogeneous Graph Representation","text":"<p>Segger models spatial transcriptomics data as a heterogeneous graph G = (V, E) with two distinct node types:</p> <ul> <li>Transcript Nodes (T): Represent individual gene expression measurements with spatial coordinates</li> <li>Boundary Nodes (C): Represent cell or region boundaries as geometric polygons</li> </ul> <p>The graph contains two types of edges: - ETT: Transcript-to-transcript edges capturing spatial colocalization - ETC: Transcript-to-cell edges representing initial assignments</p>"},{"location":"api/models/architecture/#2-link-prediction-framework","title":"2. Link Prediction Framework","text":"<p>Rather than traditional image-based segmentation, Segger frames cell segmentation as a link prediction problem:</p> <pre><code>Given: Heterogeneous graph with transcript and boundary nodes\nTask: Predict transcript-to-cell associations\nOutput: Probability scores for each transcript-cell pair\n</code></pre> <p>This approach enables the model to: - Learn from spatial relationships between transcripts - Leverage biological priors through gene embeddings - Handle incomplete initial segmentations - Identify both cells and cell fragments</p>"},{"location":"api/models/architecture/#model-architecture-details","title":"Model Architecture Details","text":""},{"location":"api/models/architecture/#input-processing-layer","title":"Input Processing Layer","text":"<p>The model automatically handles different input types through specialized processing:</p>"},{"location":"api/models/architecture/#transcript-node-processing","title":"Transcript Node Processing","text":"<pre><code># For transcript nodes with gene embeddings\nif has_scrnaseq_embeddings:\n    x_tx = gene_celltype_embeddings[gene_labels]\nelse:\n    # One-hot encoding fallback\n    x_tx = embedding_layer(gene_token_ids)\n</code></pre>"},{"location":"api/models/architecture/#boundary-node-processing","title":"Boundary Node Processing","text":"<pre><code># For boundary nodes, compute geometric features\nboundary_features = [\n    area(Bi),           # Surface area\n    convexity(Bi),      # A(Conv(Bi))/A(Bi)\n    elongation(Bi),     # A(MBR(Bi))/A(Env(Bi))\n    circularity(Bi)     # A(Bi)/r_min(Bi)\u00b2\n]\nx_bd = linear_transform(boundary_features)\n</code></pre>"},{"location":"api/models/architecture/#graph-attention-layers","title":"Graph Attention Layers","text":"<p>The core of the model consists of multiple GATv2Conv layers:</p>"},{"location":"api/models/architecture/#layer-structure","title":"Layer Structure","text":"<pre><code>Input Features \u2192 Node Type Detection \u2192 Feature Processing \u2192 GATv2Conv Layers \u2192 Output Embeddings\n     \u2193              \u2193                    \u2193                \u2193                \u2193\nTranscripts    Auto-routing         Embedding/Linear   Attention Mech.   Learned Features\nBoundaries     (1D vs Multi-D)     Transformations    Spatial Learning   Biological Insights\n</code></pre>"},{"location":"api/models/architecture/#gatv2conv-implementation","title":"GATv2Conv Implementation","text":"<p>Each GATv2Conv layer computes attention coefficients:</p> <pre><code># Attention computation for edge (i, j)\ne_ij = a^T LeakyReLU([Wh_i || Wh_j])\n\n# Normalized attention weights\n\u03b1_ij = softmax(e_ij) = exp(e_ij) / \u03a3_k exp(e_ik)\n\n# Node update\nh_i^(l+1) = \u03c3(\u03a3_j\u2208N(i) \u03b1_ij W^(l) h_j^(l))\n</code></pre> <p>Where: - <code>W^(l)</code> is a learnable weight matrix for layer l - <code>a</code> is a learnable attention vector - <code>\u03c3</code> is the activation function (ReLU) - <code>N(i)</code> represents the neighborhood of node i</p>"},{"location":"api/models/architecture/#multi-head-attention","title":"Multi-head Attention","text":"<p>The model uses multiple attention heads in parallel:</p> <pre><code># Multi-head attention with K heads\nh_i^(l+1) = ||_k=1^K \u03c3(\u03a3_j\u2208N(i) \u03b1_ij^(k) W^(k) h_j^(l))\n</code></pre> <p>This allows the model to capture different types of relationships simultaneously.</p>"},{"location":"api/models/architecture/#layer-configuration","title":"Layer Configuration","text":"<p>The model architecture is configurable with the following parameters:</p> <pre><code>class Segger(torch.nn.Module):\n    def __init__(\n        self,\n        num_tx_tokens: int,      # Vocabulary size for transcripts\n        init_emb: int = 16,      # Initial embedding dimension\n        hidden_channels: int = 32, # Hidden layer size\n        num_mid_layers: int = 3,  # Number of hidden GAT layers\n        out_channels: int = 32,   # Output embedding dimension\n        heads: int = 3,           # Number of attention heads\n    ):\n</code></pre>"},{"location":"api/models/architecture/#recommended-architecture-sizes","title":"Recommended Architecture Sizes","text":"<p>Based on the Segger paper:</p> <ul> <li>Small Datasets (&lt; 10k nodes): 2-3 layers, 32-64 hidden channels</li> <li>Medium Datasets (10k-100k nodes): 3-4 layers, 64-128 hidden channels  </li> <li>Large Datasets (&gt; 100k nodes): 4-5 layers, 128-256 hidden channels</li> </ul>"},{"location":"api/models/architecture/#output-processing","title":"Output Processing","text":"<p>The final layer produces embeddings in a common latent space:</p> <pre><code># Final embeddings for link prediction\nf(t_i) \u2208 R^d3  # Transcript embedding\nf(c_j) \u2208 R^d3  # Cell embedding\n\n# Similarity score computation\ns_ij = \u27e8f(t_i), f(c_j)\u27e9 = f(t_i)^T f(c_j)\n\n# Probability of association\nP(link_ij) = \u03c3(s_ij) = 1 / (1 + e^(-s_ij))\n</code></pre>"},{"location":"api/models/architecture/#heterogeneous-graph-processing","title":"Heterogeneous Graph Processing","text":""},{"location":"api/models/architecture/#edge-type-handling","title":"Edge Type Handling","text":"<p>The model processes different edge types with specialized attention:</p>"},{"location":"api/models/architecture/#transcript-transcript-edges-ett","title":"Transcript-Transcript Edges (ETT)","text":"<ul> <li>Purpose: Capture spatial proximity and gene co-expression patterns</li> <li>Construction: k-nearest neighbor graph with distance threshold</li> <li>Parameters: <code>k_tx</code> (number of neighbors), <code>dist_tx</code> (maximum distance)</li> </ul>"},{"location":"api/models/architecture/#transcript-cell-edges-etc","title":"Transcript-Cell Edges (ETC)","text":"<ul> <li>Purpose: Represent initial transcript-to-cell assignments</li> <li>Construction: Spatial overlap between transcripts and boundaries</li> <li>Training: Used as positive examples for link prediction</li> </ul>"},{"location":"api/models/architecture/#message-passing-mechanism","title":"Message Passing Mechanism","text":"<p>Information flows through the heterogeneous graph:</p> <pre><code>Transcript Nodes \u2190\u2192 Transcript Nodes (spatial proximity)\n       \u2193                    \u2191\n       \u2193                    \u2191\n    Cell Nodes \u2190\u2192 Transcript Nodes (containment)\n</code></pre> <p>This enables: - Spatial Context: Transcripts learn from nearby neighbors - Biological Context: Transcripts learn from cell assignments - Geometric Context: Cells learn from transcript distributions</p>"},{"location":"api/models/architecture/#training-strategy","title":"Training Strategy","text":""},{"location":"api/models/architecture/#loss-function","title":"Loss Function","text":"<p>The model uses binary cross-entropy loss for link prediction:</p> <pre><code>L = -\u03a3_(t_i,c_j)\u2208E_TC [y_ij log(\u03c3(s_ij)) + (1-y_ij) log(1-\u03c3(s_ij))]\n</code></pre> <p>Where: - <code>y_ij = 1</code> for positive edges (observed transcript-cell associations) - <code>y_ij = 0</code> for negative edges (sampled non-associations)</p>"},{"location":"api/models/architecture/#negative-sampling","title":"Negative Sampling","text":"<p>To handle class imbalance, negative edges are sampled:</p> <pre><code># Sample negative edges at ratio 1:5 (positive:negative)\nE^- = random_sample(E_TC^c, size=5|E^+|)\n</code></pre>"},{"location":"api/models/architecture/#training-process","title":"Training Process","text":"<ol> <li>Forward Pass: Compute embeddings for all nodes</li> <li>Link Prediction: Calculate similarity scores for all transcript-cell pairs</li> <li>Loss Computation: Binary cross-entropy on positive and negative edges</li> <li>Backpropagation: Update model parameters</li> <li>Validation: Monitor AUROC and F1 score on validation set</li> </ol>"},{"location":"api/models/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/models/architecture/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Time per layer: O(|E| \u00d7 F \u00d7 H)</li> <li>|E|: Number of edges</li> <li>F: Feature dimension</li> <li> <p>H: Number of attention heads</p> </li> <li> <p>Memory usage: O(|V| \u00d7 F + |E| \u00d7 H)</p> </li> <li>|V|: Number of nodes</li> <li>|E|: Number of edges</li> </ul>"},{"location":"api/models/architecture/#scalability-features","title":"Scalability Features","text":"<ul> <li>Efficient Attention: GATv2Conv optimized for sparse graphs</li> <li>GPU Acceleration: Full CUDA support with PyTorch</li> <li>Batch Processing: Mini-batch training for large datasets</li> <li>Multi-GPU: Distributed training with PyTorch Lightning</li> </ul>"},{"location":"api/models/architecture/#integration-with-segger-pipeline","title":"Integration with Segger Pipeline","text":""},{"location":"api/models/architecture/#data-flow","title":"Data Flow","text":"<pre><code>Spatial Data \u2192 Graph Construction \u2192 Segger Model \u2192 Node Embeddings \u2192 Link Prediction \u2192 Cell Segmentation\n     \u2193              \u2193                \u2193              \u2193                \u2193                \u2193\nTranscripts    Heterogeneous    GNN Processing   Learned Features   Similarity      Final\nBoundaries     Graph Creation   Attention Mech.   Spatial Context    Scores         Assignments\n</code></pre>"},{"location":"api/models/architecture/#key-integration-points","title":"Key Integration Points","text":"<ol> <li>Data Preprocessing: Tiled graph construction with balanced regions</li> <li>Model Training: PyTorch Lightning integration with validation</li> <li>Inference: Batch-wise prediction with GPU acceleration</li> <li>Post-processing: Connected components for unassigned transcripts</li> </ol>"},{"location":"api/models/architecture/#best-practices","title":"Best Practices","text":""},{"location":"api/models/architecture/#architecture-selection","title":"Architecture Selection","text":"<ul> <li>Layer Depth: Balance expressiveness with over-smoothing (3-5 layers recommended)</li> <li>Hidden Dimensions: Scale with dataset size and task complexity</li> <li>Attention Heads: Use 4-8 heads for robust learning</li> <li>Embedding Size: Match output dimension to downstream tasks</li> </ul>"},{"location":"api/models/architecture/#training-configuration","title":"Training Configuration","text":"<ul> <li>Learning Rate: Start with 0.001 and adjust based on convergence</li> <li>Batch Size: Use largest size that fits in memory</li> <li>Regularization: Apply weight decay (1e-5) to prevent overfitting</li> <li>Early Stopping: Monitor validation AUROC to prevent overfitting</li> </ul>"},{"location":"api/models/architecture/#data-preparation","title":"Data Preparation","text":"<ul> <li>Graph Construction: Ensure proper spatial edge construction</li> <li>Feature Engineering: Provide meaningful transcript and boundary features</li> <li>Validation Strategy: Use spatial-aware train/val/test splits</li> <li>Quality Control: Filter low-quality transcripts and boundaries</li> </ul>"},{"location":"api/models/architecture/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements include:</p> <ul> <li>Additional Attention Types: Support for different attention mechanisms</li> <li>Multi-modal Integration: Support for additional data types (images, protein markers)</li> <li>Distributed Training: Multi-node training capabilities</li> <li>Model Compression: Efficient deployment of trained models</li> <li>Interpretability Tools: Understanding learned spatial relationships</li> </ul>"},{"location":"api/models/architecture/#references","title":"References","text":"<ul> <li>Graph Attention Networks: Veli\u010dkovi\u0107 et al. (2018) - ICLR</li> <li>GATv2: Brody et al. (2022) - ICLR  </li> <li>Heterogeneous GNNs: Hong et al. (2020) - AAAI</li> <li>Link Prediction: Kipf &amp; Welling (2016) - arXiv</li> <li>Spatial Transcriptomics: Nature Reviews Genetics (2016)</li> </ul>"},{"location":"api/models/inference/","title":"Segger Model Inference","text":""},{"location":"api/models/inference/#overview","title":"Overview","text":"<p>Inference with the trained Segger model involves using the learned Graph Neural Network to predict transcript-to-cell associations in spatial transcriptomics data. The inference process transforms spatial data into cell segmentation results through link prediction, enabling both cell identification and fragment detection.</p>"},{"location":"api/models/inference/#inference-pipeline","title":"Inference Pipeline","text":""},{"location":"api/models/inference/#overall-workflow","title":"Overall Workflow","text":"<pre><code>Trained Model \u2192 Spatial Data \u2192 Graph Construction \u2192 Node Embeddings \u2192 Similarity Scores \u2192 Cell Assignment \u2192 Post-processing\n     \u2193              \u2193              \u2193                \u2193              \u2193              \u2193              \u2193\nLearned Weights   Transcripts    Heterogeneous   GNN Forward    Link Prediction  Thresholding   Final Results\n                 Boundaries     Graphs          Pass            Scores           Filtering      + Fragments\n</code></pre>"},{"location":"api/models/inference/#key-steps","title":"Key Steps","text":"<ol> <li>Model Loading: Load trained Segger model from checkpoint</li> <li>Data Preparation: Construct heterogeneous graphs from spatial data</li> <li>Embedding Generation: Generate node embeddings using the trained model</li> <li>Similarity Computation: Calculate transcript-to-cell similarity scores</li> <li>Assignment Decision: Assign transcripts to cells based on confidence scores</li> <li>Fragment Detection: Group unassigned transcripts into fragments</li> </ol>"},{"location":"api/models/inference/#model-loading","title":"Model Loading","text":""},{"location":"api/models/inference/#loading-trained-model","title":"Loading Trained Model","text":"<pre><code>from segger.models.segger_model import Segger\nimport torch\n\n# Load trained model\nmodel = Segger(\n    num_tx_tokens=5000,\n    init_emb=16,\n    hidden_channels=64,\n    num_mid_layers=3,\n    out_channels=32,\n    heads=4\n)\n\n# Load trained weights\ncheckpoint = torch.load('path/to/checkpoint.ckpt')\nmodel.load_state_dict(checkpoint['state_dict'])\nmodel.eval()  # Set to evaluation mode\n</code></pre>"},{"location":"api/models/inference/#model-configuration","title":"Model Configuration","text":"<p>Ensure inference parameters match training configuration:</p> <pre><code># Verify model configuration matches training\nassert model.num_tx_tokens == 5000, \"Vocabulary size mismatch\"\nassert model.hidden_channels == 64, \"Hidden dimension mismatch\"\nassert model.out_channels == 32, \"Output dimension mismatch\"\nassert model.heads == 4, \"Attention heads mismatch\"\n</code></pre>"},{"location":"api/models/inference/#data-preparation","title":"Data Preparation","text":""},{"location":"api/models/inference/#graph-construction","title":"Graph Construction","text":"<p>Inference requires the same graph structure used during training:</p> <pre><code># Construct heterogeneous graph for inference\ndata = {\n    \"tx\": {  # Transcript nodes\n        \"id\": transcript_ids,\n        \"pos\": spatial_coordinates,\n        \"x\": feature_vectors\n    },\n    \"bd\": {  # Boundary nodes\n        \"id\": boundary_ids,\n        \"pos\": centroid_coordinates,\n        \"x\": geometric_features\n    },\n    \"tx,neighbors,tx\": {  # Transcript proximity edges\n        \"edge_index\": neighbor_connections\n    },\n    \"tx,belongs,bd\": {  # Transcript-boundary edges (for inference)\n        \"edge_index\": containment_relationships\n    }\n}\n</code></pre>"},{"location":"api/models/inference/#feature-processing","title":"Feature Processing","text":""},{"location":"api/models/inference/#transcript-features","title":"Transcript Features","text":"<pre><code># Use same feature processing as training\nif has_scrnaseq_embeddings:\n    transcript_features = gene_celltype_embeddings[gene_labels]\nelse:\n    transcript_features = embedding_layer(gene_token_ids)\n</code></pre>"},{"location":"api/models/inference/#boundary-features","title":"Boundary Features","text":"<pre><code># Compute geometric features for boundaries\ndef compute_boundary_features(boundary_polygons):\n    features = []\n    for polygon in boundary_polygons:\n        area_val = polygon.area\n        convex_hull = polygon.convex_hull\n        convexity = convex_hull.area / area_val\n\n        # Minimum bounding rectangle\n        mbr = polygon.minimum_rotated_rectangle\n        mbr_area = mbr.area\n\n        # Envelope (axis-aligned bounding box)\n        envelope = polygon.envelope\n        env_area = envelope.area\n        elongation = mbr_area / env_area\n\n        # Circularity\n        min_radius = compute_minimum_bounding_radius(polygon)\n        circularity = area_val / (min_radius ** 2)\n\n        features.append([area_val, convexity, elongation, circularity])\n\n    return torch.tensor(features, dtype=torch.float32)\n</code></pre>"},{"location":"api/models/inference/#inference-process","title":"Inference Process","text":""},{"location":"api/models/inference/#forward-pass","title":"Forward Pass","text":"<pre><code># Generate node embeddings\nwith torch.no_grad():\n    embeddings = model(data.x, data.edge_index)\n\n# Separate transcript and boundary embeddings\ntx_embeddings = embeddings[data.tx_mask]\nbd_embeddings = embeddings[data.bd_mask]\n</code></pre>"},{"location":"api/models/inference/#similarity-score-computation","title":"Similarity Score Computation","text":""},{"location":"api/models/inference/#transcript-to-cell-similarity","title":"Transcript-to-Cell Similarity","text":"<pre><code>def compute_similarity_scores(tx_embeddings, bd_embeddings, edge_index):\n    \"\"\"Compute similarity scores between transcripts and boundaries.\"\"\"\n\n    # Extract source and target indices\n    tx_indices = edge_index[0]\n    bd_indices = edge_index[1]\n\n    # Get embeddings for connected nodes\n    tx_emb = tx_embeddings[tx_indices]\n    bd_emb = bd_embeddings[bd_indices]\n\n    # Compute dot product similarity\n    similarity_scores = torch.sum(tx_emb * bd_emb, dim=1)\n\n    # Apply sigmoid for probability\n    probabilities = torch.sigmoid(similarity_scores)\n\n    return probabilities, similarity_scores\n</code></pre>"},{"location":"api/models/inference/#receptive-field-construction","title":"Receptive Field Construction","text":"<pre><code>def construct_receptive_field(transcripts, boundaries, k_bd=3, dist_bd=10.0):\n    \"\"\"Construct receptive field for transcript-to-cell assignment.\"\"\"\n\n    # Find nearest boundary cells for each transcript\n    from sklearn.neighbors import NearestNeighbors\n\n    # Extract coordinates\n    tx_coords = transcripts[['x', 'y']].values\n    bd_coords = boundaries[['centroid_x', 'centroid_y']].values\n\n    # Build nearest neighbor index\n    nn = NearestNeighbors(n_neighbors=k_bd, radius=dist_bd)\n    nn.fit(bd_coords)\n\n    # Find neighbors\n    distances, indices = nn.kneighbors(tx_coords)\n\n    # Filter by distance threshold\n    mask = distances &lt;= dist_bd\n    filtered_indices = []\n    filtered_distances = []\n\n    for i, (dist, idx) in enumerate(zip(distances, indices)):\n        valid_mask = mask[i]\n        filtered_indices.append(idx[valid_mask])\n        filtered_distances.append(dist[valid_mask])\n\n    return filtered_indices, filtered_distances\n</code></pre>"},{"location":"api/models/inference/#cell-assignment","title":"Cell Assignment","text":""},{"location":"api/models/inference/#assignment-decision","title":"Assignment Decision","text":"<pre><code>def assign_transcripts_to_cells(similarity_scores, score_threshold=0.7):\n    \"\"\"Assign transcripts to cells based on similarity scores.\"\"\"\n\n    # Find best matching cell for each transcript\n    best_scores, best_cells = torch.max(similarity_scores, dim=1)\n\n    # Apply confidence threshold\n    confident_mask = best_scores &gt;= score_threshold\n\n    # Create assignment results\n    assignments = {\n        'transcript_id': [],\n        'cell_id': [],\n        'confidence_score': [],\n        'assigned': []\n    }\n\n    for i, (score, cell_id) in enumerate(zip(best_scores, best_cells)):\n        assignments['transcript_id'].append(i)\n        assignments['cell_id'].append(cell_id.item())\n        assignments['confidence_score'].append(score.item())\n        assignments['assigned'].append(confident_mask[i].item())\n\n    return assignments\n</code></pre>"},{"location":"api/models/inference/#confidence-score-analysis","title":"Confidence Score Analysis","text":"<pre><code>def analyze_confidence_scores(scores):\n    \"\"\"Analyze distribution of confidence scores.\"\"\"\n\n    import numpy as np\n    from scipy import stats\n\n    # Convert to numpy for analysis\n    scores_np = scores.detach().cpu().numpy()\n\n    # Basic statistics\n    stats_summary = {\n        'mean': np.mean(scores_np),\n        'std': np.std(scores_np),\n        'min': np.min(scores_np),\n        'max': np.max(scores_np),\n        'median': np.median(scores_np)\n    }\n\n    # Percentiles\n    percentiles = [25, 50, 75, 90, 95, 99]\n    for p in percentiles:\n        stats_summary[f'p{p}'] = np.percentile(scores_np, p)\n\n    # Find knee point for automatic thresholding\n    # Use the method from the Segger paper\n    knee_point = find_knee_point(scores_np)\n    stats_summary['knee_point'] = knee_point\n\n    return stats_summary\n\ndef find_knee_point(scores):\n    \"\"\"Find knee point in score distribution for automatic thresholding.\"\"\"\n\n    from kneed import KneeLocator\n\n    # Sort scores\n    sorted_scores = np.sort(scores)\n\n    # Create cumulative distribution\n    cumulative = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)\n\n    # Find knee point\n    kneedle = KneeLocator(\n        sorted_scores, cumulative, \n        S=1.0, curve='concave', direction='increasing'\n    )\n\n    return kneedle.knee if kneedle.knee else np.median(scores)\n</code></pre>"},{"location":"api/models/inference/#fragment-detection","title":"Fragment Detection","text":""},{"location":"api/models/inference/#unassigned-transcript-handling","title":"Unassigned Transcript Handling","text":"<pre><code>def detect_fragments(unassigned_transcripts, k_tx=4, dist_tx=5.0, similarity_threshold=0.5):\n    \"\"\"Group unassigned transcripts into fragments.\"\"\"\n\n    # Construct transcript-transcript similarity graph\n    fragment_graph = construct_fragment_graph(\n        unassigned_transcripts, k_tx, dist_tx, similarity_threshold\n    )\n\n    # Find connected components\n    from scipy.sparse.csgraph import connected_components\n\n    n_components, labels = connected_components(fragment_graph, directed=False)\n\n    # Assign fragment IDs\n    fragment_assignments = {}\n    for i, label in enumerate(labels):\n        transcript_id = unassigned_transcripts[i]\n        fragment_assignments[transcript_id] = f\"fragment_{label}\"\n\n    return fragment_assignments, n_components\n</code></pre>"},{"location":"api/models/inference/#fragment-graph-construction","title":"Fragment Graph Construction","text":"<pre><code>def construct_fragment_graph(transcripts, k_tx, dist_tx, similarity_threshold):\n    \"\"\"Construct similarity graph for unassigned transcripts.\"\"\"\n\n    # Extract coordinates and features\n    coords = transcripts[['x', 'y']].values\n    features = transcripts['features'].values\n\n    # Build nearest neighbor graph\n    nn = NearestNeighbors(n_neighbors=k_tx, radius=dist_tx)\n    nn.fit(coords)\n\n    # Find neighbors\n    distances, indices = nn.radius_neighbors(coords, radius=dist_tx)\n\n    # Compute similarity scores\n    edges = []\n    for i, neighbors in enumerate(indices):\n        for j in neighbors:\n            if i != j:\n                # Compute feature similarity\n                sim_score = compute_feature_similarity(features[i], features[j])\n\n                # Add edge if similarity exceeds threshold\n                if sim_score &gt;= similarity_threshold:\n                    edges.append((i, j, sim_score))\n\n    # Convert to sparse matrix\n    from scipy.sparse import csr_matrix\n\n    if edges:\n        rows, cols, data = zip(*edges)\n        n_transcripts = len(transcripts)\n        fragment_graph = csr_matrix((data, (rows, cols)), shape=(n_transcripts, n_transcripts))\n    else:\n        fragment_graph = csr_matrix((len(transcripts), len(transcripts)))\n\n    return fragment_graph\n\ndef compute_feature_similarity(feature1, feature2):\n    \"\"\"Compute similarity between transcript features.\"\"\"\n\n    # Cosine similarity\n    dot_product = np.dot(feature1, feature2)\n    norm1 = np.linalg.norm(feature1)\n    norm2 = np.linalg.norm(feature2)\n\n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n\n    similarity = dot_product / (norm1 * norm2)\n    return similarity\n</code></pre>"},{"location":"api/models/inference/#batch-processing","title":"Batch Processing","text":""},{"location":"api/models/inference/#large-dataset-handling","title":"Large Dataset Handling","text":"<pre><code>def batch_inference(model, data_loader, device='cuda'):\n    \"\"\"Perform inference on large datasets in batches.\"\"\"\n\n    model.to(device)\n    model.eval()\n\n    all_results = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            # Move batch to device\n            batch = batch.to(device)\n\n            # Generate embeddings\n            embeddings = model(batch.x, batch.edge_index)\n\n            # Compute similarity scores\n            scores = compute_batch_similarities(batch, embeddings)\n\n            # Store results\n            batch_results = {\n                'transcript_ids': batch.transcript_ids,\n                'cell_ids': batch.cell_ids,\n                'similarity_scores': scores,\n                'batch_idx': batch.batch\n            }\n\n            all_results.append(batch_results)\n\n    # Combine results from all batches\n    combined_results = combine_batch_results(all_results)\n\n    return combined_results\n</code></pre>"},{"location":"api/models/inference/#memory-management","title":"Memory Management","text":"<pre><code>def optimize_memory_usage(batch_size, model_size):\n    \"\"\"Optimize memory usage during inference.\"\"\"\n\n    # Estimate memory requirements\n    estimated_memory = estimate_inference_memory(batch_size, model_size)\n\n    # Adjust batch size if needed\n    if estimated_memory &gt; available_memory:\n        optimal_batch_size = find_optimal_batch_size(model_size, available_memory)\n        print(f\"Reducing batch size from {batch_size} to {optimal_batch_size}\")\n        return optimal_batch_size\n\n    return batch_size\n\ndef estimate_inference_memory(batch_size, model_size):\n    \"\"\"Estimate memory usage for inference.\"\"\"\n\n    # Model parameters\n    model_memory = model_size * 4  # 4 bytes per float32\n\n    # Activations (approximate)\n    activation_memory = batch_size * 1000 * 64 * 4  # Assume 1000 nodes, 64 features\n\n    # Total memory\n    total_memory = model_memory + activation_memory\n\n    return total_memory\n</code></pre>"},{"location":"api/models/inference/#post-processing","title":"Post-processing","text":""},{"location":"api/models/inference/#result-aggregation","title":"Result Aggregation","text":"<pre><code>def aggregate_inference_results(assignments, fragments):\n    \"\"\"Aggregate inference results into final segmentation.\"\"\"\n\n    # Combine cell assignments and fragment assignments\n    final_results = {\n        'transcript_id': [],\n        'final_cell_id': [],\n        'assignment_type': [],  # 'cell' or 'fragment'\n        'confidence_score': []\n    }\n\n    # Add cell assignments\n    for tx_id, cell_id, score, assigned in zip(\n        assignments['transcript_id'],\n        assignments['cell_id'],\n        assignments['confidence_score'],\n        assignments['assigned']\n    ):\n        if assigned:\n            final_results['transcript_id'].append(tx_id)\n            final_results['final_cell_id'].append(cell_id)\n            final_results['assignment_type'].append('cell')\n            final_results['confidence_score'].append(score)\n\n    # Add fragment assignments\n    for tx_id, fragment_id in fragments.items():\n        final_results['transcript_id'].append(tx_id)\n        final_results['final_cell_id'].append(fragment_id)\n        final_results['assignment_type'].append('fragment')\n        final_results['confidence_score'].append(0.0)  # No confidence for fragments\n\n    return final_results\n</code></pre>"},{"location":"api/models/inference/#quality-control","title":"Quality Control","text":"<pre><code>def quality_control_checks(results, min_transcripts_per_cell=5):\n    \"\"\"Perform quality control checks on inference results.\"\"\"\n\n    # Count transcripts per cell\n    from collections import Counter\n    cell_counts = Counter(results['final_cell_id'])\n\n    # Filter cells with too few transcripts\n    valid_cells = {cell_id: count for cell_id, count in cell_counts.items() \n                   if count &gt;= min_transcripts_per_cell}\n\n    # Filter results\n    filtered_results = {\n        'transcript_id': [],\n        'final_cell_id': [],\n        'assignment_type': [],\n        'confidence_score': []\n    }\n\n    for i, cell_id in enumerate(results['final_cell_id']):\n        if cell_id in valid_cells:\n            for key in filtered_results:\n                filtered_results[key].append(results[key][i])\n\n    return filtered_results, valid_cells\n</code></pre>"},{"location":"api/models/inference/#output-formats","title":"Output Formats","text":""},{"location":"api/models/inference/#standard-output","title":"Standard Output","text":"<pre><code>def save_inference_results(results, output_path, format='parquet'):\n    \"\"\"Save inference results in specified format.\"\"\"\n\n    import pandas as pd\n\n    # Convert to DataFrame\n    df = pd.DataFrame(results)\n\n    if format == 'parquet':\n        df.to_parquet(output_path, index=False)\n    elif format == 'csv':\n        df.to_csv(output_path, index=False)\n    elif format == 'h5ad':\n        # Convert to AnnData format\n        adata = convert_to_anndata(df)\n        adata.write(output_path)\n    else:\n        raise ValueError(f\"Unsupported format: {format}\")\n\n    print(f\"Results saved to {output_path}\")\n\ndef convert_to_anndata(results_df):\n    \"\"\"Convert results to AnnData format for downstream analysis.\"\"\"\n\n    import scanpy as sc\n    import anndata as ad\n\n    # Group by cell ID\n    cell_groups = results_df.groupby('final_cell_id')\n\n    # Create cell-gene matrix\n    cell_gene_matrix = []\n    cell_ids = []\n\n    for cell_id, group in cell_groups:\n        # Count transcripts per gene\n        gene_counts = group['gene_name'].value_counts()\n        cell_gene_matrix.append(gene_counts)\n        cell_ids.append(cell_id)\n\n    # Convert to DataFrame\n    cell_gene_df = pd.DataFrame(cell_gene_matrix, index=cell_ids)\n    cell_gene_df = cell_gene_df.fillna(0)\n\n    # Create AnnData object\n    adata = ad.AnnData(X=cell_gene_df.values, \n                       obs=pd.DataFrame(index=cell_ids),\n                       var=pd.DataFrame(index=cell_gene_df.columns))\n\n    return adata\n</code></pre>"},{"location":"api/models/inference/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/models/inference/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code>def optimize_gpu_inference(model, data, device='cuda'):\n    \"\"\"Optimize GPU inference performance.\"\"\"\n\n    # Move model and data to GPU\n    model = model.to(device)\n    data = data.to(device)\n\n    # Enable CUDA optimizations\n    torch.backends.cudnn.benchmark = True\n\n    # Use mixed precision if available\n    if hasattr(torch, 'autocast'):\n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            embeddings = model(data.x, data.edge_index)\n    else:\n        embeddings = model(data.x, data.edge_index)\n\n    return embeddings\n</code></pre>"},{"location":"api/models/inference/#parallel-processing","title":"Parallel Processing","text":"<pre><code>def parallel_inference(model, data_list, num_workers=4):\n    \"\"\"Perform inference in parallel across multiple workers.\"\"\"\n\n    from concurrent.futures import ProcessPoolExecutor\n    import multiprocessing as mp\n\n    # Set multiprocessing start method\n    mp.set_start_method('spawn', force=True)\n\n    # Split data across workers\n    chunk_size = len(data_list) // num_workers\n    data_chunks = [data_list[i:i+chunk_size] for i in range(0, len(data_list), chunk_size)]\n\n    # Process in parallel\n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(process_chunk, model, chunk) for chunk in data_chunks]\n        results = [future.result() for future in futures]\n\n    # Combine results\n    combined_results = combine_chunk_results(results)\n\n    return combined_results\n</code></pre>"},{"location":"api/models/inference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/models/inference/#common-issues","title":"Common Issues","text":""},{"location":"api/models/inference/#memory-errors","title":"Memory Errors","text":"<pre><code># Solutions:\n# 1. Reduce batch size\nbatch_size = 1  # Reduce from default\n\n# 2. Use gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# 3. Clear GPU cache\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"api/models/inference/#slow-inference","title":"Slow Inference","text":"<pre><code># Solutions:\n# 1. Enable CUDA optimizations\ntorch.backends.cudnn.benchmark = True\n\n# 2. Use mixed precision\nwith torch.autocast(device_type='cuda'):\n    embeddings = model(data.x, data.edge_index)\n\n# 3. Optimize data loading\ndata_loader = DataLoader(dataset, batch_size=batch_size, \n                        num_workers=4, pin_memory=True)\n</code></pre>"},{"location":"api/models/inference/#poor-quality-results","title":"Poor Quality Results","text":"<pre><code># Solutions:\n# 1. Check model configuration matches training\n# 2. Verify data preprocessing is identical\n# 3. Adjust confidence threshold\n# 4. Check for data distribution shifts\n</code></pre>"},{"location":"api/models/inference/#best-practices","title":"Best Practices","text":""},{"location":"api/models/inference/#inference-configuration","title":"Inference Configuration","text":"<ol> <li>Model Consistency: Ensure inference parameters match training exactly</li> <li>Data Preprocessing: Use identical preprocessing as training</li> <li>Confidence Thresholds: Start with recommended thresholds and adjust based on data</li> <li>Memory Management: Monitor GPU memory usage and optimize batch sizes</li> </ol>"},{"location":"api/models/inference/#quality-assurance","title":"Quality Assurance","text":"<ol> <li>Validation Checks: Verify inference results against known ground truth</li> <li>Confidence Analysis: Analyze score distributions for appropriate thresholds</li> <li>Fragment Detection: Enable fragment detection for comprehensive coverage</li> <li>Post-processing: Apply quality control filters to remove low-quality assignments</li> </ol>"},{"location":"api/models/inference/#performance-optimization_1","title":"Performance Optimization","text":"<ol> <li>GPU Utilization: Maximize GPU usage with appropriate batch sizes</li> <li>Parallel Processing: Use multiple workers for data loading</li> <li>Memory Efficiency: Optimize memory usage with mixed precision</li> <li>Batch Processing: Process large datasets in manageable chunks</li> </ol>"},{"location":"api/models/inference/#future-enhancements","title":"Future Enhancements","text":"<p>Planned inference improvements include:</p> <ul> <li>Real-time Inference: Streaming inference for live data</li> <li>Model Compression: Quantized models for faster inference</li> <li>Distributed Inference: Multi-node inference capabilities</li> <li>Adaptive Thresholding: Dynamic confidence thresholds based on data</li> <li>Uncertainty Quantification: Confidence intervals for predictions</li> </ul>"},{"location":"api/models/segger_model/","title":"segger.models.segger_model","text":"<p>The <code>segger_model</code> module contains the core Graph Neural Network architecture for spatial transcriptomics analysis. This module implements the <code>Segger</code> class, a sophisticated attention-based GNN designed specifically for processing heterogeneous graphs with transcript and boundary nodes.</p>"},{"location":"api/models/segger_model/#core-classes","title":"Core Classes","text":""},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger","title":"Segger","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>class Segger(torch.nn.Module):\n    def __init__(\n        self,\n        num_tx_tokens: int,\n        init_emb: int = 16,\n        hidden_channels: int = 32,\n        num_mid_layers: int = 3,\n        out_channels: int = 32,\n        heads: int = 3,\n    ):\n        \"\"\"\n        Initializes the Segger model.\n\n        Args:\n            num_tx_tokens (int)  : Number of unique 'tx' tokens for embedding.\n            init_emb (int)       : Initial embedding size for both 'tx' and boundary (non-token) nodes.\n            hidden_channels (int): Number of hidden channels.\n            num_mid_layers (int) : Number of hidden layers (excluding first and last layers).\n            out_channels (int)   : Number of output channels.\n            heads (int)          : Number of attention heads.\n        \"\"\"\n        super().__init__()\n\n        # Embedding for 'tx' (transcript) nodes\n        self.tx_embedding = Embedding(num_tx_tokens, init_emb)\n\n        # Linear layer for boundary (non-token) nodes\n        self.lin0 = Linear(-1, init_emb, bias=False)\n\n        # First GATv2Conv layer\n        self.conv_first = GATv2Conv(\n            (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n        )\n        # self.lin_first = Linear(-1, hidden_channels * heads)\n\n        # Middle GATv2Conv layers\n        self.num_mid_layers = num_mid_layers\n        if num_mid_layers &gt; 0:\n            self.conv_mid_layers = torch.nn.ModuleList()\n            # self.lin_mid_layers = torch.nn.ModuleList()\n            for _ in range(num_mid_layers):\n                self.conv_mid_layers.append(\n                    GATv2Conv(\n                        (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n                    )\n                )\n                # self.lin_mid_layers.append(Linear(-1, hidden_channels * heads))\n\n        # Last GATv2Conv layer\n        self.conv_last = GATv2Conv(\n            (-1, -1), out_channels, heads=heads, add_self_loops=False\n        )\n        # self.lin_last = Linear(-1, out_channels * heads)\n\n    def forward(self, x: Tensor, edge_index: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass for the Segger model.\n\n        Args:\n            x (Tensor): Node features.\n            edge_index (Tensor): Edge indices.\n\n        Returns:\n            Tensor: Output node embeddings.\n        \"\"\"\n        x = torch.nan_to_num(x, nan=0)\n        is_one_dim = (x.ndim == 1) * 1\n        x = x[:, None]\n        x = self.tx_embedding(\n            ((x.sum(-1) * is_one_dim).int())\n        ) * is_one_dim + self.lin0(x.float()) * (1 - is_one_dim)\n        x = x.squeeze()\n        # First layer\n        x = x.relu()\n        x = self.conv_first(x, edge_index)  # + self.lin_first(x)\n        x = x.relu()\n\n        # Middle layers\n        if self.num_mid_layers &gt; 0:\n            for conv_mid in self.conv_mid_layers:\n                x = conv_mid(x, edge_index)  # + lin_mid(x)\n                x = x.relu()\n\n        # Last layer\n        x = self.conv_last(x, edge_index)  # + self.lin_last(x)\n\n        # x = F.normalize(x)\n\n        return x\n\n    def decode(self, z: Tensor, edge_index: Union[Tensor]) -&gt; Tensor:\n        \"\"\"\n        Decode the node embeddings to predict edge values.\n\n        Args:\n            z (Tensor): Node embeddings.\n            edge_index (EdgeIndex): Edge label indices.\n\n        Returns:\n            Tensor: Predicted edge values.\n        \"\"\"\n        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.conv_first","title":"conv_first  <code>instance-attribute</code>","text":"<pre><code>conv_first = GATv2Conv((-1, -1), hidden_channels, heads=heads, add_self_loops=False)\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.conv_last","title":"conv_last  <code>instance-attribute</code>","text":"<pre><code>conv_last = GATv2Conv((-1, -1), out_channels, heads=heads, add_self_loops=False)\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.conv_mid_layers","title":"conv_mid_layers  <code>instance-attribute</code>","text":"<pre><code>conv_mid_layers = ModuleList()\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.lin0","title":"lin0  <code>instance-attribute</code>","text":"<pre><code>lin0 = Linear(-1, init_emb, bias=False)\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.num_mid_layers","title":"num_mid_layers  <code>instance-attribute</code>","text":"<pre><code>num_mid_layers = num_mid_layers\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.tx_embedding","title":"tx_embedding  <code>instance-attribute</code>","text":"<pre><code>tx_embedding = Embedding(num_tx_tokens, init_emb)\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.__init__","title":"__init__","text":"<pre><code>__init__(num_tx_tokens, init_emb=16, hidden_channels=32, num_mid_layers=3, out_channels=32, heads=3)\n</code></pre> <p>Initializes the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>num_tx_tokens</code> <code>int)  </code> <p>Number of unique 'tx' tokens for embedding.</p> required <code>init_emb</code> <code>int)       </code> <p>Initial embedding size for both 'tx' and boundary (non-token) nodes.</p> <code>16</code> <code>hidden_channels</code> <code>int</code> <p>Number of hidden channels.</p> <code>32</code> <code>num_mid_layers</code> <code>int) </code> <p>Number of hidden layers (excluding first and last layers).</p> <code>3</code> <code>out_channels</code> <code>int)   </code> <p>Number of output channels.</p> <code>32</code> <code>heads</code> <code>int)          </code> <p>Number of attention heads.</p> <code>3</code> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def __init__(\n    self,\n    num_tx_tokens: int,\n    init_emb: int = 16,\n    hidden_channels: int = 32,\n    num_mid_layers: int = 3,\n    out_channels: int = 32,\n    heads: int = 3,\n):\n    \"\"\"\n    Initializes the Segger model.\n\n    Args:\n        num_tx_tokens (int)  : Number of unique 'tx' tokens for embedding.\n        init_emb (int)       : Initial embedding size for both 'tx' and boundary (non-token) nodes.\n        hidden_channels (int): Number of hidden channels.\n        num_mid_layers (int) : Number of hidden layers (excluding first and last layers).\n        out_channels (int)   : Number of output channels.\n        heads (int)          : Number of attention heads.\n    \"\"\"\n    super().__init__()\n\n    # Embedding for 'tx' (transcript) nodes\n    self.tx_embedding = Embedding(num_tx_tokens, init_emb)\n\n    # Linear layer for boundary (non-token) nodes\n    self.lin0 = Linear(-1, init_emb, bias=False)\n\n    # First GATv2Conv layer\n    self.conv_first = GATv2Conv(\n        (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n    )\n    # self.lin_first = Linear(-1, hidden_channels * heads)\n\n    # Middle GATv2Conv layers\n    self.num_mid_layers = num_mid_layers\n    if num_mid_layers &gt; 0:\n        self.conv_mid_layers = torch.nn.ModuleList()\n        # self.lin_mid_layers = torch.nn.ModuleList()\n        for _ in range(num_mid_layers):\n            self.conv_mid_layers.append(\n                GATv2Conv(\n                    (-1, -1), hidden_channels, heads=heads, add_self_loops=False\n                )\n            )\n            # self.lin_mid_layers.append(Linear(-1, hidden_channels * heads))\n\n    # Last GATv2Conv layer\n    self.conv_last = GATv2Conv(\n        (-1, -1), out_channels, heads=heads, add_self_loops=False\n    )\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.decode","title":"decode","text":"<pre><code>decode(z, edge_index)\n</code></pre> <p>Decode the node embeddings to predict edge values.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Node embeddings.</p> required <code>edge_index</code> <code>EdgeIndex</code> <p>Edge label indices.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Predicted edge values.</p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def decode(self, z: Tensor, edge_index: Union[Tensor]) -&gt; Tensor:\n    \"\"\"\n    Decode the node embeddings to predict edge values.\n\n    Args:\n        z (Tensor): Node embeddings.\n        edge_index (EdgeIndex): Edge label indices.\n\n    Returns:\n        Tensor: Predicted edge values.\n    \"\"\"\n    return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n</code></pre>"},{"location":"api/models/segger_model/#src.segger.models.segger_model.Segger.forward","title":"forward","text":"<pre><code>forward(x, edge_index)\n</code></pre> <p>Forward pass for the Segger model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Node features.</p> required <code>edge_index</code> <code>Tensor</code> <p>Edge indices.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Output node embeddings.</p> Source code in <code>src/segger/models/segger_model.py</code> <pre><code>def forward(self, x: Tensor, edge_index: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass for the Segger model.\n\n    Args:\n        x (Tensor): Node features.\n        edge_index (Tensor): Edge indices.\n\n    Returns:\n        Tensor: Output node embeddings.\n    \"\"\"\n    x = torch.nan_to_num(x, nan=0)\n    is_one_dim = (x.ndim == 1) * 1\n    x = x[:, None]\n    x = self.tx_embedding(\n        ((x.sum(-1) * is_one_dim).int())\n    ) * is_one_dim + self.lin0(x.float()) * (1 - is_one_dim)\n    x = x.squeeze()\n    # First layer\n    x = x.relu()\n    x = self.conv_first(x, edge_index)  # + self.lin_first(x)\n    x = x.relu()\n\n    # Middle layers\n    if self.num_mid_layers &gt; 0:\n        for conv_mid in self.conv_mid_layers:\n            x = conv_mid(x, edge_index)  # + lin_mid(x)\n            x = x.relu()\n\n    # Last layer\n    x = self.conv_last(x, edge_index)  # + self.lin_last(x)\n\n    # x = F.normalize(x)\n\n    return x\n</code></pre>"},{"location":"api/models/segger_model/#overview","title":"Overview","text":"<p>The <code>Segger</code> class implements a Graph Neural Network architecture specifically designed for spatial transcriptomics data. It uses Graph Attention Networks (GAT) with GATv2Conv layers to learn complex spatial relationships between transcripts and cellular boundaries.</p>"},{"location":"api/models/segger_model/#key-features","title":"Key Features","text":"<ul> <li>Heterogeneous Graph Processing: Automatically handles different node types (transcripts vs. boundaries)</li> <li>Attention Mechanisms: GATv2Conv layers for learning spatial relationships</li> <li>Configurable Architecture: Adjustable depth, width, and attention heads</li> <li>PyTorch Integration: Native PyTorch module with full compatibility</li> <li>Spatial Optimization: Designed specifically for spatial transcriptomics data</li> </ul>"},{"location":"api/models/segger_model/#architecture-details","title":"Architecture Details","text":""},{"location":"api/models/segger_model/#node-type-processing","title":"Node Type Processing","text":"<p>The model automatically differentiates between node types based on input feature dimensions:</p> <ol> <li>Transcript Nodes (1D features): Processed through embedding layers</li> <li>Boundary Nodes (Multi-dimensional features): Processed through linear transformations</li> </ol>"},{"location":"api/models/segger_model/#layer-structure","title":"Layer Structure","text":"<pre><code>Input Features \u2192 Node Type Detection \u2192 Feature Processing \u2192 GATv2Conv Layers \u2192 Output Embeddings\n     \u2193              \u2193                    \u2193                \u2193                \u2193\nTranscripts    Auto-routing         Embedding/Linear   Attention Mech.   Learned Features\nBoundaries     (1D vs Multi-D)     Transformations    Spatial Learning   Biological Insights\n</code></pre>"},{"location":"api/models/segger_model/#attention-mechanism","title":"Attention Mechanism","text":"<p>The model uses Graph Attention Networks (GAT) with the following attention computation:</p> <pre><code>\u03b1_ij = softmax(LeakyReLU(a^T [Wh_i || Wh_j]))\n</code></pre> <p>Where: - <code>\u03b1_ij</code> is the attention coefficient between nodes i and j - <code>a</code> is a learnable attention vector - <code>W</code> is a learnable weight matrix - <code>h_i, h_j</code> are node features</p>"},{"location":"api/models/segger_model/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/segger_model/#basic-model-initialization","title":"Basic Model Initialization","text":"<pre><code>from segger.models.segger_model import Segger\n\n# Initialize with default parameters\nmodel = Segger(\n    num_tx_tokens=5000,      # Number of unique transcript types\n    init_emb=16,             # Initial embedding dimension\n    hidden_channels=32,       # Hidden layer size\n    num_mid_layers=3,        # Number of hidden layers\n    out_channels=32,          # Output dimension\n    heads=3                   # Number of attention heads\n)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n</code></pre>"},{"location":"api/models/segger_model/#forward-pass","title":"Forward Pass","text":"<pre><code>import torch\n\n# Create sample data\nbatch_size = 100\nnum_nodes = 1000\nx = torch.randn(num_nodes, 64)  # Node features\nedge_index = torch.randint(0, num_nodes, (2, 2000))  # Edge indices\n\n# Forward pass\nwith torch.no_grad():\n    output = model(x, edge_index)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Edge index shape: {edge_index.shape}\")\n</code></pre>"},{"location":"api/models/segger_model/#training-configuration","title":"Training Configuration","text":"<pre><code>import torch.nn as nn\nimport torch.optim as optim\n\n# Model configuration for large dataset\nmodel = Segger(\n    num_tx_tokens=10000,     # Large vocabulary\n    init_emb=64,             # Larger embeddings\n    hidden_channels=128,      # Wider layers\n    num_mid_layers=5,        # Deeper architecture\n    out_channels=256,         # Rich output features\n    heads=8                   # More attention heads\n)\n\n# Optimizer and loss\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nmodel.train()\nfor epoch in range(100):\n    optimizer.zero_grad()\n\n    # Forward pass\n    out = model(x, edge_index)\n\n    # Compute loss (example: node classification)\n    loss = criterion(out, labels)\n\n    # Backward pass\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"api/models/segger_model/#model-parameters","title":"Model Parameters","text":""},{"location":"api/models/segger_model/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>num_tx_tokens</code> (int): Number of unique transcript types in your dataset</li> <li>This determines the size of the transcript embedding layer</li> <li>Should match the number of unique genes/transcripts in your data</li> </ul>"},{"location":"api/models/segger_model/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>init_emb</code> (int, default=16): Initial embedding dimension</li> <li>Used for both transcript embeddings and boundary feature transformation</li> <li> <p>Larger values provide more expressive features but increase memory usage</p> </li> <li> <p><code>hidden_channels</code> (int, default=32): Number of hidden channels</p> </li> <li>Size of intermediate layer representations</li> <li> <p>Affects model capacity and computational cost</p> </li> <li> <p><code>num_mid_layers</code> (int, default=3): Number of hidden GAT layers</p> </li> <li>More layers enable learning of more complex patterns</li> <li> <p>Balance between expressiveness and overfitting</p> </li> <li> <p><code>out_channels</code> (int, default=32): Output embedding dimension</p> </li> <li>Size of final node representations</li> <li> <p>Should match your downstream task requirements</p> </li> <li> <p><code>heads</code> (int, default=3): Number of attention heads</p> </li> <li>Multiple heads learn different types of relationships</li> <li>More heads generally improve performance but increase computation</li> </ul>"},{"location":"api/models/segger_model/#architecture-components","title":"Architecture Components","text":""},{"location":"api/models/segger_model/#1-input-processing-layer","title":"1. Input Processing Layer","text":"<pre><code># Automatic node type detection and processing\nif x.ndim == 1:  # Transcript nodes\n    x = self.tx_embedding(x.int())\nelse:  # Boundary nodes\n    x = self.lin0(x.float())\n</code></pre>"},{"location":"api/models/segger_model/#2-graph-attention-layers","title":"2. Graph Attention Layers","text":"<pre><code># First attention layer\nx = F.relu(x)\nx = self.conv_first(x, edge_index)\nx = F.relu(x)\n\n# Middle attention layers\nfor conv_mid in self.conv_mid_layers:\n    x = conv_mid(x, edge_index)\n    x = F.relu(x)\n\n# Final attention layer\nx = self.conv_last(x, edge_index)\n</code></pre>"},{"location":"api/models/segger_model/#3-output-processing","title":"3. Output Processing","text":"<pre><code># Final embeddings can be used for various tasks\n# - Node classification\n# - Link prediction\n# - Graph-level tasks\n# - Downstream analysis\n</code></pre>"},{"location":"api/models/segger_model/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"api/models/segger_model/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Time Complexity: O(|E| \u00d7 F \u00d7 H) per layer</li> <li>|E|: Number of edges</li> <li>F: Feature dimension</li> <li> <p>H: Number of attention heads</p> </li> <li> <p>Memory Usage: Scales with graph size and model parameters</p> </li> <li>Node features: O(|V| \u00d7 F)</li> <li>Edge attention: O(|E| \u00d7 H)</li> <li>Model parameters: O(F\u00b2 \u00d7 L \u00d7 H)</li> </ul>"},{"location":"api/models/segger_model/#optimization-features","title":"Optimization Features","text":"<ul> <li>Efficient Attention: GATv2Conv optimized for sparse graphs</li> <li>Memory Management: Automatic handling of different node types</li> <li>PyTorch Optimization: Leverages PyTorch's optimized operations</li> <li>GPU Acceleration: Full CUDA support for training and inference</li> </ul>"},{"location":"api/models/segger_model/#integration-with-pytorch-geometric","title":"Integration with PyTorch Geometric","text":"<p>The model is designed to work seamlessly with PyTorch Geometric:</p> <pre><code>from torch_geometric.data import Data\nfrom torch_geometric.transforms import ToUndirected\n\n# Create PyG data object\ndata = Data(x=x, edge_index=edge_index)\n\n# Apply transformations\ndata = ToUndirected()(data)\n\n# Process with model\noutput = model(data.x, data.edge_index)\n</code></pre>"},{"location":"api/models/segger_model/#training-strategies","title":"Training Strategies","text":""},{"location":"api/models/segger_model/#1-learning-rate-scheduling","title":"1. Learning Rate Scheduling","text":"<pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR\n\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\n# Use in training loop\nscheduler.step()\n</code></pre>"},{"location":"api/models/segger_model/#2-regularization","title":"2. Regularization","text":"<pre><code># Weight decay in optimizer\noptimizer = optim.AdamW(model.parameters(), weight_decay=1e-5)\n\n# Dropout (can be added to model if needed)\ndropout = nn.Dropout(0.1)\nx = dropout(x)\n</code></pre>"},{"location":"api/models/segger_model/#3-early-stopping","title":"3. Early Stopping","text":"<pre><code># Monitor validation loss\nbest_val_loss = float('inf')\npatience = 10\npatience_counter = 0\n\nfor epoch in range(max_epochs):\n    # Training...\n    val_loss = validate(model, val_loader)\n\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        patience_counter += 1\n\n    if patience_counter &gt;= patience:\n        print(\"Early stopping triggered\")\n        break\n</code></pre>"},{"location":"api/models/segger_model/#best-practices","title":"Best Practices","text":""},{"location":"api/models/segger_model/#model-architecture-selection","title":"Model Architecture Selection","text":"<ul> <li>Small Datasets (&lt; 10k nodes): Use fewer layers and smaller dimensions</li> <li>Medium Datasets (10k-100k nodes): Balanced architecture with moderate complexity</li> <li>Large Datasets (&gt; 100k nodes): Deeper models with more attention heads</li> </ul>"},{"location":"api/models/segger_model/#training-configuration_1","title":"Training Configuration","text":"<ul> <li>Learning Rate: Start with 0.001 and adjust based on convergence</li> <li>Batch Size: Use largest size that fits in memory</li> <li>Regularization: Apply weight decay and consider dropout</li> <li>Monitoring: Track both training and validation metrics</li> </ul>"},{"location":"api/models/segger_model/#data-preparation","title":"Data Preparation","text":"<ul> <li>Feature Normalization: Normalize input features for stable training</li> <li>Graph Construction: Ensure proper edge construction for spatial relationships</li> <li>Validation Strategy: Use spatial-aware validation splits</li> <li>Data Augmentation: Consider spatial augmentations for robustness</li> </ul>"},{"location":"api/models/segger_model/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api/models/segger_model/#1-cell-type-classification","title":"1. Cell Type Classification","text":"<pre><code># Train model for cell type prediction\nmodel = Segger(num_tx_tokens=5000, out_channels=num_cell_types)\n# ... training ...\npredictions = model(x, edge_index)\ncell_types = torch.argmax(predictions, dim=1)\n</code></pre>"},{"location":"api/models/segger_model/#2-spatial-relationship-learning","title":"2. Spatial Relationship Learning","text":"<pre><code># Learn spatial relationships between transcripts and boundaries\nembeddings = model(x, edge_index)\n# Use embeddings for downstream analysis\nsimilarity = torch.mm(embeddings, embeddings.t())\n</code></pre>"},{"location":"api/models/segger_model/#3-tissue-architecture-analysis","title":"3. Tissue Architecture Analysis","text":"<pre><code># Analyze tissue-level patterns\nmodel = Segger(num_tx_tokens=5000, out_channels=128)\nembeddings = model(x, edge_index)\n# Apply clustering or other analysis to embeddings\n</code></pre>"},{"location":"api/models/segger_model/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/models/segger_model/#common-issues","title":"Common Issues","text":"<ol> <li>Memory Errors: Reduce model size or batch size</li> <li>Training Instability: Lower learning rate or add regularization</li> <li>Poor Performance: Check data quality and feature engineering</li> <li>Slow Convergence: Adjust learning rate or model architecture</li> </ol>"},{"location":"api/models/segger_model/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use appropriate model size for your dataset</li> <li>Monitor training metrics to detect issues early</li> <li>Validate on held-out data to prevent overfitting</li> <li>Use mixed precision training for faster training on modern GPUs</li> </ol>"},{"location":"api/models/segger_model/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements include:</p> <ul> <li>Additional Attention Types: Support for different attention mechanisms</li> <li>Multi-modal Integration: Support for additional data types</li> <li>Distributed Training: Multi-GPU and multi-node support</li> <li>Model Compression: Efficient deployment of trained models</li> <li>Interpretability Tools: Understanding learned spatial relationships</li> </ul>"},{"location":"api/models/segger_model/#dependencies","title":"Dependencies","text":"<ul> <li>PyTorch: Core neural network functionality</li> <li>PyTorch Geometric: Graph neural network operations</li> <li>NumPy: Numerical operations (optional, for data preprocessing)</li> </ul>"},{"location":"api/models/segger_model/#contributing","title":"Contributing","text":"<p>Contributions to improve the Segger model are welcome:</p> <ul> <li>Architecture Improvements: Better attention mechanisms and layer designs</li> <li>Performance Optimization: Faster training and inference</li> <li>Feature Extensions: Support for additional node and edge types</li> <li>Testing: Comprehensive test coverage and validation</li> </ul>"},{"location":"api/models/training/","title":"Segger Model Training","text":""},{"location":"api/models/training/#overview","title":"Overview","text":"<p>Training the Segger model involves optimizing a Graph Neural Network for transcript-to-cell link prediction in spatial transcriptomics data. The training process leverages PyTorch Lightning for scalable multi-GPU training, with specialized data handling and validation strategies designed for heterogeneous graphs.</p>"},{"location":"api/models/training/#training-framework","title":"Training Framework","text":""},{"location":"api/models/training/#pytorch-lightning-integration","title":"PyTorch Lightning Integration","text":"<p>Segger uses PyTorch Lightning for training orchestration, providing:</p> <ul> <li>Multi-GPU Training: Automatic data parallel training across multiple devices</li> <li>Mixed Precision: Support for 16-bit mixed precision training</li> <li>Distributed Training: Multi-node training capabilities</li> <li>Automatic Logging: Built-in metrics tracking and visualization</li> <li>Checkpoint Management: Automatic model saving and restoration</li> </ul>"},{"location":"api/models/training/#training-architecture","title":"Training Architecture","text":"<pre><code>Training Tiles \u2192 Data Loaders \u2192 Segger Model \u2192 Link Prediction \u2192 Loss Computation \u2192 Optimization\n     \u2193              \u2193              \u2193              \u2193              \u2193              \u2193\nSpatial Graphs   Mini-batches   GNN Forward    Similarity      Binary CE      Adam Optimizer\nValidation Set   GPU Transfer   Attention      Scores          Loss + AUROC   Weight Updates\n</code></pre>"},{"location":"api/models/training/#data-preparation","title":"Data Preparation","text":""},{"location":"api/models/training/#training-data-structure","title":"Training Data Structure","text":"<p>Training data consists of spatial tiles represented as PyTorch Geometric graphs:</p> <pre><code># Each tile contains:\ndata = {\n    \"tx\": {  # Transcript nodes\n        \"id\": transcript_ids,\n        \"pos\": spatial_coordinates,\n        \"x\": feature_vectors\n    },\n    \"bd\": {  # Boundary nodes\n        \"id\": boundary_ids,\n        \"pos\": centroid_coordinates,\n        \"x\": geometric_features\n    },\n    \"tx,neighbors,tx\": {  # Transcript proximity edges\n        \"edge_index\": neighbor_connections\n    },\n    \"tx,belongs,bd\": {  # Transcript-boundary edges\n        \"edge_index\": containment_relationships,\n        \"edge_label\": positive/negative labels\n    }\n}\n</code></pre>"},{"location":"api/models/training/#data-splitting-strategy","title":"Data Splitting Strategy","text":"<p>Tiles are randomly assigned to training, validation, and test sets:</p> <pre><code># Recommended split ratios\ntrain_ratio = 0.7    # 70% for training\nval_ratio = 0.2      # 20% for validation  \ntest_ratio = 0.1     # 10% for testing\n\n# Spatial-aware splitting ensures:\n# - No information leakage between splits\n# - Representative spatial coverage in each split\n# - Balanced distribution of cell types\n</code></pre>"},{"location":"api/models/training/#negative-edge-sampling","title":"Negative Edge Sampling","text":"<p>To handle class imbalance, negative edges are sampled during training:</p> <pre><code># Sample negative edges at 1:5 ratio (positive:negative)\nneg_sampling_ratio = 5\n\n# Negative edges represent:\n# - Transcripts assigned to wrong cells\n# - Random transcript-cell pairs\n# - Spatially distant but transcriptionally similar pairs\n</code></pre>"},{"location":"api/models/training/#training-configuration","title":"Training Configuration","text":""},{"location":"api/models/training/#model-parameters","title":"Model Parameters","text":"<p>Key training parameters based on the Segger paper:</p> <pre><code># Architecture configuration\nmodel_config = {\n    'num_tx_tokens': 5000,      # Vocabulary size (adjust for dataset)\n    'init_emb': 16,             # Initial embedding dimension\n    'hidden_channels': 64,       # Hidden layer size\n    'num_mid_layers': 3,        # Number of GAT layers\n    'out_channels': 32,          # Output dimension\n    'heads': 4                   # Number of attention heads\n}\n\n# Training configuration\ntraining_config = {\n    'learning_rate': 0.001,     # Initial learning rate\n    'batch_size': 2,            # Batch size per GPU\n    'max_epochs': 200,          # Maximum training epochs\n    'weight_decay': 1e-5,       # L2 regularization\n    'patience': 10,             # Early stopping patience\n}\n</code></pre>"},{"location":"api/models/training/#hardware-configuration","title":"Hardware Configuration","text":"<pre><code># GPU configuration\ngpu_config = {\n    'accelerator': 'cuda',       # Use CUDA acceleration\n    'devices': 4,               # Number of GPUs\n    'strategy': 'ddp',          # Distributed data parallel\n    'precision': '16-mixed'     # Mixed precision training\n}\n\n# Memory optimization\nmemory_config = {\n    'gradient_clip_val': 1.0,   # Gradient clipping\n    'accumulate_grad_batches': 1, # Gradient accumulation\n    'num_workers': 4            # Data loading workers\n}\n</code></pre>"},{"location":"api/models/training/#training-process","title":"Training Process","text":""},{"location":"api/models/training/#training-loop","title":"Training Loop","text":"<p>The training process follows this sequence:</p> <pre><code># Training loop (PyTorch Lightning handles this automatically)\nfor epoch in range(max_epochs):\n    # Training phase\n    for batch in train_loader:\n        # Forward pass\n        embeddings = model(batch.x, batch.edge_index)\n\n        # Link prediction\n        scores = model.decode(embeddings, batch.edge_label_index)\n\n        # Loss computation\n        loss = criterion(scores, batch.edge_label)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # Validation phase\n    for batch in val_loader:\n        with torch.no_grad():\n            embeddings = model(batch.x, batch.edge_index)\n            scores = model.decode(embeddings, batch.edge_label_index)\n            val_loss = criterion(scores, batch.edge_label)\n\n            # Compute metrics\n            auroc = compute_auroc(scores, batch.edge_label)\n            f1_score = compute_f1(scores, batch.edge_label)\n</code></pre>"},{"location":"api/models/training/#loss-function","title":"Loss Function","text":"<p>The model uses binary cross-entropy loss for link prediction:</p> <pre><code># Binary cross-entropy loss\ncriterion = nn.BCEWithLogitsLoss()\n\n# Loss computation\nloss = -\u03a3_(t_i,c_j) [y_ij log(\u03c3(s_ij)) + (1-y_ij) log(1-\u03c3(s_ij))]\n\n# Where:\n# y_ij: Ground truth label (1 for positive, 0 for negative)\n# s_ij: Raw similarity score from model\n# \u03c3(s_ij): Sigmoid activation for probability\n</code></pre>"},{"location":"api/models/training/#optimization","title":"Optimization","text":"<p>Training uses the Adam optimizer with learning rate scheduling:</p> <pre><code># Optimizer configuration\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    weight_decay=weight_decay,\n    betas=(0.9, 0.999)\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, \n    T_max=max_epochs,\n    eta_min=1e-6\n)\n</code></pre>"},{"location":"api/models/training/#validation-and-monitoring","title":"Validation and Monitoring","text":""},{"location":"api/models/training/#validation-metrics","title":"Validation Metrics","text":"<p>The model is evaluated using:</p>"},{"location":"api/models/training/#auroc-area-under-roc-curve","title":"AUROC (Area Under ROC Curve)","text":"<pre><code>def compute_auroc(scores, labels):\n    \"\"\"Compute Area Under ROC Curve for link prediction.\"\"\"\n    fpr, tpr, _ = roc_curve(labels, scores)\n    return auc(fpr, tpr)\n</code></pre>"},{"location":"api/models/training/#f1-score","title":"F1 Score","text":"<pre><code>def compute_f1(scores, labels):\n    \"\"\"Compute F1 score for link prediction.\"\"\"\n    predictions = (scores &gt; 0.5).float()\n    return f1_score(labels, predictions)\n</code></pre>"},{"location":"api/models/training/#training-monitoring","title":"Training Monitoring","text":"<p>PyTorch Lightning provides automatic logging:</p> <pre><code># Metrics logged automatically\nself.log('train_loss', train_loss, on_step=True, on_epoch=True)\nself.log('val_loss', val_loss, on_epoch=True)\nself.log('val_auroc', val_auroc, on_epoch=True)\nself.log('val_f1', val_f1, on_epoch=True)\n\n# Learning rate logging\nself.log('lr', self.optimizer.param_groups[0]['lr'], on_epoch=True)\n</code></pre>"},{"location":"api/models/training/#early-stopping","title":"Early Stopping","text":"<p>Training stops automatically when validation performance plateaus:</p> <pre><code># Early stopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_auroc',\n    mode='max',\n    patience=patience,\n    verbose=True\n)\n\n# Model checkpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_auroc',\n    mode='max',\n    save_top_k=3,\n    filename='segger-{epoch:02d}-{val_auroc:.3f}'\n)\n</code></pre>"},{"location":"api/models/training/#multi-gpu-training","title":"Multi-GPU Training","text":""},{"location":"api/models/training/#data-parallel-strategy","title":"Data Parallel Strategy","text":"<p>Segger supports distributed training across multiple GPUs:</p> <pre><code># Distributed training configuration\ntrainer = pl.Trainer(\n    accelerator='cuda',\n    devices=4,                    # Use 4 GPUs\n    strategy='ddp',               # Distributed data parallel\n    precision='16-mixed',         # Mixed precision\n    max_epochs=max_epochs,\n    callbacks=[early_stopping, checkpoint_callback]\n)\n</code></pre>"},{"location":"api/models/training/#batch-size-scaling","title":"Batch Size Scaling","text":"<pre><code># Effective batch size = batch_size \u00d7 num_gpus\neffective_batch_size = batch_size * num_gpus\n\n# Example: batch_size=2, num_gpus=4\n# Effective batch size = 8\n\n# Adjust learning rate for larger effective batch size\nscaled_lr = base_lr * (effective_batch_size / 32)  # Linear scaling rule\n</code></pre>"},{"location":"api/models/training/#memory-management","title":"Memory Management","text":"<pre><code># Memory optimization techniques\nmemory_config = {\n    'gradient_checkpointing': True,    # Trade compute for memory\n    'find_unused_parameters': False,   # Optimize for DDP\n    'sync_batchnorm': False,           # Not needed for GNNs\n    'deterministic': False             # Allow non-deterministic operations\n}\n</code></pre>"},{"location":"api/models/training/#training-strategies","title":"Training Strategies","text":""},{"location":"api/models/training/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":""},{"location":"api/models/training/#cosine-annealing","title":"Cosine Annealing","text":"<pre><code>scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, \n    T_max=max_epochs,\n    eta_min=1e-6\n)\n</code></pre>"},{"location":"api/models/training/#warmup-cosine","title":"Warmup + Cosine","text":"<pre><code># Warmup for first 10% of training\nwarmup_epochs = int(0.1 * max_epochs)\n\ndef get_lr_multiplier(epoch):\n    if epoch &lt; warmup_epochs:\n        return epoch / warmup_epochs\n    else:\n        # Cosine decay\n        progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)\n        return 0.5 * (1 + math.cos(math.pi * progress))\n</code></pre>"},{"location":"api/models/training/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"api/models/training/#weight-decay","title":"Weight Decay","text":"<pre><code># L2 regularization in optimizer\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    weight_decay=1e-5\n)\n</code></pre>"},{"location":"api/models/training/#dropout-optional","title":"Dropout (Optional)","text":"<pre><code># Add dropout to attention layers if needed\nclass SeggerWithDropout(Segger):\n    def __init__(self, *args, dropout=0.1, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, edge_index):\n        # Apply dropout after attention layers\n        x = super().forward(x, edge_index)\n        x = self.dropout(x)\n        return x\n</code></pre>"},{"location":"api/models/training/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/models/training/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code># Enable mixed precision for faster training\ntrainer = pl.Trainer(\n    precision='16-mixed',  # 16-bit mixed precision\n    # Automatic mixed precision provides:\n    # - Faster training (1.5-2x speedup)\n    # - Lower memory usage\n    # - Maintained numerical stability\n)\n</code></pre>"},{"location":"api/models/training/#gradient-accumulation","title":"Gradient Accumulation","text":"<pre><code># Accumulate gradients over multiple batches\ntrainer = pl.Trainer(\n    accumulate_grad_batches=4,  # Effective batch size = batch_size \u00d7 4\n    # Useful when:\n    # - GPU memory is limited\n    # - Large effective batch size is desired\n    # - Training stability is important\n)\n</code></pre>"},{"location":"api/models/training/#data-loading-optimization","title":"Data Loading Optimization","text":"<pre><code># Optimize data loading\ndataloader_config = {\n    'num_workers': 4,           # Parallel data loading\n    'pin_memory': True,         # Faster GPU transfer\n    'persistent_workers': True,  # Keep workers alive between epochs\n    'prefetch_factor': 2        # Prefetch batches\n}\n</code></pre>"},{"location":"api/models/training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/models/training/#common-training-issues","title":"Common Training Issues","text":""},{"location":"api/models/training/#training-instability","title":"Training Instability","text":"<pre><code># Solutions:\n# 1. Reduce learning rate\nlearning_rate = 0.0001  # Reduce from 0.001\n\n# 2. Add gradient clipping\ntrainer = pl.Trainer(gradient_clip_val=1.0)\n\n# 3. Check data quality and normalization\n</code></pre>"},{"location":"api/models/training/#memory-errors","title":"Memory Errors","text":"<pre><code># Solutions:\n# 1. Reduce batch size\nbatch_size = 1  # Reduce from 2\n\n# 2. Enable gradient checkpointing\ntrainer = pl.Trainer(enable_checkpointing=True)\n\n# 3. Use mixed precision\ntrainer = pl.Trainer(precision='16-mixed')\n</code></pre>"},{"location":"api/models/training/#poor-convergence","title":"Poor Convergence","text":"<pre><code># Solutions:\n# 1. Check learning rate schedule\n# 2. Verify data preprocessing\n# 3. Adjust model architecture\n# 4. Check for data leakage\n</code></pre>"},{"location":"api/models/training/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Monitor training progress\nclass TrainingMonitor(pl.Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # Log training metrics\n        train_loss = trainer.callback_metrics['train_loss']\n        print(f\"Epoch {trainer.current_epoch}: Train Loss = {train_loss:.4f}\")\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        # Log validation metrics\n        val_auroc = trainer.callback_metrics['val_auroc']\n        val_f1 = trainer.callback_metrics['val_f1']\n        print(f\"Validation: AUROC = {val_auroc:.4f}, F1 = {val_f1:.4f}\")\n</code></pre>"},{"location":"api/models/training/#best-practices","title":"Best Practices","text":""},{"location":"api/models/training/#training-configuration_1","title":"Training Configuration","text":"<ol> <li>Start with Default Parameters: Use recommended settings from the Segger paper</li> <li>Monitor Validation Metrics: Focus on AUROC and F1 score, not just loss</li> <li>Use Early Stopping: Prevent overfitting with patience-based stopping</li> <li>Enable Mixed Precision: Use 16-bit training for speed and memory efficiency</li> </ol>"},{"location":"api/models/training/#data-preparation_1","title":"Data Preparation","text":"<ol> <li>Quality Control: Filter low-quality transcripts and boundaries</li> <li>Spatial Validation: Ensure train/val/test splits are spatially representative</li> <li>Feature Normalization: Normalize transcript and boundary features</li> <li>Negative Sampling: Use appropriate negative sampling ratios</li> </ol>"},{"location":"api/models/training/#hardware-utilization","title":"Hardware Utilization","text":"<ol> <li>Multi-GPU Training: Scale training across multiple GPUs</li> <li>Memory Optimization: Use mixed precision and gradient checkpointing</li> <li>Data Loading: Optimize data loading with multiple workers</li> <li>Batch Size: Use largest batch size that fits in memory</li> </ol>"},{"location":"api/models/training/#future-enhancements","title":"Future Enhancements","text":"<p>Planned training improvements include:</p> <ul> <li>Advanced Scheduling: More sophisticated learning rate schedules</li> <li>Automated Hyperparameter Tuning: Integration with Optuna or similar tools</li> <li>Curriculum Learning: Progressive difficulty training strategies</li> <li>Multi-task Training: Joint training on multiple objectives</li> <li>Federated Learning: Distributed training across multiple institutions</li> </ul>"},{"location":"api/prediction/","title":"segger.prediction","text":"<p>prediction module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/training/","title":"segger.training","text":"<p>training module for Segger.</p> <p>Contains the implementation of the Segger model using Graph Neural Networks.</p>"},{"location":"api/validation/","title":"segger.validation","text":"<p>This module handles validation utilities for the Segger tool.</p>"},{"location":"api/validation/#submodules","title":"Submodules","text":"<ul> <li>Utils</li> <li>Xenium Explorer</li> </ul>"},{"location":"api/validation/#api-documentation","title":"API Documentation","text":""},{"location":"notebooks/benchmark_bc/","title":"Benchmark bc","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport scanpy as sc\nimport numpy as np\nfrom typing import Dict\nfrom segger.validation.utils import *\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from pathlib import Path import scanpy as sc import numpy as np from typing import Dict from segger.validation.utils import * In\u00a0[\u00a0]: Copied! <pre># Define paths and output directories\nbenchmarks_path = Path(\n    \"/dkfz/cluster/gpu/data/OE0606/elihei/segger_experiments/data_tidy/benchmarks/xe_rep1_bc\"\n)\noutput_path = benchmarks_path / \"results+\"\nfigures_path = output_path / \"figures\"\nfigures_path.mkdir(parents=True, exist_ok=True)  # Ensure the figures directory exists\n</pre> # Define paths and output directories benchmarks_path = Path(     \"/dkfz/cluster/gpu/data/OE0606/elihei/segger_experiments/data_tidy/benchmarks/xe_rep1_bc\" ) output_path = benchmarks_path / \"results+\" figures_path = output_path / \"figures\" figures_path.mkdir(parents=True, exist_ok=True)  # Ensure the figures directory exists In\u00a0[\u00a0]: Copied! <pre># Define colors for segmentation methods\nmethod_colors = {\n    \"segger\": \"#D55E00\",\n    \"segger_n0\": \"#E69F00\",\n    \"segger_n1\": \"#F0E442\",\n    \"segger_embedding\": \"#C72228\",\n    \"Baysor\": \"#000075\",\n    \"Baysor_n0\": \"#0F4A9C\",\n    \"Baysor_n1\": \"#0072B2\",\n    \"10X\": \"#8B008B\",\n    \"10X-nucleus\": \"#CC79A7\",\n    # 'BIDCell': '#009E73'\n}\n</pre> # Define colors for segmentation methods method_colors = {     \"segger\": \"#D55E00\",     \"segger_n0\": \"#E69F00\",     \"segger_n1\": \"#F0E442\",     \"segger_embedding\": \"#C72228\",     \"Baysor\": \"#000075\",     \"Baysor_n0\": \"#0F4A9C\",     \"Baysor_n1\": \"#0072B2\",     \"10X\": \"#8B008B\",     \"10X-nucleus\": \"#CC79A7\",     # 'BIDCell': '#009E73' } In\u00a0[\u00a0]: Copied! <pre># Define colors for cell types\nmajor_colors = {\n    \"B-cells\": \"#d8f55e\",\n    \"CAFs\": \"#532C8A\",\n    \"Cancer Epithelial\": \"#C72228\",\n    \"Endothelial\": \"#9e6762\",\n    \"Myeloid\": \"#ffe012\",\n    \"T-cells\": \"#3cb44b\",\n    \"Normal Epithelial\": \"#0F4A9C\",\n    \"PVL\": \"#c09d9a\",\n    \"Plasmablasts\": \"#000075\",\n}\n</pre> # Define colors for cell types major_colors = {     \"B-cells\": \"#d8f55e\",     \"CAFs\": \"#532C8A\",     \"Cancer Epithelial\": \"#C72228\",     \"Endothelial\": \"#9e6762\",     \"Myeloid\": \"#ffe012\",     \"T-cells\": \"#3cb44b\",     \"Normal Epithelial\": \"#0F4A9C\",     \"PVL\": \"#c09d9a\",     \"Plasmablasts\": \"#000075\", } In\u00a0[\u00a0]: Copied! <pre># Define segmentation file paths\nsegmentation_paths = {\n    \"segger\": benchmarks_path / \"adata_segger.h5ad\",\n    \"Baysor\": benchmarks_path / \"adata_baysor.h5ad\",\n    \"10X\": benchmarks_path / \"adata_10X.h5ad\",\n    \"10X-nucleus\": benchmarks_path / \"adata_10X_nuc.h5ad\",\n    \"BIDCell\": benchmarks_path / \"adata_BIDCell.h5ad\",\n}\n</pre> # Define segmentation file paths segmentation_paths = {     \"segger\": benchmarks_path / \"adata_segger.h5ad\",     \"Baysor\": benchmarks_path / \"adata_baysor.h5ad\",     \"10X\": benchmarks_path / \"adata_10X.h5ad\",     \"10X-nucleus\": benchmarks_path / \"adata_10X_nuc.h5ad\",     \"BIDCell\": benchmarks_path / \"adata_BIDCell.h5ad\", } In\u00a0[\u00a0]: Copied! <pre># Load the segmentations and the scRNAseq data\nsegmentations_dict = load_segmentations(segmentation_paths)\nsegmentations_dict = {\n    k: segmentations_dict[k] for k in method_colors.keys() if k in segmentations_dict\n}\nscRNAseq_adata = sc.read(benchmarks_path / \"scRNAseq.h5ad\")\n</pre> # Load the segmentations and the scRNAseq data segmentations_dict = load_segmentations(segmentation_paths) segmentations_dict = {     k: segmentations_dict[k] for k in method_colors.keys() if k in segmentations_dict } scRNAseq_adata = sc.read(benchmarks_path / \"scRNAseq.h5ad\") <p>Generate general statistics plots plot_general_statistics_plots(segmentations_dict, figures_path, method_colors)</p> In\u00a0[\u00a0]: Copied! <pre>plot_cell_counts(segmentations_dict, figures_path, palette=method_colors)\nplot_cell_area(segmentations_dict, figures_path, palette=method_colors)\n</pre> plot_cell_counts(segmentations_dict, figures_path, palette=method_colors) plot_cell_area(segmentations_dict, figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Find markers for scRNAseq data\nmarkers = find_markers(\n    scRNAseq_adata,\n    cell_type_column=\"celltype_major\",\n    pos_percentile=30,\n    neg_percentile=5,\n)\n</pre> # Find markers for scRNAseq data markers = find_markers(     scRNAseq_adata,     cell_type_column=\"celltype_major\",     pos_percentile=30,     neg_percentile=5, ) <p>Annotate spatial segmentations with scRNAseq reference data for method in segmentation_paths.keys(): segmentations_dict[method] = annotate_query_with_reference( reference_adata=scRNAseq_adata, query_adata=segmentations_dict[method], transfer_column='celltype_major' ) segmentations_dict[method].write(segmentation_paths[method])</p> In\u00a0[\u00a0]: Copied! <pre>sc._settings.ScanpyConfig.figdir = figures_path\nsegmentations_dict[\"segger_embedding\"].obsm[\"spatial\"] = (\n    segmentations_dict[\"segger_embedding\"]\n    .obs[[\"cell_centroid_x\", \"cell_centroid_y\"]]\n    .values\n)\nsc.pl.spatial(\n    segmentations_dict[\"segger_embedding\"],\n    spot_size=10,\n    save=\"embedding.pdf\",\n    color=\"celltype_major\",\n    palette=major_colors,\n)\n</pre> sc._settings.ScanpyConfig.figdir = figures_path segmentations_dict[\"segger_embedding\"].obsm[\"spatial\"] = (     segmentations_dict[\"segger_embedding\"]     .obs[[\"cell_centroid_x\", \"cell_centroid_y\"]]     .values ) sc.pl.spatial(     segmentations_dict[\"segger_embedding\"],     spot_size=10,     save=\"embedding.pdf\",     color=\"celltype_major\",     palette=major_colors, ) In\u00a0[\u00a0]: Copied! <pre># Find mutually exclusive genes based on scRNAseq data\nexclusive_gene_pairs = find_mutually_exclusive_genes(\n    adata=scRNAseq_adata, markers=markers, cell_type_column=\"celltype_major\"\n)\n</pre> # Find mutually exclusive genes based on scRNAseq data exclusive_gene_pairs = find_mutually_exclusive_genes(     adata=scRNAseq_adata, markers=markers, cell_type_column=\"celltype_major\" ) In\u00a0[\u00a0]: Copied! <pre># Compute MECR for each segmentation method\nmecr_results = {}\nfor method in segmentations_dict.keys():\n    mecr_results[method] = compute_MECR(\n        segmentations_dict[method], exclusive_gene_pairs\n    )\n</pre> # Compute MECR for each segmentation method mecr_results = {} for method in segmentations_dict.keys():     mecr_results[method] = compute_MECR(         segmentations_dict[method], exclusive_gene_pairs     ) In\u00a0[\u00a0]: Copied! <pre># Compute quantized MECR area and counts using the mutually exclusive gene pairs\nquantized_mecr_area = {}\nquantized_mecr_counts = {}\n</pre> # Compute quantized MECR area and counts using the mutually exclusive gene pairs quantized_mecr_area = {} quantized_mecr_counts = {} In\u00a0[\u00a0]: Copied! <pre>for method in segmentations_dict.keys():\n    if \"cell_area\" in segmentations_dict[method].obs.columns:\n        quantized_mecr_area[method] = compute_quantized_mecr_area(\n            adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs\n        )\n    quantized_mecr_counts[method] = compute_quantized_mecr_counts(\n        adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs\n    )\n</pre> for method in segmentations_dict.keys():     if \"cell_area\" in segmentations_dict[method].obs.columns:         quantized_mecr_area[method] = compute_quantized_mecr_area(             adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs         )     quantized_mecr_counts[method] = compute_quantized_mecr_counts(         adata=segmentations_dict[method], gene_pairs=exclusive_gene_pairs     ) In\u00a0[\u00a0]: Copied! <pre># Plot MECR results\nplot_mecr_results(mecr_results, output_path=figures_path, palette=method_colors)\nplot_quantized_mecr_area(\n    quantized_mecr_area, output_path=figures_path, palette=method_colors\n)\nplot_quantized_mecr_counts(\n    quantized_mecr_counts, output_path=figures_path, palette=method_colors\n)\n</pre> # Plot MECR results plot_mecr_results(mecr_results, output_path=figures_path, palette=method_colors) plot_quantized_mecr_area(     quantized_mecr_area, output_path=figures_path, palette=method_colors ) plot_quantized_mecr_counts(     quantized_mecr_counts, output_path=figures_path, palette=method_colors ) In\u00a0[\u00a0]: Copied! <pre># Filter segmentation methods for contamination analysis\nnew_segmentations_dict = {\n    k: v\n    for k, v in segmentations_dict.items()\n    if k in [\"segger\", \"Baysor\", \"10X\", \"10X-nucleus\", \"BIDCell\"]\n}\n</pre> # Filter segmentation methods for contamination analysis new_segmentations_dict = {     k: v     for k, v in segmentations_dict.items()     if k in [\"segger\", \"Baysor\", \"10X\", \"10X-nucleus\", \"BIDCell\"] } In\u00a0[\u00a0]: Copied! <pre># Compute contamination results\ncontamination_results = {}\nfor method, adata in new_segmentations_dict.items():\n    if (\n        \"cell_centroid_x\" in adata.obs.columns\n        and \"cell_centroid_y\" in adata.obs.columns\n    ):\n        contamination_results[method] = calculate_contamination(\n            adata=adata,\n            markers=markers,  # Assuming you have a dictionary of markers for cell types\n            radius=15,\n            n_neighs=20,\n            celltype_column=\"celltype_major\",\n            num_cells=10000,\n        )\n</pre> # Compute contamination results contamination_results = {} for method, adata in new_segmentations_dict.items():     if (         \"cell_centroid_x\" in adata.obs.columns         and \"cell_centroid_y\" in adata.obs.columns     ):         contamination_results[method] = calculate_contamination(             adata=adata,             markers=markers,  # Assuming you have a dictionary of markers for cell types             radius=15,             n_neighs=20,             celltype_column=\"celltype_major\",             num_cells=10000,         ) In\u00a0[\u00a0]: Copied! <pre># Prepare contamination data for boxplots\nboxplot_data = []\nfor method, df in contamination_results.items():\n    melted_df = df.reset_index().melt(\n        id_vars=[\"Source Cell Type\"],\n        var_name=\"Target Cell Type\",\n        value_name=\"Contamination\",\n    )\n    melted_df[\"Segmentation Method\"] = method\n    boxplot_data.append(melted_df)\n</pre> # Prepare contamination data for boxplots boxplot_data = [] for method, df in contamination_results.items():     melted_df = df.reset_index().melt(         id_vars=[\"Source Cell Type\"],         var_name=\"Target Cell Type\",         value_name=\"Contamination\",     )     melted_df[\"Segmentation Method\"] = method     boxplot_data.append(melted_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all contamination dataframes into one\nboxplot_data = pd.concat(boxplot_data)\n</pre> # Concatenate all contamination dataframes into one boxplot_data = pd.concat(boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot contamination results\nplot_contamination_results(\n    contamination_results, output_path=figures_path, palette=method_colors\n)\nplot_contamination_boxplots(\n    boxplot_data, output_path=figures_path, palette=method_colors\n)\n</pre> # Plot contamination results plot_contamination_results(     contamination_results, output_path=figures_path, palette=method_colors ) plot_contamination_boxplots(     boxplot_data, output_path=figures_path, palette=method_colors ) In\u00a0[\u00a0]: Copied! <pre># Separate Segger into nucleus-positive and nucleus-negative cells\nsegmentations_dict[\"segger_n1\"] = segmentations_dict[\"segger\"][\n    segmentations_dict[\"segger\"].obs.has_nucleus\n]\nsegmentations_dict[\"segger_n0\"] = segmentations_dict[\"segger\"][\n    ~segmentations_dict[\"segger\"].obs.has_nucleus\n]\n</pre> # Separate Segger into nucleus-positive and nucleus-negative cells segmentations_dict[\"segger_n1\"] = segmentations_dict[\"segger\"][     segmentations_dict[\"segger\"].obs.has_nucleus ] segmentations_dict[\"segger_n0\"] = segmentations_dict[\"segger\"][     ~segmentations_dict[\"segger\"].obs.has_nucleus ] In\u00a0[\u00a0]: Copied! <pre># Compute clustering scores for all segmentation methods\nclustering_scores = {}\nfor method, adata in segmentations_dict.items():\n    ch_score, sh_score = compute_clustering_scores(\n        adata, cell_type_column=\"celltype_major\"\n    )\n    clustering_scores[method] = (ch_score, sh_score)\n</pre> # Compute clustering scores for all segmentation methods clustering_scores = {} for method, adata in segmentations_dict.items():     ch_score, sh_score = compute_clustering_scores(         adata, cell_type_column=\"celltype_major\"     )     clustering_scores[method] = (ch_score, sh_score) In\u00a0[\u00a0]: Copied! <pre># Plot UMAPs with clustering scores in the title\nplot_umaps_with_scores(\n    segmentations_dict, clustering_scores, figures_path, palette=major_colors\n)\n</pre> # Plot UMAPs with clustering scores in the title plot_umaps_with_scores(     segmentations_dict, clustering_scores, figures_path, palette=major_colors ) In\u00a0[\u00a0]: Copied! <pre># Compute neighborhood metrics for methods with spatial data\nfor method, adata in segmentations_dict.items():\n    if \"spatial\" in list(adata.obsm.keys()):\n        compute_neighborhood_metrics(adata, radius=15, celltype_column=\"celltype_major\")\n</pre> # Compute neighborhood metrics for methods with spatial data for method, adata in segmentations_dict.items():     if \"spatial\" in list(adata.obsm.keys()):         compute_neighborhood_metrics(adata, radius=15, celltype_column=\"celltype_major\") In\u00a0[\u00a0]: Copied! <pre># Prepare neighborhood entropy data for boxplots\nentropy_boxplot_data = []\nfor method, adata in segmentations_dict.items():\n    if \"neighborhood_entropy\" in adata.obs.columns:\n        entropy_df = pd.DataFrame(\n            {\n                \"Cell Type\": adata.obs[\"celltype_major\"],\n                \"Neighborhood Entropy\": adata.obs[\"neighborhood_entropy\"],\n                \"Segmentation Method\": method,\n            }\n        )\n        # Filter out NaN values, keeping only the subsetted cells\n        entropy_df = entropy_df.dropna(subset=[\"Neighborhood Entropy\"])\n        entropy_boxplot_data.append(entropy_df)\n</pre> # Prepare neighborhood entropy data for boxplots entropy_boxplot_data = [] for method, adata in segmentations_dict.items():     if \"neighborhood_entropy\" in adata.obs.columns:         entropy_df = pd.DataFrame(             {                 \"Cell Type\": adata.obs[\"celltype_major\"],                 \"Neighborhood Entropy\": adata.obs[\"neighborhood_entropy\"],                 \"Segmentation Method\": method,             }         )         # Filter out NaN values, keeping only the subsetted cells         entropy_df = entropy_df.dropna(subset=[\"Neighborhood Entropy\"])         entropy_boxplot_data.append(entropy_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all entropy dataframes into one\nentropy_boxplot_data = pd.concat(entropy_boxplot_data)\n</pre> # Concatenate all entropy dataframes into one entropy_boxplot_data = pd.concat(entropy_boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot neighborhood entropy boxplots\nplot_entropy_boxplots(entropy_boxplot_data, figures_path, palette=method_colors)\n</pre> # Plot neighborhood entropy boxplots plot_entropy_boxplots(entropy_boxplot_data, figures_path, palette=method_colors) In\u00a0[\u00a0]: Copied! <pre># Find markers for sensitivity calculation\npurified_markers = find_markers(\n    scRNAseq_adata, \"celltype_major\", pos_percentile=20, percentage=75\n)\n</pre> # Find markers for sensitivity calculation purified_markers = find_markers(     scRNAseq_adata, \"celltype_major\", pos_percentile=20, percentage=75 ) In\u00a0[\u00a0]: Copied! <pre># Calculate sensitivity for each segmentation method\nsensitivity_results_per_method = {}\nfor method, adata in segmentations_dict.items():\n    sensitivity_results = calculate_sensitivity(\n        adata, purified_markers, max_cells_per_type=2000\n    )\n    sensitivity_results_per_method[method] = sensitivity_results\n</pre> # Calculate sensitivity for each segmentation method sensitivity_results_per_method = {} for method, adata in segmentations_dict.items():     sensitivity_results = calculate_sensitivity(         adata, purified_markers, max_cells_per_type=2000     )     sensitivity_results_per_method[method] = sensitivity_results In\u00a0[\u00a0]: Copied! <pre># Prepare data for sensitivity boxplots\nsensitivity_boxplot_data = []\nfor method, sensitivity_results in sensitivity_results_per_method.items():\n    for cell_type, sensitivities in sensitivity_results.items():\n        method_df = pd.DataFrame(\n            {\n                \"Cell Type\": cell_type,\n                \"Sensitivity\": sensitivities,\n                \"Segmentation Method\": method,\n            }\n        )\n        sensitivity_boxplot_data.append(method_df)\n</pre> # Prepare data for sensitivity boxplots sensitivity_boxplot_data = [] for method, sensitivity_results in sensitivity_results_per_method.items():     for cell_type, sensitivities in sensitivity_results.items():         method_df = pd.DataFrame(             {                 \"Cell Type\": cell_type,                 \"Sensitivity\": sensitivities,                 \"Segmentation Method\": method,             }         )         sensitivity_boxplot_data.append(method_df) In\u00a0[\u00a0]: Copied! <pre># Concatenate all sensitivity dataframes into one\nsensitivity_boxplot_data = pd.concat(sensitivity_boxplot_data)\n</pre> # Concatenate all sensitivity dataframes into one sensitivity_boxplot_data = pd.concat(sensitivity_boxplot_data) In\u00a0[\u00a0]: Copied! <pre># Plot sensitivity boxplots\nplot_sensitivity_boxplots(sensitivity_boxplot_data, figures_path, palette=method_colors)\n</pre> # Plot sensitivity boxplots plot_sensitivity_boxplots(sensitivity_boxplot_data, figures_path, palette=method_colors)"},{"location":"notebooks/segger_tutorial/","title":"Introduction to Segger","text":"<p>Installing segger from the GitHub repository:</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/EliHei2/segger_dev.git\n%cd segger_dev\n!pip install \".[rapids12]\" -q\n</pre> !git clone https://github.com/EliHei2/segger_dev.git %cd segger_dev !pip install \".[rapids12]\" -q <p>Downloading the Xenium Human Pancreatic Dataset:</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir data_xenium\n%cd data_xenium\n!wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n!unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip\n%cd ..\n</pre> !mkdir data_xenium %cd data_xenium !wget https://cf.10xgenomics.com/samples/xenium/1.6.0/Xenium_V1_hPancreas_Cancer_Add_on_FFPE/Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip !unzip Xenium_V1_hPancreas_Cancer_Add_on_FFPE_outs.zip %cd .. In\u00a0[\u00a0]: Copied! <pre>from segger.data.sample import STSampleParquet\nfrom segger.data._utils import calculate_gene_celltype_abundance_embedding, find_markers, find_mutually_exclusive_genes # Optional: import if using a scRNAseq data\nfrom segger.training.segger_data_module import SeggerDataModule\nfrom segger.training.train import LitSegger\nfrom segger.prediction.predict_parquet import segment, load_model\nfrom lightning.pytorch.loggers import CSVLogger\nfrom pytorch_lightning import Trainer\nfrom pathlib import Path\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport scanpy as sc\n</pre> from segger.data.sample import STSampleParquet from segger.data._utils import calculate_gene_celltype_abundance_embedding, find_markers, find_mutually_exclusive_genes # Optional: import if using a scRNAseq data from segger.training.segger_data_module import SeggerDataModule from segger.training.train import LitSegger from segger.prediction.predict_parquet import segment, load_model from lightning.pytorch.loggers import CSVLogger from pytorch_lightning import Trainer from pathlib import Path import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import scanpy as sc In\u00a0[\u00a0]: Copied! <pre>xenium_data_dir = Path('data_xenium')\nsegger_data_dir = Path('data_segger')\n\nsample = STSampleParquet(\n    base_dir=xenium_data_dir,\n    n_workers=4,\n    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n    # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available\n)\n</pre> xenium_data_dir = Path('data_xenium') segger_data_dir = Path('data_segger')  sample = STSampleParquet(     base_dir=xenium_data_dir,     n_workers=4,     sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.     # weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available ) In\u00a0[\u00a0]: Copied! <pre>scrnaseq_file = Path('my_scRNAseq_file.h5ad')\ncelltype_column = 'celltype_column'\nscrnaseq_adata = sc.read(scrnaseq_file)\ngene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(\n    scrnaseq_adata,\n    celltype_column\n)\n\nsample = STSampleParquet(\n    base_dir=xenium_data_dir,\n    n_workers=4,\n    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n    weights=gene_celltype_abundance_embedding,  # This is where the gene embeddings are plugged in.\n)\n</pre> scrnaseq_file = Path('my_scRNAseq_file.h5ad') celltype_column = 'celltype_column' scrnaseq_adata = sc.read(scrnaseq_file) gene_celltype_abundance_embedding = calculate_gene_celltype_abundance_embedding(     scrnaseq_adata,     celltype_column )  sample = STSampleParquet(     base_dir=xenium_data_dir,     n_workers=4,     sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.     weights=gene_celltype_abundance_embedding,  # This is where the gene embeddings are plugged in. ) In\u00a0[\u00a0]: Copied! <pre># Restrict to genes available in the sample to avoid OOV\ngenes = list(set(scrnaseq_adata.var_names) &amp; set(sample.transcripts_metadata['feature_names']))\nscrnaseq_adata_sub = scrnaseq_adata[:, genes]\n\nmarkers = find_markers(\n    scrnaseq_adata_sub,\n    cell_type_column=celltype_column,\n    pos_percentile=90,\n    neg_percentile=20,\n    percentage=20,\n)\n\nexclusive_gene_pairs = find_mutually_exclusive_genes(\n    adata=scrnaseq_adata,\n    markers=markers,\n    cell_type_column=celltype_column,\n)\n</pre> # Restrict to genes available in the sample to avoid OOV genes = list(set(scrnaseq_adata.var_names) &amp; set(sample.transcripts_metadata['feature_names'])) scrnaseq_adata_sub = scrnaseq_adata[:, genes]  markers = find_markers(     scrnaseq_adata_sub,     cell_type_column=celltype_column,     pos_percentile=90,     neg_percentile=20,     percentage=20, )  exclusive_gene_pairs = find_mutually_exclusive_genes(     adata=scrnaseq_adata,     markers=markers,     cell_type_column=celltype_column, ) In\u00a0[\u00a0]: Copied! <pre># Save tiles using mutually exclusive genes\nsample.save(\n    data_dir=\"/path/to/processed_tiles\",\n    k_bd=3, dist_bd=15.0,\n    k_tx=20, dist_tx=5.0,\n    k_tx_ex=100, dist_tx_ex=20.0, # Optional when using mutually exclusive genes\n    tile_size=50_000,\n    neg_sampling_ratio=5.0,\n    val_prob=0.1, test_prob=0.2,\n    mutually_exclusive_genes=exclusive_gene_pairs, # Optional when using mutually exclusive genes\n)\n</pre> # Save tiles using mutually exclusive genes sample.save(     data_dir=\"/path/to/processed_tiles\",     k_bd=3, dist_bd=15.0,     k_tx=20, dist_tx=5.0,     k_tx_ex=100, dist_tx_ex=20.0, # Optional when using mutually exclusive genes     tile_size=50_000,     neg_sampling_ratio=5.0,     val_prob=0.1, test_prob=0.2,     mutually_exclusive_genes=exclusive_gene_pairs, # Optional when using mutually exclusive genes ) In\u00a0[\u00a0]: Copied! <pre># Base directory to store Pytorch Lightning models\nmodels_dir = Path('models')\n\n# Initialize the Lightning data module\ndm = SeggerDataModule(\n    data_dir=segger_data_dir,\n    batch_size=2,\n    num_workers=2,\n)\n\ndm.setup()\n\nnum_tx_tokens = 500\n\n# If you use custom gene embeddings, use the following two lines instead:\n# num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes\n\n\nmodel = Segger(\n    # is_token_based=is_token_based,\n    num_tx_tokens=num_tx_tokens,\n    init_emb=8,\n    hidden_channels=64,\n    out_channels=16,\n    heads=4,\n    num_mid_layers=3,\n)\nmodel = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")\n\nbatch = dm.train[0]\nmodel.forward(batch.x_dict, batch.edge_index_dict)\n# Wrap the model in LitSegger\nls = LitSegger(model=model)\n\n# Initialize the Lightning trainer\ntrainer = Trainer(\n    accelerator='cuda',\n    strategy='auto',\n    precision='16-mixed',\n    devices=1, # set higher number if more gpus are available\n    max_epochs=100,\n    default_root_dir=models_dir,\n    logger=CSVLogger(models_dir),\n)\n</pre> # Base directory to store Pytorch Lightning models models_dir = Path('models')  # Initialize the Lightning data module dm = SeggerDataModule(     data_dir=segger_data_dir,     batch_size=2,     num_workers=2, )  dm.setup()  num_tx_tokens = 500  # If you use custom gene embeddings, use the following two lines instead: # num_tx_tokens = dm.train[0].x_dict[\"tx\"].shape[1] # Set the number of tokens to the number of genes   model = Segger(     # is_token_based=is_token_based,     num_tx_tokens=num_tx_tokens,     init_emb=8,     hidden_channels=64,     out_channels=16,     heads=4,     num_mid_layers=3, ) model = to_hetero(model, ([\"tx\", \"bd\"], [(\"tx\", \"belongs\", \"bd\"), (\"tx\", \"neighbors\", \"tx\")]), aggr=\"sum\")  batch = dm.train[0] model.forward(batch.x_dict, batch.edge_index_dict) # Wrap the model in LitSegger ls = LitSegger(model=model)  # Initialize the Lightning trainer trainer = Trainer(     accelerator='cuda',     strategy='auto',     precision='16-mixed',     devices=1, # set higher number if more gpus are available     max_epochs=100,     default_root_dir=models_dir,     logger=CSVLogger(models_dir), ) In\u00a0[\u00a0]: Copied! <pre># Fit model\ntrainer.fit(\n    model=ls,\n    datamodule=dm\n)\n</pre> # Fit model trainer.fit(     model=ls,     datamodule=dm ) <p>Key parameters for training:</p> <ul> <li><code>--data_dir</code>: Directory containing the training data.</li> <li><code>--model_dir</code>: Directory in which to store models.</li> <li><code>--epochs</code>: Specifies the number of training epochs.</li> <li><code>--batch_size</code>: Batch sizes for training and validation data.</li> <li><code>--learning_rate</code>: The initial learning rate for the optimizer.</li> <li><code>--hidden_channels</code>: Number of hidden channels in the GNN layers.</li> <li><code>--heads</code>: Number of attention heads used in each graph convolutional layer.</li> <li><code>--init_emb</code>: Sets the dimensionality of the initial embeddings applied to the input node features (e.g., transcripts). A higher embedding dimension may capture more feature complexity but also requires more computation.</li> <li><code>--out_channels</code>: Specifies the number of output channels after the final graph attention layer, e.g. the final learned representations of the graph nodes.</li> </ul> <p>Additional Options for Training the Segger Model:</p> <ul> <li><code>--aggr</code>: This option controls the aggregation method used in the graph convolution layers.</li> <li><code>--accelerator</code>: Controls the hardware used for training, such as <code>cuda</code> for GPU training. This enables Segger to leverage GPU resources for faster training, especially useful for large datasets.</li> <li><code>--strategy</code>: Defines the distributed training strategy, with <code>auto</code> allowing PyTorch Lightning to automatically configure the best strategy based on the hardware setup.</li> <li><code>--precision</code>: Enables mixed precision training (e.g., <code>16-mixed</code>), which can speed up training and reduce memory usage while maintaining accuracy.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Evaluate results\nmodel_version = 0  # 'v_num' from training output above\nmodel_path = Path('../human_CRC') / 'lightning_logs' / f'version_{model_version}'\nmetrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)\n\nfig, ax = plt.subplots(1,1, figsize=(2,2))\n\nfor col in metrics.columns.difference(['epoch']):\n    metric = metrics[col].dropna()\n    ax.plot(metric.index, metric.values, label=col)\n\nax.legend(loc=(1, 0.33))\nax.set_ylim(0, 1)\nax.set_xlabel('Step')\n</pre> # Evaluate results model_version = 0  # 'v_num' from training output above model_path = Path('../human_CRC') / 'lightning_logs' / f'version_{model_version}' metrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)  fig, ax = plt.subplots(1,1, figsize=(2,2))  for col in metrics.columns.difference(['epoch']):     metric = metrics[col].dropna()     ax.plot(metric.index, metric.values, label=col)  ax.legend(loc=(1, 0.33)) ax.set_ylim(0, 1) ax.set_xlabel('Step') In\u00a0[\u00a0]: Copied! <pre># Evaluate results\nmodel_version = 0  # 'v_num' from training output above\nmodel_path = models_dir / 'lightning_logs' / f'version_{model_version}'\nmetrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)\n\nfig, ax = plt.subplots(1,1, figsize=(2,2))\n\nfor col in metrics.columns.difference(['epoch']):\n    metric = metrics[col].dropna()\n    ax.plot(metric.index, metric.values, label=col)\n\nax.legend(loc=(1, 0.33))\nax.set_ylim(0, 1)\nax.set_xlabel('Step')\n</pre> # Evaluate results model_version = 0  # 'v_num' from training output above model_path = models_dir / 'lightning_logs' / f'version_{model_version}' metrics = pd.read_csv(model_path / 'metrics.csv', index_col=1)  fig, ax = plt.subplots(1,1, figsize=(2,2))  for col in metrics.columns.difference(['epoch']):     metric = metrics[col].dropna()     ax.plot(metric.index, metric.values, label=col)  ax.legend(loc=(1, 0.33)) ax.set_ylim(0, 1) ax.set_xlabel('Step') Out[\u00a0]: <pre>Text(0.5, 0, 'Step')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>dm = SeggerDataModule(\n    data_dir='data_segger',\n    batch_size=1,\n    num_workers=4,\n)\n\ndm.setup()\n\nmodel_version = 0\nmodel_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\"\nmodel = load_model(model_path / \"checkpoints\")\n\nreceptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}\n\nsegment(\n    model,\n    dm,\n    save_dir='benchmarks',\n    seg_tag='segger_output',\n    transcript_file='data_xenium/transcripts.parquet',\n    receptive_field=receptive_field,\n    min_transcripts=5,\n    cell_id_col='segger_cell_id',\n    use_cc=False,\n    knn_method='kd_tree',\n    verbose=True,\n)\n</pre> dm = SeggerDataModule(     data_dir='data_segger',     batch_size=1,     num_workers=4, )  dm.setup()  model_version = 0 model_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\" model = load_model(model_path / \"checkpoints\")  receptive_field = {'k_bd': 4, 'dist_bd': 12, 'k_tx': 15, 'dist_tx': 3}  segment(     model,     dm,     save_dir='benchmarks',     seg_tag='segger_output',     transcript_file='data_xenium/transcripts.parquet',     receptive_field=receptive_field,     min_transcripts=5,     cell_id_col='segger_cell_id',     use_cc=False,     knn_method='kd_tree',     verbose=True, )  <pre>Starting segmentation for segger_embedding_1001...\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(2,2))\nsns.histplot(\n    segmentation['score'],\n    bins=50,\n    ax=ax,\n)\nax.set_ylabel('Count')\nax.set_xlabel('Segger Similarity Score')\nax.set_yscale('log')\n</pre> fig, ax = plt.subplots(1,1, figsize=(2,2)) sns.histplot(     segmentation['score'],     bins=50,     ax=ax, ) ax.set_ylabel('Count') ax.set_xlabel('Segger Similarity Score') ax.set_yscale('log') In\u00a0[\u00a0]: Copied! <pre>import itertools\nimport pandas as pd\n</pre> import itertools import pandas as pd In\u00a0[\u00a0]: Copied! <pre>tuning_dir = Path('path/to/tutorial/tuning/')\nsampling_rate = 0.125\n</pre> tuning_dir = Path('path/to/tutorial/tuning/') sampling_rate = 0.125 In\u00a0[\u00a0]: Copied! <pre># Fixed function arguments used for each trial\ntranscripts_path = xenium_data_dir / 'transcripts.parquet'\n\nboundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'\n\ndataset_kwargs = dict(\n    x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,\n    num_workers=4, sampling_rate=sampling_rate,\n)\n\nmodel_kwargs = dict(\n    metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),\n    num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,\n    heads=2, num_mid_layers=2, aggr='sum',\n)\n\ntrainer_kwargs = dict(\n    accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,\n    max_epochs=100,\n)\n\npredict_kwargs = dict(score_cut=0.2, use_cc=True)\n</pre> # Fixed function arguments used for each trial transcripts_path = xenium_data_dir / 'transcripts.parquet'  boundaries_path = xenium_data_dir / 'nucleus_boundaries.parquet'  dataset_kwargs = dict(     x_size=80, y_size=80, d_x=80, d_y=80, margin_x=10, margin_y=10,     num_workers=4, sampling_rate=sampling_rate, )  model_kwargs = dict(     metadata=(['tx', 'bd'], [('tx', 'belongs', 'bd'), ('tx', 'neighbors', 'tx')]),     num_tx_tokens=500, init_emb=8, hidden_channels=32, out_channels=8,     heads=2, num_mid_layers=2, aggr='sum', )  trainer_kwargs = dict(     accelerator='cuda', strategy='auto', precision='16-mixed', devices=1,     max_epochs=100, )  predict_kwargs = dict(score_cut=0.2, use_cc=True) In\u00a0[\u00a0]: Copied! <pre>def trainable(config):\n\n    receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}\n\n    # Dataset creation\n    xs = XeniumSample(verbose=False)\n    xs.set_file_paths(transcripts_path, boundaries_path)\n    xs.set_metadata()\n    try:\n        xs.save_dataset_for_segger(\n            processed_dir=config['data_dir'],\n            receptive_field=receptive_field,\n            **dataset_kwargs,\n        )\n    except:\n        pass\n\n    # Model training\n    ls = LitSegger(**model_kwargs)\n    dm = SeggerDataModule(\n        data_dir=config['data_dir'],\n        batch_size=2,\n        num_workers=dataset_kwargs['num_workers'],\n    )\n    trainer = Trainer(\n        default_root_dir=config['model_dir'],\n        logger=CSVLogger(config['model_dir']),\n        **trainer_kwargs,\n    )\n    trainer.fit(model=ls, datamodule=dm)\n\n    segmentation = predict(\n        load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),\n        dm.train_dataloader(),\n        receptive_field=receptive_field,\n        **predict_kwargs,\n    )\n\n    metrics = evaluate(segmentation)\n\n\ndef evaluate(segmentation: pd.DataFrame, score_cut: float) -&gt; pd.Series:\n\n    assigned = segmentation['score'] &gt; score_cut\n    metrics = pd.Series(dtype=float)\n    metrics['frac_assigned'] = assigned.mean()\n    cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()\n    assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()\n    cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()\n    metrics['cell_size_assigned'] = assigned_avg\n    metrics['cell_size_cc'] = cc_avg\n    return metrics\n</pre> def trainable(config):      receptive_field = {k: config[k] for k in ['k_bd', 'k_tx', 'dist_bd', 'dist_tx']}      # Dataset creation     xs = XeniumSample(verbose=False)     xs.set_file_paths(transcripts_path, boundaries_path)     xs.set_metadata()     try:         xs.save_dataset_for_segger(             processed_dir=config['data_dir'],             receptive_field=receptive_field,             **dataset_kwargs,         )     except:         pass      # Model training     ls = LitSegger(**model_kwargs)     dm = SeggerDataModule(         data_dir=config['data_dir'],         batch_size=2,         num_workers=dataset_kwargs['num_workers'],     )     trainer = Trainer(         default_root_dir=config['model_dir'],         logger=CSVLogger(config['model_dir']),         **trainer_kwargs,     )     trainer.fit(model=ls, datamodule=dm)      segmentation = predict(         load_model(config['model_dir']/'lightning_logs/version_0/checkpoints'),         dm.train_dataloader(),         receptive_field=receptive_field,         **predict_kwargs,     )      metrics = evaluate(segmentation)   def evaluate(segmentation: pd.DataFrame, score_cut: float) -&gt; pd.Series:      assigned = segmentation['score'] &gt; score_cut     metrics = pd.Series(dtype=float)     metrics['frac_assigned'] = assigned.mean()     cell_sizes = segmentation.groupby(assigned)['segger_cell_id'].value_counts()     assigned_avg = 0 if True not in cell_sizes.index else cell_sizes[True].mean()     cc_avg = 0 if False not in cell_sizes.index else cell_sizes[False].mean()     metrics['cell_size_assigned'] = assigned_avg     metrics['cell_size_cc'] = cc_avg     return metrics In\u00a0[\u00a0]: Copied! <pre>param_space = {\n    \"k_bd\": [3, 5, 10],\n    \"dist_bd\": [5, 10, 15, 20],\n    \"k_tx\": [3, 5, 10],\n    \"dist_tx\": [3, 5, 10],\n}\n\nmetrics = []\n\nfor params in itertools.product(*param_space.values()):\n\n    config = dict(zip(param_space.keys(), params))\n\n    # Setup directories\n    trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])\n\n    data_dir = trial_dir / 'segger_data'\n    data_dir.mkdir(exist_ok=True, parents=True)\n    config['data_dir'] = data_dir\n\n    model_dir = trial_dir / 'models'\n    model_dir.mkdir(exist_ok=True, parents=True)\n    config['model_dir'] = model_dir\n\n    segmentation = trainable(config)\n    trial = evaluate(segmentation, predict_kwargs['score_cut'])\n    trial = pd.concat([pd.Series(config), trial])\n    metrics.append(trial)\n\nmetrics = pd.DataFrame(metrics)\n</pre> param_space = {     \"k_bd\": [3, 5, 10],     \"dist_bd\": [5, 10, 15, 20],     \"k_tx\": [3, 5, 10],     \"dist_tx\": [3, 5, 10], }  metrics = []  for params in itertools.product(*param_space.values()):      config = dict(zip(param_space.keys(), params))      # Setup directories     trial_dir = tuning_dir / '_'.join([f'{k}={v}' for k, v in config.items()])      data_dir = trial_dir / 'segger_data'     data_dir.mkdir(exist_ok=True, parents=True)     config['data_dir'] = data_dir      model_dir = trial_dir / 'models'     model_dir.mkdir(exist_ok=True, parents=True)     config['model_dir'] = model_dir      segmentation = trainable(config)     trial = evaluate(segmentation, predict_kwargs['score_cut'])     trial = pd.concat([pd.Series(config), trial])     metrics.append(trial)  metrics = pd.DataFrame(metrics) In\u00a0[\u00a0]: Copied! <pre>metrics\n</pre> metrics"},{"location":"notebooks/segger_tutorial/#introduction-to-segger","title":"Introduction to Segger\u00b6","text":"<p>Important note (Dec 2024): As segger is currently undergoing constant development we highly recommend installing directly via github.</p> <p>Segger is a cutting-edge cell segmentation model specifically designed for single-molecule resolved spatial omics datasets. It addresses the challenge of accurately segmenting individual cells in complex imaging datasets, leveraging a unique approach based on graph neural networks (GNNs).</p> <p>The core idea behind Segger is to model both nuclei and transcripts as graph nodes, with edges connecting them based on their spatial proximity. This allows the model to learn from the co-occurrence of nucleic and cytoplasmic molecules, resulting in more refined and accurate cell boundaries. By using spatial information and GNNs, Segger achieves state-of-the-art performance in segmenting single cells in datasets such as 10X Xenium and MERSCOPE, outperforming traditional methods like Baysor and Cellpose.</p> <p>Segger's workflow consists of:</p> <ol> <li>Dataset creation: Converting raw transcriptomic data into a graph-based dataset.</li> <li>Training: Training the Segger model on the graph to learn cell boundaries.</li> <li>Prediction: Using the trained model to make predictions on new datasets.</li> </ol> <p>This tutorial will guide you through each step of the process, ensuring you can train and apply Segger for your own data.</p>"},{"location":"notebooks/segger_tutorial/#1-create-your-segger-dataset","title":"1. Create your Segger Dataset\u00b6","text":"<p>In this step, we generate the dataset required for Segger's cell segmentation tasks.</p> <p>Segger relies on spatial transcriptomics data, combining staining boundaries (e.g., nuclei or membrane stainings) and transcripts from single-cell resolved imaging datasets. These nuclei and transcript nodes are represented in a graph, and the spatial proximity of transcripts to nuclei is used to establish edges between them.</p> <p>To use Segger with a Xenium dataset, you need the <code>transcripts.parquet</code> and <code>nucleus_boundaries.parquet</code> (or <code>cell_boundaries.parquet</code>, in case the Xenium samples comes with the segmentation kit) files. The transcripts file contains spatial coordinates and information for each transcript, while the boundaries file defines the polygon boundaries of the nuclei or cells. These files enable segger to map transcripts to their respective nuclei and perform cell segmentation based on spatial relationships. Segger can also be extended to other platforms by modifying the column names or formats in the input files to match its expected structure, making it adaptable for various spatial transcriptomics technologies. See this for Xenium settings.</p>"},{"location":"notebooks/segger_tutorial/#11-fast-dataset-creation-with-segger","title":"1.1. Fast Dataset Creation with segger\u00b6","text":"<p>Segger introduces a fast and efficient pipeline for processing spatial transcriptomics data. This method accelerates dataset creation, particularly for large datasets, by using ND-tree-based spatial partitioning and parallel processing. This results in a much faster preparation of the dataset, which is saved in PyTorch Geometric (PyG) format, similar to the previous method.</p> <p>Note: The previous dataset creation method will soon be deprecated in favor of this optimized pipeline.</p> <p>The pipeline requires the following inputs:</p> <ul> <li>base_dir: The directory containing the raw dataset.</li> <li>data_dir: The directory where the processed dataset (tiles in PyG format) will be saved.</li> <li>--sample_type: (Optional) Specifies the type of dataset (e.g., \"xenium\" or \"merscope\"). Defaults to None.</li> <li>--n_workers: Number of workers for parallel processing (default: 1).</li> </ul> <p>The core improvements in this method come from the use of ND-tree partitioning, which splits the data efficiently into spatial regions, and parallel processing, which speeds up the handling of these regions across multiple CPU cores. For example, using this pipeline, the Xenium Human Pancreatic Dataset can be processed in just a few minutes when running with 16 workers.</p> <p>Below is an example of how to create a dataset using the faster Segger pipeline:</p>"},{"location":"notebooks/segger_tutorial/#12-using-scrnaseq-data-for-gene-embeddings-and-mutually-exclusive-genes","title":"1.2. Using scRNAseq data for gene embeddings and mutually exclusive genes\u00b6","text":"<p>In the default mode, segger initially tokenizes transcripts based on their gene type simply in a one-hot manner. However, one can use other genes embeddings (e.g., pre-trained embeddings). Furthermore, optionally, one can idea identify mutually exclusive gene pairs that guide repulsive/attractive transcript edges. The following example shows how to employ a cell-type-annotated scRNAseq reference of the same tissue type (not necessary same sample or experiment) to embed genes based on their abaundance in different cell types:</p>"},{"location":"notebooks/segger_tutorial/#parameters","title":"Parameters\u00b6","text":"<p>Here is a complete list of parameters you can use to control the dataset creation process:</p> <ul> <li>--base_dir: Directory containing the raw spatial transcriptomics dataset.</li> <li>--data_dir: Directory where the processed Segger dataset (in PyG format) will be saved.</li> <li>--scrnaseq_file: Path to the scRNAseq file (default: None).</li> <li>--celltype_column: Column name for cell type annotations in the scRNAseq file (default: None).</li> <li>--k_bd: Number of nearest neighbors for boundary nodes (default: 3).</li> <li>--dist_bd: Maximum distance for boundary neighbors (default: 15.0).</li> <li>--k_tx: Number of nearest neighbors for transcript nodes (default: 3).</li> <li>--dist_tx: Maximum distance for transcript neighbors (default: 5.0).</li> <li>--k_tx_ex: Number of nearest neighbors for transcript exclusion (used when applying mutually exclusive gene pairs) (default: 100).</li> <li>--dist_tx_ex: Maximum distance for transcript exclusion neighbors (paired with k_tx_ex) (default: 20.0).</li> <li>--tile_size: Specifies the size of the tile. If provided, it overrides both tile_width and tile_height.</li> <li>--tile_width: Width of the tiles in pixels (ignored if tile_size is provided).</li> <li>--tile_height: Height of the tiles in pixels (ignored if tile_size is provided).</li> <li>--neg_sampling_ratio: Ratio of negative samples (default: 5.0).</li> <li>--frac: Fraction of the dataset to process (default: 1.0).</li> <li>--val_prob: Proportion of data used for validation split (default: 0.1).</li> <li>--test_prob: Proportion of data used for testing split (default: 0.2).</li> </ul>"},{"location":"notebooks/segger_tutorial/#2-train-your-segger-model","title":"2. Train your Segger Model\u00b6","text":"<p>The Segger model training process begins after the dataset has been created. This model is a heterogeneous graph neural network (GNN) designed to segment single cells by leveraging both nuclei and transcript data.</p> <p>Segger uses graph attention layers to propagate information across nodes (nuclei and transcripts) and refine cell boundaries. The model architecture includes initial embedding layers, attention-based graph convolutions, and residual connections for stable learning.</p> <p>Segger leverages the PyTorch Lightning framework to streamline the training and evaluation of its graph neural network (GNN). PyTorch Lightning simplifies the training process by abstracting away much of the boilerplate code, allowing users to focus on model development and experimentation. It also supports multi-GPU training, mixed-precision, and efficient scaling, making it an ideal framework for training complex models like Segger.</p>"},{"location":"notebooks/segger_tutorial/#troubleshooting-1","title":"Troubleshooting #1\u00b6","text":"<p>In the cell below, we are visualizing key metrics from the model training and validation process. The plot displays training loss, validation loss, F1 validation score, and AUROC validation score over training steps. We expect to see the loss curves decreasing over time, signaling the model's improvement, and the F1 and AUROC scores increasing, reflecting improved segmentation performance as the model learns.</p> <p>If training is not working effectively, you might observe the following in the plot displaying training loss, validation loss, F1 score, and AUROC:</p> <ul> <li>Training loss not decreasing: If the training loss remains high or fluctuates without a consistent downward trend, this indicates that the model is not learning effectively from the training data.</li> <li>Validation loss decreases, then increases: If validation loss decreases initially but starts to increase while training loss continues to drop, this could be a sign of overfitting, where the model is performing well on the training data but not generalizing to the validation data.</li> <li>F1 score and AUROC not improving: If these metrics remain flat or show inconsistent improvement, the model may be struggling to correctly segment cells or classify transcripts, indicating an issue with learning performance.</li> </ul>"},{"location":"notebooks/segger_tutorial/#3-make-predictions","title":"3. Make Predictions\u00b6","text":"<p>Once the Segger model is trained, it can be used to make predictions on seen (partially trained) data or be transfered to unseen data. This step involves using a trained checkpoint to predict cell boundaries and refine transcript-nuclei associations.</p>"},{"location":"notebooks/segger_tutorial/#requirements-for-the-faster-prediction-pipeline","title":"Requirements for the Faster Prediction Pipeline\u00b6","text":"<p>The pipeline requires the following inputs:</p> <ul> <li>segger_data_dir: The directory containing the processed Segger dataset (in PyG format).</li> <li>models_dir: The directory containing the trained Segger model checkpoints.</li> <li>benchmarks_dir: The directory where the segmentation results will be saved.</li> <li>transcripts_file: Path to the file containing the transcript data for prediction.</li> </ul>"},{"location":"notebooks/segger_tutorial/#running-the-faster-prediction-pipeline","title":"Running the Faster Prediction Pipeline\u00b6","text":"<p>Below is an example of how to run the faster Segger prediction pipeline using the command line:</p>"},{"location":"notebooks/segger_tutorial/#parameters","title":"Parameters\u00b6","text":"<p>Here is a detailed explanation of each parameter used in the faster prediction pipeline:</p> <ul> <li>--segger_data_dir: The directory containing the processed Segger dataset, saved as PyTorch Geometric data objects, that will be used for prediction.</li> <li>--models_dir: The directory containing the trained Segger model checkpoints. These checkpoints store the learned weights required for making predictions.</li> <li>--benchmarks_dir: The directory where the segmentation results will be saved.</li> <li>--transcripts_file: Path to the transcripts.parquet file.</li> <li>--batch_size: Specifies the batch size for processing during prediction. Larger batch sizes speed up inference but use more memory (default: 1).</li> <li>--num_workers: Number of workers to use for parallel data loading (default: 1).</li> <li>--model_version: Version of the trained model to load for predictions, based on the version number from the training logs (default: 0).</li> <li>--save_tag: A tag used to name and organize the segmentation results (default: segger_embedding).</li> <li>--min_transcripts: The minimum number of transcripts required for segmentation (default: 5).</li> <li>--cell_id_col: The name of the column that stores the cell IDs (default: segger_cell_id).</li> <li>--use_cc: Enables the use of connected components (CC) for grouping transcripts that are not associated with any nucleus (default: False).</li> <li>--knn_method: Method for KNN (K-Nearest Neighbors) computation. Only option is \"cuda\" for this pipeline (default: cuda).</li> <li>--file_format: The format for saving the output segmentation data. Only option is \"anndata\" for this pipeline (default: anndata).</li> <li>--k_bd: Number of nearest neighbors for boundary nodes during segmentation (default: 4).</li> <li>--dist_bd: Maximum distance for boundary nodes during segmentation (default: 12.0).</li> <li>--k_tx: Number of nearest neighbors for transcript nodes during segmentation (default: 5).</li> <li>--dist_tx: Maximum distance for transcript nodes during segmentation (default: 5.0).</li> </ul>"},{"location":"notebooks/segger_tutorial/#troubleshooting-2","title":"Troubleshooting #2\u00b6","text":"<p>In the cell below, we are visualizing the distribution of Segger similarity scores using a histogram. The Segger similarity score reflects how closely transcripts are associated with their respective nuclei in the segmentation process. Higher scores indicate stronger associations between transcripts and their nuclei, suggesting more accurate cell boundaries. Lower scores might indicate weaker associations, which could highlight potential segmentation errors or challenging regions in the data. We expect to see a large number of the scores clustering toward higher values, which would indicate strong overall performance of the model in associating transcripts with nuclei.</p> <p>The following would indicate potential issues with the model's predictions:</p> <ul> <li>A very large portion of scores near zero: If many scores are concentrated at the lower end of the scale, this suggests that the model is frequently failing to associate transcripts with their corresponding nuclei, indicating poor segmentation quality.</li> <li>No clear peak in the distribution: If the histogram is flat or shows a wide, spread-out distribution, this could indicate that the model is struggling to consistently assign similarity scores, which may be a sign that the training process did not optimize the model correctly.</li> </ul> <p>Both cases would suggest that the model requires further tuning, such as adjusting hyperparameters, data preprocessing, or the training procedure (see below)</p>"},{"location":"notebooks/segger_tutorial/#the-importance-of-the-receptive-field-in-segger","title":"The Importance of the Receptive Field in Segger\u00b6","text":"<p>The receptive field is a critical parameter in Segger, as it directly influences how the model interprets the spatial relationships between transcripts and nuclei. In the context of spatial transcriptomics, the receptive field determines the size of the neighborhood that each node (representing transcripts or nuclei) can \"see\" during graph construction and model training. Segger is particularly sensitive to the size of the receptive field because it affects the model's ability to propagate information across the graph. If the receptive field is too small, the model may fail to capture sufficient context for correct cell boundary delineation. Conversely, a very large receptive field may introduce noise by linking unrelated or distant nodes, reducing segmentation accuracy.</p>"},{"location":"notebooks/segger_tutorial/#parameters-affecting-the-receptive-field-in-segger","title":"Parameters affecting the receptive field in Segger:\u00b6","text":"<ul> <li><code>--r</code>: This parameter defines the radius used when connecting transcripts to nuclei. A larger <code>r</code> expands the receptive field, linking more distant nodes. Fine-tuning this parameter helps ensure that Segger captures the right level of spatial interaction in the dataset.</li> <li><code>--k_bd</code> and <code>--k_tx</code>: These control the number of nearest neighbors (nuclei and transcripts, respectively) considered in the graph. By increasing these values, the receptive field is effectively broadened, allowing more nodes to contribute to the information propagation.</li> <li><code>--dist_bd</code> and <code>--dist_tx</code>: These parameters specify the maximum distances used to connect nuclei (<code>dist_bd</code>) and transcripts (<code>dist_tx</code>) to their neighbors during graph construction. They directly affect the receptive field by defining the cut-off distance for forming edges in the graph. Larger distance values expand the receptive field, connecting nodes that are further apart spatially. Careful tuning of these values is necessary to ensure that Segger captures relevant spatial relationships without introducing noise.</li> </ul>"},{"location":"notebooks/segger_tutorial/#4-tune-parameters","title":"4. Tune Parameters\u00b6","text":""},{"location":"notebooks/segger_tutorial/#evaluating-receptive-field-parameters-with-grid-search","title":"Evaluating Receptive Field Parameters with Grid Search\u00b6","text":"<p>To evaluate the impact of different receptive field parameters in Segger, we use a grid search approach. The parameters <code>k_bd</code>, <code>k_tx</code>, <code>dist_bd</code>, and <code>dist_tx</code> (which control the number of neighbors and distances for nuclei and transcripts) are explored through various configurations defined in <code>param_space</code>. Each combination of these parameters is passed to the <code>trainable</code> function, which creates the dataset, trains the model, and makes predictions based on the specified receptive field.</p> <p>For each parameter combination:</p> <ol> <li>A dataset is created with the specified receptive field.</li> <li>The Segger model is trained on this dataset.</li> <li>Predictions are made, and segmentation results are evaluated using the custom <code>evaluate</code> function. This function computes metrics like the fraction of assigned transcripts and average cell sizes.</li> </ol> <p>The results from each configuration are saved, allowing us to compare how different receptive field settings impact the model\u2019s performance. This process enables a thorough search of the parameter space, optimizing the model for accurate segmentation.</p>"},{"location":"notebooks/segger_tutorial/#interpreting-output-metrics","title":"Interpreting Output Metrics\u00b6","text":"<p>The key output metrics include:</p> <ul> <li><code>frac_assigned</code>: The fraction of transcripts that were successfully assigned to a cell. A higher value indicates that the model is doing a good job associating transcripts with nuclei, which is a strong indicator of successful segmentation.</li> <li><code>cell_size_assigned</code>: The average size of cells that have assigned transcripts. This helps assess how well the model is predicting cell boundaries, with unusually large or small values indicating potential issues with segmentation accuracy.</li> <li><code>cell_size_cc</code>: The average size of connected components that were not assigned to a cell (i.e., nucleus-less regions). Large values here may suggest that transcripts are being incorrectly grouped together in the absence of a nucleus, which could indicate problems with the receptive field parameters or the segmentation process.</li> </ul> <p>These metrics illuminate the effectiveness of the model by highlighting both the success in associating transcripts with cells and potential areas where the model may need further tuning.</p>"},{"location":"source/conf/","title":"Conf","text":"<p>Configuration file for the Sphinx documentation builder.</p> <p>For the full list of built-in configuration values, see the documentation: https://www.sphinx-doc.org/en/master/usage/configuration.html</p> <p>-- Project information ----------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information</p> In\u00a0[\u00a0]: Copied! <pre>project = \"segger\"\ncopyright = \"2024, Elyas Heidari\"\nauthor = \"Elyas Heidari\"\nrelease = \"0.01\"\n</pre> project = \"segger\" copyright = \"2024, Elyas Heidari\" author = \"Elyas Heidari\" release = \"0.01\" <p>-- General configuration --------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration</p> In\u00a0[\u00a0]: Copied! <pre>extensions = []\n</pre> extensions = [] In\u00a0[\u00a0]: Copied! <pre>templates_path = [\"_templates\"]\nexclude_patterns = []\n</pre> templates_path = [\"_templates\"] exclude_patterns = [] <p>-- Options for HTML output ------------------------------------------------- https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output</p> In\u00a0[\u00a0]: Copied! <pre>html_theme = \"alabaster\"\nhtml_static_path = [\"_static\"]\n</pre> html_theme = \"alabaster\" html_static_path = [\"_static\"]"},{"location":"user_guide/","title":"API Reference","text":""},{"location":"user_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Dataset Creation</li> <li>Training</li> <li>Validation</li> </ul>"},{"location":"user_guide/data_creation/","title":"Data Preparation for <code>segger</code>","text":"<p>The <code>segger</code> package provides a streamlined, settings-driven pipeline to transform raw spatial transcriptomics outputs (e.g., Xenium, Merscope) into graph tiles ready for model training and evaluation.</p> <p>Note</p> <p>Currently, <code>segger</code> supports Xenium and Merscope datasets via technology-specific settings.</p>"},{"location":"user_guide/data_creation/#steps","title":"Steps","text":"<p>The data preparation pipeline includes:</p> <ol> <li>Settings-driven I/O: Uses a <code>sample_type</code> (e.g., <code>xenium</code>, <code>merscope</code>) to resolve input file names and column mappings.</li> <li>Lazy Loading + Filtering: Efficiently reads Parquet in spatial regions and filters transcripts/boundaries.</li> <li>Tiling: Partitions the whole slide into spatial tiles (fixed size or balanced by transcript count).</li> <li>Graph Construction: Builds PyTorch Geometric <code>HeteroData</code> with typed nodes/edges and labels for link prediction.</li> <li>Splitting: Writes tiles into <code>train</code>, <code>val</code>, and <code>test</code> subsets.</li> </ol> <p>Key Technologies</p> <ul> <li>PyTorch Geometric (PyG): Heterogeneous graphs for GNNs.</li> <li>Shapely &amp; GeoPandas: Geometry operations (polygons, centroids, areas).</li> <li>PyArrow Parquet: Efficient I/O with schema-aware reads.</li> </ul>"},{"location":"user_guide/data_creation/#core-components","title":"Core Components","text":""},{"location":"user_guide/data_creation/#1-stsampleparquet","title":"1. <code>STSampleParquet</code>","text":"<p>Settings-based entry point for preparing a sample into graph tiles.</p> <ul> <li>Constructor: resolves input file paths and metadata using <code>sample_type</code> settings (e.g., file names, column names, quality fields). Ensures transcript IDs exist.</li> <li>Embeddings: optionally accepts a <code>weights</code> DataFrame (index: gene names; columns: embedding dims) to encode transcript features.</li> <li>Saving: orchestrates region partitioning, tiling, PyG graph construction, negative sampling, and dataset splitting.</li> </ul>"},{"location":"user_guide/data_creation/#key-params","title":"Key Params","text":"<ul> <li><code>base_dir</code>: folder with required Parquet files (transcripts and boundaries).</li> <li><code>sample_type</code>: one of <code>xenium</code>, <code>merscope</code> (determines settings such as file names, columns, nuclear flags, scale factors).</li> <li><code>weights</code>: optional <code>pd.DataFrame</code> of gene embeddings used by <code>TranscriptEmbedding</code>.</li> <li><code>scale_factor</code>: optional override for boundary scaling used during spatial queries.</li> </ul>"},{"location":"user_guide/data_creation/#2-stinmemorydataset","title":"2. <code>STInMemoryDataset</code>","text":"<p>Internal helper that loads filtered transcripts/boundaries for a region, pre-builds a KDTree, and generates tile bounds (fixed-size or balanced by count).</p>"},{"location":"user_guide/data_creation/#3-sttile","title":"3. <code>STTile</code>","text":"<p>Per-tile builder that assembles a <code>HeteroData</code> graph: - Nodes: <code>tx</code> (transcripts) with <code>pos</code>, <code>x</code> (features); <code>bd</code> (boundaries) with <code>pos</code>, <code>x</code> (polygon properties). - Edges:   - <code>('tx','neighbors','tx')</code>: transcript proximity (KDTree-based; <code>k_tx</code>, <code>dist_tx</code>).   - <code>('tx','neighbors','bd')</code>: transcript-to-boundary proximity for receptive field construction.   - <code>('tx','belongs','bd')</code>: positive labels from nuclear overlap or provided assignment; negative sampling performed from receptive-field candidates.</p>"},{"location":"user_guide/data_creation/#workflow","title":"Workflow","text":""},{"location":"user_guide/data_creation/#step-1-initialize-sample-from-settings","title":"Step 1: Initialize sample from settings","text":"<ul> <li>Provide <code>base_dir</code> containing technology outputs.</li> <li>Pick <code>sample_type</code> to resolve filenames/columns.</li> <li>Optionally provide <code>weights</code> for transcript embeddings.</li> </ul>"},{"location":"user_guide/data_creation/#step-2-region-partitioning-and-tiling","title":"Step 2: Region partitioning and tiling","text":"<ul> <li>If multiple workers are set, extents are split into balanced regions (ND-tree over boundaries).</li> <li>Tiles are created either by fixed width/height or by a target <code>tile_size</code> (balanced by transcript count).</li> </ul>"},{"location":"user_guide/data_creation/#step-3-graph-construction-per-tile","title":"Step 3: Graph construction per tile","text":"<ul> <li>Build <code>HeteroData</code> with transcript (<code>tx</code>) and boundary (<code>bd</code>) nodes.</li> <li>Add proximity edges and <code>belongs</code> labels (positives + sampled negatives).</li> </ul>"},{"location":"user_guide/data_creation/#step-4-splitting-and-saving","title":"Step 4: Splitting and saving","text":"<ul> <li>Tiles are written to <code>&lt;data_dir&gt;/{train_tiles,val_tiles,test_tiles}/processed/*.pt</code> according to <code>val_prob</code>/<code>test_prob</code>.</li> </ul>"},{"location":"user_guide/data_creation/#output","title":"Output","text":"<ul> <li>A directory structure with train/val/test tiles in PyG <code>HeteroData</code> format ready for the Segger model and <code>STPyGDataset</code>.</li> </ul> <pre><code>&lt;data_dir&gt;/\n  train_tiles/\n    processed/\n      tiles_x=..._y=..._w=..._h=....pt\n  val_tiles/\n    processed/\n      ...\n  test_tiles/\n    processed/\n      ...\n</code></pre>"},{"location":"user_guide/data_creation/#example-usage","title":"Example Usage","text":""},{"location":"user_guide/data_creation/#xenium-with-optional-scrna-seq-derived-embeddings","title":"Xenium (with optional scRNA-seq-derived embeddings)","text":"<pre><code>from pathlib import Path\nimport pandas as pd\n\n# Optional: provide transcript embeddings (rows: genes, cols: embedding dims)\n# For example, cell-type abundance embeddings indexed by gene name\n# weights = pd.DataFrame(..., index=gene_names)\nweights = None  # set to a DataFrame if available\n\nfrom segger.data.sample import STSampleParquet\n\nbase_dir = Path(\"/path/to/xenium_output\")\ndata_dir = Path(\"/path/to/processed_tiles\")\n\nsample = STSampleParquet(\n    base_dir=base_dir,\n    sample_type=\"xenium\",\n    n_workers=4,            # controls parallel tiling across regions\n    # weights=weights,        # optional transcript embeddings\n    scale_factor=1.0,       # optional override (geometry scaling)\n)\n\n# Save tiles (choose either tile_size OR tile_width+tile_height)\nsample.save(\n    data_dir=data_dir,\n    # Receptive fields (neighbors)\n    k_bd=3,        # nearest boundaries per transcript\n    dist_bd=15.0,  # max distance for tx-&gt;bd neighbors (\u00b5m-equivalent)\n    k_tx=20,       # nearest transcripts per transcript\n    dist_tx=5.0,   # max distance for tx-&gt;tx neighbors\n    # Optional broader receptive fields for mutually exclusive genes (if used)\n    # Tiling\n    tile_size=50000,   # alternative: tile_width=..., tile_height=...\n    # Sampling/splitting\n    neg_sampling_ratio=5.0,\n    frac=1.0,\n    val_prob=0.1,\n    test_prob=0.2,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#merscope-fixed-size-tiling","title":"Merscope (fixed-size tiling)","text":"<pre><code>from pathlib import Path\nfrom segger.data.sample import STSampleParquet\n\nbase_dir = Path(\"/path/to/merscope_output\")\ndata_dir = Path(\"/path/to/processed_tiles\")\n\nsample = STSampleParquet(\n    base_dir=base_dir,\n    sample_type=\"merscope\",\n    n_workers=2,\n)\n\nsample.save(\n    data_dir=data_dir,\n    # Nearest neighbors\n    k_bd=3,\n    dist_bd=15.0,\n    k_tx=15,\n    dist_tx=5.0,\n    # Fixed-size tiling in sample units\n    tile_width=300,\n    tile_height=300,\n    # Splits\n    neg_sampling_ratio=3.0,\n    val_prob=0.1,\n    test_prob=0.2,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#debug-mode-step-by-step-logging","title":"Debug mode (step-by-step logging)","text":"<pre><code>sample.save_debug(\n    data_dir=data_dir,\n    k_bd=3,\n    dist_bd=15.0,\n    k_tx=20,\n    dist_tx=5.0,\n    tile_width=300,\n    tile_height=300,\n    neg_sampling_ratio=5.0,\n    frac=1.0,\n    val_prob=0.1,\n    test_prob=0.2,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#notes-and-recommendations","title":"Notes and Recommendations","text":"<ul> <li>Settings and columns: Filenames and columns for transcripts/boundaries are resolved via <code>sample_type</code> settings. See <code>segger.data._settings/*</code> for details.</li> <li>Transcript IDs: The constructor ensures an ID column exists in transcripts; if missing, it is added deterministically.</li> <li>Quality filtering: Uses settings-defined columns (e.g., QV) and filter substrings. Genes absent from provided <code>weights</code> will be auto-added to filter substrings to avoid OOV embeddings.</li> <li>Neighbors: Set <code>k_tx/dist_tx</code> based on typical nuclear radii and transcript densities; <code>k_bd/dist_bd</code> controls candidate boundaries per transcript.</li> <li>Splits: Tiles with no <code>('tx','belongs','bd')</code> edges are automatically placed in <code>test_tiles</code>.</li> <li>Embeddings: If no <code>weights</code> are provided, transcripts fall back to token/ID-based embeddings.</li> </ul>"},{"location":"user_guide/data_creation/#using-scrna-seq-for-embeddings-and-mutually-exclusive-genes","title":"Using scRNA-seq for embeddings and mutually exclusive genes","text":"<p>You can leverage scRNA-seq data both to create transcript embeddings (weights) and to identify mutually exclusive gene pairs that guide repulsive/attractive transcript edges.</p>"},{"location":"user_guide/data_creation/#1-compute-transcript-embeddings-weights-from-scrna-seq","title":"1) Compute transcript embeddings (weights) from scRNA-seq","text":"<pre><code>import scanpy as sc\nfrom segger.data._utils import calculate_gene_celltype_abundance_embedding\n\n# Load a reference AnnData\nadata = sc.read(\"/path/to/reference_scrnaseq.h5ad\")\nsc.pp.subsample(adata, 0.25)        # optional downsampling\nadata.var_names_make_unique()\nsc.pp.log1p(adata)\nsc.pp.normalize_total(adata)\n\n# Column in adata.obs with cell-type annotations\ncelltype_column = \"celltype_minor\"\n\n# Compute gene x cell-type abundance matrix (DataFrame indexed by gene names)\nweights = calculate_gene_celltype_abundance_embedding(\n    adata,\n    celltype_column,\n)\n\n# Pass weights to STSampleParquet to encode transcript features\nfrom segger.data.sample import STSampleParquet\nsample = STSampleParquet(\n    base_dir=\"/path/to/technology_output\",\n    sample_type=\"xenium\",      # or \"merscope\"\n    n_workers=4,\n    weights=weights,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#2-optional-identify-mutually-exclusive-genes-from-scrna-seq","title":"2) [OPTIONAL] Identify mutually exclusive genes from scRNA-seq","text":"<pre><code>from segger.data._utils import find_markers, find_mutually_exclusive_genes\n\n# Optionally restrict to genes present in the sample\ngenes = list(set(adata.var_names) &amp; set(sample.transcripts_metadata[\"feature_names\"]))\nadata_sub = adata[:, genes]\n\n# Find cell-type markers (tune thresholds as needed)\nmarkers = find_markers(\n    adata_sub,\n    cell_type_column=celltype_column,\n    pos_percentile=90,\n    neg_percentile=20,\n    percentage=20,\n)\n\n# Compute mutually exclusive gene pairs using markers\nexclusive_gene_pairs = find_mutually_exclusive_genes(\n    adata=adata,\n    markers=markers,\n    cell_type_column=celltype_column,\n)\n</code></pre>"},{"location":"user_guide/data_creation/#3-save-tiles-with-both-weights-and-mutually-exclusive-genes","title":"3) Save tiles with both weights and mutually exclusive genes","text":"<pre><code>sample.save(\n    data_dir=\"/path/to/processed_tiles\",\n    # Nearest-neighbor receptive fields\n    k_bd=3, dist_bd=15.0,\n    k_tx=20, dist_tx=5.0,\n    # Optional broader receptive fields used for mutually exclusive genes\n    k_tx_ex=100, dist_tx_ex=20.0,\n    # Tiling and splits\n    tile_size=50_000,\n    neg_sampling_ratio=5.0,\n    val_prob=0.1, test_prob=0.2,\n    # Use mutually exclusive pairs to add repulsive/attractive tx-tx labels\n    mutually_exclusive_genes=exclusive_gene_pairs,\n)\n</code></pre>"},{"location":"user_guide/training/","title":"Training the <code>segger</code> Model","text":""},{"location":"user_guide/training/#the-model","title":"The Model","text":"<p>The <code>segger</code> model is a graph neural network designed to handle heterogeneous graphs with two primary node types: transcripts and nuclei or cell boundaries. It leverages attention-based graph convolutional layers to compute node embeddings and relationships in spatial transcriptomics data. The architecture includes an initial embedding layer for node feature transformation, multiple graph attention layers (GATv2Conv), and residual linear connections.</p>"},{"location":"user_guide/training/#model-architecture","title":"Model Architecture","text":"<ul> <li> <p>Input Node Features:    For input node features \\(\\mathbf{x}\\), the model distinguishes between transcript nodes and boundary (or nucleus) nodes.</p> </li> <li> <p>Transcript Nodes: If \\(\\mathbf{x}\\) is 1-dimensional (e.g., for tokenized transcript data), the model applies an embedding layer:</p> </li> </ul> \\[ \\mathbf{h}_{i}^{(0)} = \\text{nn.Embedding}(i) \\] <p>where \\(i\\) is the transcript token index.</p> <ul> <li>Nuclei or Cell Boundary Nodes: If \\(\\mathbf{x}\\) has multiple dimensions, the model applies a linear transformation:</li> </ul> \\[ \\mathbf{h}_{i}^{(0)} = \\mathbf{W} \\mathbf{x}_{i} \\] <p>where \\(\\mathbf{W}\\) is a learnable weight matrix.</p> <ul> <li>Graph Attention Layers (GATv2Conv):    The node embeddings are updated through multiple attention-based layers. The update for a node \\(i\\) at layer \\(l+1\\) is given by:</li> </ul> \\[ \\mathbf{h}_{i}^{(l+1)} = \\text{ReLU}\\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W}^{(l)} \\mathbf{h}_{j}^{(l)} \\right) \\] <p>where:    - \\(\\alpha_{ij}\\) is the attention coefficient between node \\(i\\) and node \\(j\\), computed as:</p> \\[ \\alpha_{ij} = \\frac{\\exp\\left( \\text{LeakyReLU}\\left( \\mathbf{a}^{\\top} \\left[\\mathbf{W}^{(l)} \\mathbf{h}_{i}^{(l)} || \\mathbf{W}^{(l)} \\mathbf{h}_{j}^{(l)}\\right] \\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left( \\text{LeakyReLU}\\left( \\mathbf{a}^{\\top} \\left[\\mathbf{W}^{(l)} \\mathbf{h}_{i}^{(l)} || \\mathbf{W}^{(l)} \\mathbf{h}_{k}^{(l)}\\right] \\right)\\right)} \\] <ul> <li> <p>\\(\\mathbf{a}\\) is a learnable attention vector.</p> </li> <li> <p>Residual Linear Connections:    After each attention layer, a residual connection is added via a linear transformation to stabilize learning:</p> </li> </ul> \\[ \\mathbf{h}_{i}^{(l+1)} = \\text{ReLU}\\left( \\mathbf{h}_{i}^{(l+1)} + \\mathbf{W}_{res} \\mathbf{h}_{i}^{(l)} \\right) \\] <p>where \\(\\mathbf{W}_{res}\\) is a residual weight matrix.</p> <ul> <li>L2 Normalization:    Finally, the embeddings are normalized using L2 normalization:</li> </ul> \\[ \\mathbf{h}_{i} = \\frac{\\mathbf{h}_{i}}{\\|\\mathbf{h}_{i}\\|} \\] <p>ensuring the final node embeddings have unit norm.</p>"},{"location":"user_guide/training/#heterogeneous-graph-transformation","title":"Heterogeneous Graph Transformation","text":"<p>In the next step, the <code>segger</code> model is transformed into a heterogeneous graph neural network using PyTorch Geometric's <code>to_hetero</code> function. This transformation enables the model to handle distinct node and edge types (transcripts and nuclei or cell boundaries) with separate mechanisms for modeling their relationships.</p>"},{"location":"user_guide/training/#usage","title":"Usage","text":"<p>To instantiate and run the <code>segger</code> model:</p> <pre><code>model = segger(\n    num_tx_tokens=5000,  # Number of unique 'tx' tokens\n    init_emb=32,  # Initial embedding dimension\n    hidden_channels=64,  # Number of hidden channels\n    num_mid_layers=2,  # Number of middle layers\n    out_channels=128,  # Number of output channels\n    heads=4,  # Number of attention heads\n)\n\noutput = model(x, edge_index)\n</code></pre> <p>Once transformed to a heterogeneous model and trained using PyTorch Lightning, the model can efficiently learn relationships between transcripts and nuclei or cell boundaries.</p>"},{"location":"user_guide/training/#training-the-heterogeneous-gnn-with-pytorch-lightning","title":"Training the heterogeneous GNN with <code>pytorch-lightning</code>","text":"<p>The training module makes use of PyTorch Lightning for efficient and scalable training, alongside PyTorch Geometric for processing the graph-based data. The module is built to handle multi-GPU setups and allows the flexibility to adjust hyperparameters, aggregation methods, and embedding sizes.</p> <p>The <code>SpatialTranscriptomicsDataset</code> class is used to load and manage spatial transcriptomics data stored in the format of PyTorch Geometric <code>Data</code> objects. It inherits from <code>InMemoryDataset</code> to load preprocessed datasets, ensuring efficient in-memory data handling for training and validation phases.</p>"},{"location":"user_guide/training/#example-training-command","title":"Example Training Command","text":"<pre><code>python scripts/train_model.py \\ \n  --train_dir path/to/train/tiles \\\n  --val_dir path/to/val/tiles \\\n  --batch_size_train 4 \\\n  --batch_size_val 4 \\\n  --num_tx_tokens 500 \\\n  --init_emb 8 \\\n  --hidden_channels 64 \\\n  --out_channels 16 \\\n  --heads 4 \\\n  --mid_layers 1 \\\n  --aggr sum \\\n  --accelerator cuda \\\n  --strategy auto \\\n  --precision 16-mixed \\\n  --devices 4 \\\n  --epochs 100 \\\n  --default_root_dir ./models/clean2\n</code></pre> <p>The <code>scripts/train_model.py</code> file can be found on the github repo. This example submits a job to train the <code>segger</code> model on 4 GPUs with a batch size of 4 for both training and validation, utilizing 16-bit mixed precision.</p>"},{"location":"user_guide/validation/","title":"Validating the segmentations","text":"<p>This module provides utilities for validating segmentation methods in single-cell transcriptomics, focusing on evaluating performance across metrics such as sensitivity, specificity, and spatial localization.</p>"},{"location":"user_guide/validation/#benchmarking-and-validation-of-segmentation-methods","title":"Benchmarking and Validation of Segmentation Methods","text":"<p>To rigorously evaluate segmentation performance, we use a suite of metrics grouped into four categories: General Statistics, Sensitivity, Spatial Localization, and Specificity and Contamination. These metrics provide a comprehensive framework for assessing the accuracy and precision of segmentation methods.</p>"},{"location":"user_guide/validation/#general-statistics","title":"General Statistics","text":"<ul> <li>Percent Assigned Transcripts: Measures the proportion of transcripts correctly assigned to cells.</li> </ul> \\[ \\text{Percent Assigned} = \\frac{N_{\\text{assigned}}}{N_{\\text{total}}} \\times 100 \\] <ul> <li>Transcript Density: Assesses transcript counts relative to cell size.</li> </ul> \\[ D = \\frac{\\text{transcript counts}}{\\text{cell area}} \\]"},{"location":"user_guide/validation/#sensitivity","title":"Sensitivity","text":"<ul> <li>F1 Score for Cell Type Purity: Evaluates how well a segmentation method can identify cells based on marker genes.</li> </ul> \\[ \\text{F1}_{\\text{purity}} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\] <ul> <li>Gene-specific Assignment Metrics: Measures the proportion of correctly assigned transcripts for a specific gene.</li> </ul> \\[ \\text{Percent Assigned}_g = \\frac{N_{\\text{assigned}}^g}{N_{\\text{total}}^g} \\times 100 \\]"},{"location":"user_guide/validation/#spatial-localization","title":"Spatial Localization","text":"<ul> <li>Percent Cytoplasmic and Percent Nucleus Transcripts: Evaluates the spatial distribution of transcripts within cells.</li> </ul> \\[ \\text{Percent Cytoplasmic} = \\frac{N_{\\text{cytoplasmic}}}{N_{\\text{assigned}}} \\times 100 \\] \\[ \\text{Percent Nucleus} = \\frac{N_{\\text{nucleus}}}{N_{\\text{assigned}}} \\times 100 \\] <ul> <li>Neighborhood Entropy: Measures the diversity of neighboring cell types.</li> </ul> \\[ E = -\\sum_{c} p(c) \\log(p(c)) \\]"},{"location":"user_guide/validation/#specificity-and-contamination","title":"Specificity and Contamination","text":"<ul> <li>Mutually Exclusive Co-expression Rate (MECR): Quantifies how mutually exclusive gene expression is across cells.</li> </ul> \\[ \\text{MECR}(g_1, g_2) = \\frac{P(g_1 \\cap g_2)}{P(g_1 \\cup g_2)} \\] <ul> <li>Contamination from Neighboring Cells: Assesses transcript contamination from adjacent cells.</li> </ul> \\[ C_{ij} = \\frac{\\sum_{k \\in \\text{neighbors}} m_{ik} \\cdot w_{kj}}{\\sum_{k \\in \\text{neighbors}} m_{ik}} \\]"},{"location":"user_guide/validation/#comparison-across-segmentation-methods","title":"Comparison Across Segmentation Methods","text":"<p>A correlation analysis is used to compare different segmentation methods based on metrics such as transcript count and cell area. The Comparison Metric is defined as:</p> \\[ \\text{Comparison Metric}(m_1, m_2) = \\frac{\\sum_{i=1}^{n} (M_1(i) - \\bar{M_1}) (M_2(i) - \\bar{M_2})}{\\sqrt{\\sum_{i=1}^{n} (M_1(i) - \\bar{M_1})^2 \\sum_{i=1}^{n} (M_2(i) - \\bar{M_2})^2}} \\]"}]}