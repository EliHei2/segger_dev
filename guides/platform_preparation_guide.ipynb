{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8232f15",
   "metadata": {},
   "source": [
    "# Preparing Inputs for *segger* from Different iST Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83eb19d",
   "metadata": {},
   "source": [
    "Because data formats differ across platforms, here we provide guides to convert raw spatial transcriptomics data into a format compatible with ***segger***. \n",
    "\n",
    "**NOTE**: Currently, we provide instructions for preprocessing standard outputs for the platforms listed below:\n",
    "- 10x Genomics Xenium\n",
    "- NanoString CosMX\n",
    "- Vizgen MERSCOPE\n",
    "\n",
    "However, as long as your data adheres to the input format described below, you can run Segger on custom or unsupported platforms as well.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Formatting for *segger* Inputs\n",
    "\n",
    "At minimum, ***segger*** expects the following two inputs:\n",
    "\n",
    "1. **Transcripts** (GeoParquet with point geometry). Each row must include:\n",
    "    - The gene or feature label for the transcript.\n",
    "    - A segmentation ID matching the index of one element in the boundary polygon file.\n",
    "    - An X and Y location.\n",
    "2. **Boundaries** (GeoParquet with polygon geometry). Each row must include:\n",
    "    - A GeoArrow polygon geometry column (coordinates must be in the same space as the transcript coordinates).\n",
    "\n",
    "*Note*: We expect transcript data to be pre-filtered when running segger (e.g., no negative control probes or low QV-score transcripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d44016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "from segger.io.merscope import aggregate_merscope_polygons\n",
    "from segger.io.cosmx import get_cosmx_polygons\n",
    "from segger.io._enum import Filter_Substrings\n",
    "from segger.io import _utils as utils\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('moorman')\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "from pandas.errors import DtypeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DtypeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4ee26",
   "metadata": {},
   "source": [
    "## Nanostring CosMX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692d105",
   "metadata": {},
   "source": [
    "Segger uses nuclear boundaries as input, but CosMX datasets often only\n",
    "provide labeled cell and compartment masks. This section first extracts nuclear\n",
    "boundaries from CosMX output.\n",
    "\n",
    "The script expects the following structure in the CosMX output directory:\n",
    "\n",
    "```\n",
    "data_dir/\n",
    "├── *_fov_positions_file.csv\n",
    "├── CellLabels/\n",
    "│   ├── CellLabels_F001.tif\n",
    "│   ├── CellLabels_F002.tif\n",
    "│   └── ...\n",
    "└── CompartmentLabels/\n",
    "    ├── CompartmentLabels_F001.tif\n",
    "    ├── CompartmentLabels_F002.tif\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "These files are produced by NanoString’s CosMX pipeline. Nuclear masks are\n",
    "extracted as the regions where the compartment label equals `1` (nucleus),\n",
    "intersected with cell labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1959bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample base directory\n",
    "data_dir = Path('/data1/peerd/moormana/data/segger/segmentation_benchmarking'\n",
    "                '/cosmx_human_pancreas_healthy/data/raw')\n",
    "\n",
    "# Input file directories and paths\n",
    "cell_lbl_dir = data_dir / 'CellLabels'\n",
    "comp_lbl_dir = data_dir / 'CompartmentLabels'\n",
    "fov_path = data_dir / 'Pancreas_fov_positions_file.csv'\n",
    "tx_path = data_dir / 'Pancreas_tx_file.csv'\n",
    "\n",
    "# Boundary type ('cell' or 'nucleus')\n",
    "bd_type = 'nucleus'\n",
    "\n",
    "# Output file paths\n",
    "tx_path_out = data_dir / 'transcripts_geo.parquet'\n",
    "bd_path_out = data_dir / f'{bd_type}_boundaries_geo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3df6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CosMX nucleus boundaries and convert to GeoParquet\n",
    "bd = get_cosmx_polygons(cell_lbl_dir, comp_lbl_dir, fov_path, bd_type)\n",
    "bd.to_parquet(bd_path_out, write_covering_bbox=True,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# 18.3s for 47499 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdae8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in CosMX transcript data and filter\n",
    "tx = pd.read_csv(tx_path)\n",
    "pat = '|'.join(Filter_Substrings.nanostring_cosmx.value)\n",
    "mask = ~tx.target.str.contains(pat)\n",
    "tx = tx[mask]\n",
    "\n",
    "# Convert to GeoParquet\n",
    "pts = gpd.points_from_xy(tx.x_global_px, tx.y_global_px)\n",
    "tx = gpd.GeoDataFrame(tx, geometry=pts)\n",
    "\n",
    "# Assign transcripts to boundaries using spatial join\n",
    "tx = gpd.sjoin(tx, gpd.GeoDataFrame(geometry=bd.geometry), how='left', \n",
    "               predicate='intersects')\n",
    "tx = tx[~tx.index.duplicated()]\n",
    "tx.rename({'index_right': f'{bd_type}_boundaries_id'}, inplace=True, axis=1)\n",
    "\n",
    "# Write to file\n",
    "tx = pd.DataFrame(tx.drop(columns='geometry'))\n",
    "tx.to_parquet(tx_path_out)\n",
    "\n",
    "# Performance: 3m 8.8s for 65794659 transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b410f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write counts matrix from transcript assignments (nuclei only)\n",
    "if bd_type == 'nucleus':\n",
    "\n",
    "    ad = utils.transcripts_to_anndata(\n",
    "        tx, cell_label=f'{bd_type}_boundaries_id', gene_label='target',\n",
    "        coordinate_labels=['x_global_px', 'y_global_px'])\n",
    "\n",
    "    ad.write_h5ad(data_dir / f'{bd_type}_boundaries.h5ad')\n",
    "\n",
    "# Performance: 14.5s for 47487 x 18946 AnnData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193737e7",
   "metadata": {},
   "source": [
    "## 10X Xenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037de394",
   "metadata": {},
   "source": [
    "This section expects the files listed below in a Xenium output directory:\n",
    "```\n",
    "data_dir/\n",
    "├── transcripts.parquet\n",
    "└── nucleus_boundaries.parquet\n",
    "```\n",
    "By default, these files are produced by 10x Genomics’ Xenium pipeline in the correct directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2beb710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample base directory\n",
    "data_dir = Path('/data1/peerd/moormana/data/segger/preprint'\n",
    "                '/xenium_human_colon_healthy')\n",
    "\n",
    "# Boundary type ('cell' or 'nucleus')\n",
    "bd_type = 'nucleus'\n",
    "\n",
    "# Input file paths\n",
    "tx_path = data_dir / 'transcripts.parquet'\n",
    "bd_path = data_dir / f'{bd_type}_boundaries.parquet'\n",
    "\n",
    "# Output file paths\n",
    "tx_path_out = data_dir / 'transcripts_geo.parquet'\n",
    "bd_path_out = data_dir / f'{bd_type}_boundaries_geo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc35b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 10X Xenium boundaries and convert to GeoParquet\n",
    "bd = pd.read_parquet(bd_path)\n",
    "bd = utils.contours_to_polygons(bd['vertex_x'], bd['vertex_y'], bd['cell_id'])\n",
    "bd.to_parquet(bd_path_out, write_covering_bbox=True,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# Performance: 3.6s for 275822 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 10X Xenium transcript data and filter\n",
    "tx = pd.read_parquet(tx_path)\n",
    "mask = tx.qv.ge(20)\n",
    "pat = '|'.join(Filter_Substrings.tenx_xenium.value)\n",
    "mask &= ~tx.feature_name.str.contains(pat)\n",
    "tx = tx[mask]\n",
    "\n",
    "# Convert to GeoParquet\n",
    "pts = gpd.points_from_xy(tx.x_location, tx.y_location)\n",
    "tx = gpd.GeoDataFrame(tx, geometry=pts)\n",
    "\n",
    "# Assign transcripts to boundaries using spatial join\n",
    "tx = gpd.sjoin(tx, gpd.GeoDataFrame(geometry=bd.geometry), how='left', \n",
    "               predicate='intersects')\n",
    "tx = tx[~tx.index.duplicated()]\n",
    "tx.rename({'index_right': f'{bd_type}_boundaries_id'}, inplace=True, axis=1)\n",
    "\n",
    "# Write to file\n",
    "tx = pd.DataFrame(tx.drop(columns='geometry'))\n",
    "tx.to_parquet(tx_path_out)\n",
    "\n",
    "# Performance: 1m 29.4s for 39678353 transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write counts matrix from transcript assignments (nuclei only)\n",
    "if bd_type == 'nucleus':\n",
    "\n",
    "    ad = utils.transcripts_to_anndata(\n",
    "        tx, cell_label='nucleus_boundaries_id', gene_label='feature_name',\n",
    "        coordinate_labels=['x_location', 'y_location'])\n",
    "\n",
    "    ad.write_h5ad(data_dir / 'nucleus_boundaries.h5ad')\n",
    "\n",
    "# Performance: 7.7s for 274828 x 541 AnnData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65a59a",
   "metadata": {},
   "source": [
    "## Vizgen MERSCOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629316d",
   "metadata": {},
   "source": [
    "Segger requires nuclear masks as input, but these are often missing from raw MERSCOPE datasets. The [guide below](#preprocessing-merscope-data-with-vpt-for-segger) shows how to generate them using [vizgen-postprocessing](https://github.com/Vizgen/vizgen-postprocessing), Vizgen's CLI tool for segmentation (installed as the `vpt` Python package).\n",
    "\n",
    "This section expects the files listed below in a MERSCOPE output directory:\n",
    "```\n",
    "data_dir/\n",
    "├── detected_transcripts.csv\n",
    "└── nucleus_boundaries.parquet\n",
    "```\n",
    "By default, `detected_transcripts.csv` is produced by Vizgen’s MERSCOPE pipeline, and `nucleus_boundaries.parquet` can be generated according to the guide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1eb26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample base directory\n",
    "data_dir = Path('/data1/peerd/moormana/data/segger/segmentation_benchmarking'\n",
    "                '/merscope_human_brain_healthy_1k/data/raw')\n",
    "\n",
    "# Boundary type ('cell' or 'nucleus')\n",
    "bd_type = 'nucleus'\n",
    "\n",
    "# Input file directories and paths\n",
    "tx_path = data_dir / 'detected_transcripts.csv'\n",
    "bd_path = data_dir / f'{bd_type}_boundaries.parquet'\n",
    "\n",
    "# Output file paths\n",
    "tx_path_out = data_dir / 'transcripts_geo.parquet'\n",
    "bd_path_out = data_dir / f'{bd_type}_boundaries_geo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "026d7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in MERSCOPE boundaries and aggregate overlapping polygons\n",
    "bd = gpd.read_parquet(bd_path).set_index('ID')\n",
    "bd = aggregate_merscope_polygons(bd)\n",
    "\n",
    "# Convert to GeoParquet\n",
    "bd.to_parquet(bd_path_out, write_covering_bbox=True,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# Performance: 7.0s for 109184 boundaries (from 764288 polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3acd0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in MERSCOPE transcript data and convert to GeoParquet\n",
    "tx = pd.read_csv(tx_path)\n",
    "pts = gpd.points_from_xy(tx.global_x, tx.global_y)\n",
    "tx = gpd.GeoDataFrame(tx, geometry=pts)\n",
    "\n",
    "# Assign transcripts to boundaries using spatial join\n",
    "tx = gpd.sjoin(tx, gpd.GeoDataFrame(geometry=bd.geometry), how='left', \n",
    "               predicate='intersects')\n",
    "tx = tx[~tx.index.duplicated()]\n",
    "tx.rename({'index_right': f'{bd_type}_boundaries_id'}, inplace=True, axis=1)\n",
    "\n",
    "# Write to file\n",
    "tx = pd.DataFrame(tx.drop(columns='geometry'))\n",
    "tx.to_parquet(tx_path_out)\n",
    "\n",
    "# Performance: 3m 19.8s for 70514304 transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "296f9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write counts matrix from transcript assignments (nuclei only)\n",
    "if bd_type == 'nucleus':\n",
    "\n",
    "    ad = utils.transcripts_to_anndata(\n",
    "        tx, cell_label='nucleus_boundaries_id', gene_label='gene',\n",
    "        coordinate_labels=['global_x', 'global_y'])\n",
    "\n",
    "    ad.write_h5ad(data_dir / 'nucleus_boundaries.h5ad')\n",
    "\n",
    "# Performance: 15.4s for 108163 x 960 AnnData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a678e",
   "metadata": {},
   "source": [
    "### Preprocessing MERSCOPE Data with VPT for Segger\n",
    "\n",
    "#### 1. Environment Setup\n",
    "\n",
    "If you've already set up an environment for Segger, you likely have PyTorch installed. Otherwise, install it first (see [PyTorch install instructions](https://pytorch.org/get-started/locally/)). Then install `vpt` with Cellpose support:\n",
    "\n",
    "```bash\n",
    "python -m venv vpt-env\n",
    "source vpt-env/bin/activate\n",
    "pip install torch  # if not already installed\n",
    "pip install \"vpt[cellpose]\"\n",
    "```\n",
    "\n",
    "#### 2. Prepare Configuration\n",
    "\n",
    "VPT uses a JSON file to define segmentation parameters like model type, channel index, and scaling. See [Vizgen's documentation](https://vizgen.github.io/vizgen-postprocessing/segmentation/configuration/) for details. An example config is available at `platform_guides/vpt_nuclear_segmentation.json` in this repo.\n",
    "\n",
    "#### 3. Set Paths and Run VPT\n",
    "\n",
    "```bash\n",
    "export MERSCOPE_DATA_DIR=/path/to/merscope/data\n",
    "export OUTPUTS_DIR=${MERSCOPE_DATA_DIR}/nuclear_segmentation\n",
    "\n",
    "vpt run-segmentation \\\n",
    "  --segmentation-algorithm ${OUTPUTS_DIR}/vpt_nuclear_segmentation.json \\\n",
    "  --input-images ${MERSCOPE_DATA_DIR}/images/ \\\n",
    "  --input-micron-to-mosaic ${MERSCOPE_DATA_DIR}/images/micron_to_mosaic_pixel_transform.csv \\\n",
    "  --output-path ${OUTPUTS_DIR}/ \\\n",
    "  --tile-size 4800 \\\n",
    "  --tile-overlap 200\n",
    "```\n",
    "\n",
    "The resulting segmentation files can be used as input to Segger:\n",
    "- transcripts: `${OUTPUTS_DIR}/detected_transcripts.csv`\n",
    "- segmentation boundaries: `${OUTPUTS_DIR}/nucleus_boundaries.parquet`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381d546",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
