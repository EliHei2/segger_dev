{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <2BD1B165-EC09-3F68-BCE4-8FE4E70CA7E2> /Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <552B36CA-07A6-332B-BF7F-6D22D9005F71> /Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from segger.data.parquet.sample import STSampleParquet\n",
    "from segger.training.segger_data_module import SeggerDataModule\n",
    "from segger.training.train import LitSegger\n",
    "from segger.models.segger_model import Segger\n",
    "from segger.prediction.predict import load_model\n",
    "from torch_geometric.nn import to_hetero\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "xenium_data_dir = Path('data_xenium')\n",
    "segger_data_dir = Path('data_segger')\n",
    "# Base directory to store Pytorch Lightning models\n",
    "models_dir = Path('models')\n",
    "\n",
    "\n",
    "sample = STSampleParquet(\n",
    "    base_dir=xenium_data_dir,\n",
    "    n_workers=4,\n",
    "    sample_type='xenium', # this could be 'xenium_v2' in case one uses the cell boundaries from the segmentation kit.\n",
    "    #weights=gene_celltype_abundance_embedding, # uncomment if gene-celltype embeddings are available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/segger/data/parquet/pyg_dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Lightning data module\n",
    "dm = SeggerDataModule(\n",
    "    data_dir=segger_data_dir,\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "# Get a sample batch from the data module\n",
    "batch = dm.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_df(attention_weights, gene_names=None, edge_type='tx-tx'):\n",
    "    \"\"\"\n",
    "    Extract attention weights into a structured dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_df : list of tuples\n",
    "        List of (edge_index, alpha) tuples for each layer\n",
    "    gene_names : list, optional\n",
    "        List of gene names corresponding to transcript indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing transcript, gene, and attention weights for each layer and head\n",
    "    \"\"\"\n",
    "    assert edge_type in ['tx-tx', 'tx-bd'], \"Edge type must be 'tx-tx' or 'tx-bd'\"\n",
    "    \n",
    "    # Create a list to store all the data\n",
    "    data = []\n",
    "    \n",
    "    # Process each layer\n",
    "    for layer_idx, (edge_index, alpha) in enumerate(attention_weights):\n",
    "        if edge_type == 'tx-tx':\n",
    "            alpha_tensor = alpha['tx']\n",
    "            edge_index = edge_index['tx']\n",
    "        elif edge_type == 'tx-bd':\n",
    "            alpha_tensor = alpha['bd']\n",
    "            edge_index = edge_index['bd']\n",
    "        else:\n",
    "            raise ValueError(f\"Edge type must be 'tx-tx' or 'tx-bd', got {edge_type}\")\n",
    "        \n",
    "        # Convert attention weights to numpy\n",
    "        alpha_tensor = alpha_tensor.cpu().detach().numpy()\n",
    "        # print(f\"Alpha tensor shape: {alpha_tensor.shape}\")\n",
    "        edge_index = edge_index.cpu().detach().numpy()\n",
    "        # print(f\"Edge index shape: {edge_index.shape}\")\n",
    "        \n",
    "        # Process each head\n",
    "        for head_idx in range(alpha_tensor.shape[1]):\n",
    "            # Get attention weights for this head\n",
    "            head_weights = alpha_tensor[:, head_idx]\n",
    "            print(f\"Head {head_idx + 1} shape: {head_weights.shape}\")\n",
    "            print(f\"Edge index shape: {edge_index.shape}\")\n",
    "            \n",
    "            # Create entries for each edge\n",
    "            for i, (src, dst) in enumerate(edge_index.T):\n",
    "                entry = {\n",
    "                    'source': int(src),\n",
    "                    'target': int(dst),\n",
    "                    'edge_type': edge_type,\n",
    "                    'layer': layer_idx + 1,\n",
    "                    'head': head_idx + 1,\n",
    "                    'attention_weight': float(head_weights[i])\n",
    "                }\n",
    "                \n",
    "                # Add gene names if available\n",
    "                if gene_names is not None:\n",
    "                    entry['source_gene'] = gene_names[src]\n",
    "                    entry['target_gene'] = gene_names[dst]\n",
    "                \n",
    "                data.append(entry)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_df(attention_df, layer_idx, head_idx, edge_type, gene_names=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights of a data frame as a heatmap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_df : torch.Tensor\n",
    "        Attention weights of a data frame. Keys: 'source', 'target', 'edge_type', 'layer', 'head', 'attention_weight'.\n",
    "    layer_idx : int\n",
    "        Layer index.\n",
    "    head_idx : int\n",
    "        Head index.\n",
    "    edge_type : str\n",
    "        Edge type.\n",
    "    gene_names : list\n",
    "        a list of gene names ordered by the transcript indices.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Get number of nodes\n",
    "    num_nodes = len(attention_df['source'].unique())\n",
    "    # Extract attention weights with given layer_idx and head_idx\n",
    "    attention_df = attention_df[attention_df['layer'] == layer_idx + 1]\n",
    "    attention_df = attention_df[attention_df['head'] == head_idx + 1]\n",
    "    \n",
    "    # Create adjacency matrix for visualization\n",
    "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "    \n",
    "    # Fill adjacency matrix with attention weights\n",
    "    for _, row in attention_df.iterrows():\n",
    "        src = row['source']\n",
    "        dst = row['target']\n",
    "        adj_matrix[src, dst] = row['attention_weight']\n",
    "    \n",
    "    # Sort by genes if gene names are provided\n",
    "    if gene_names is not None:\n",
    "        # Sort nodes by gene names\n",
    "        sorted_indices = sorted(attention_df['source'].unique(), key=lambda i: gene_names[i])\n",
    "        # Reorder the adjacency matrix\n",
    "        adj_matrix = adj_matrix[sorted_indices][:, sorted_indices]\n",
    "        # Get sorted gene names for labels\n",
    "        sorted_genes = [gene_names[i] for i in sorted_indices]\n",
    "    else:\n",
    "        sorted_genes = None\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(adj_matrix, cmap='viridis', annot=False, cbar=True)\n",
    "    plt.title(f'Attention Weights - Layer {layer_idx + 1}, Head {head_idx + 1}, Edge Type: {edge_type}')\n",
    "    plt.xlabel('Target Node (Transcript)')\n",
    "    plt.ylabel('Source Node (Transcript)')\n",
    "    \n",
    "    # Add gene labels if available\n",
    "    if sorted_genes is not None:\n",
    "        # Add gene labels to the plot\n",
    "        plt.xticks(np.arange(len(sorted_genes)) + 0.5, sorted_genes, rotation=90, ha='right')\n",
    "        plt.yticks(np.arange(len(sorted_genes)) + 0.5, sorted_genes, rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path('figures') / f'attention_layer_{layer_idx + 1}_head_{head_idx + 1}_{edge_type}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "/Users/finleyyu/anaconda3/envs/segger/lib/python3.11/site-packages/segger/data/parquet/pyg_dataset.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Paths to data and models\n",
    "model_version = 1\n",
    "model_path = Path('models') / \"lightning_logs\" / f\"version_{model_version}\"\n",
    "ls = load_model(model_path / \"checkpoints\")\n",
    "\n",
    "ls.eval()\n",
    "\n",
    "# load transcripts\n",
    "transcripts = pd.read_parquet(Path('data_xenium') / 'transcripts.parquet')\n",
    "\n",
    "# Move batch to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ls = ls.to(device)\n",
    "\n",
    "# Initialize the Lightning data module\n",
    "dm = SeggerDataModule(\n",
    "    data_dir=Path('data_segger'),\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "# Get gene names from the batch data and transcripts\n",
    "# Convert tensor IDs to numpy array and then to list for indexing\n",
    "transcript_ids = batch['tx'].id.cpu().numpy()\n",
    "# Create a mapping from transcript ID to gene name\n",
    "id_to_gene = dict(zip(transcripts['transcript_id'], transcripts['feature_name']))\n",
    "# Get gene names in the same order as the batch\n",
    "gene_names = [id_to_gene[id] for id in transcript_ids]\n",
    "\n",
    "# Get a sample batch from the data module\n",
    "batch = dm.train[0].to(device)\n",
    "\n",
    "# Run forward pass to get attention weights\n",
    "with torch.no_grad():\n",
    "    # Access the heterogeneous model\n",
    "    hetero_model = ls.model\n",
    "    # Get node features and edge indices\n",
    "    x_dict = batch.x_dict\n",
    "    edge_index_dict = batch.edge_index_dict\n",
    "    \n",
    "    # Run forward pass through the model\n",
    "    _, attention_weights = hetero_model(x_dict, edge_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha tensor shape: (71922, 4)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 1 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 2 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 3 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 4 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Alpha tensor shape: (71922, 4)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 1 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 2 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 3 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 4 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Alpha tensor shape: (71922, 4)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 1 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 2 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 3 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 4 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Alpha tensor shape: (71922, 4)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 1 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 2 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 3 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 4 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Alpha tensor shape: (71922, 4)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 1 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 2 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 3 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Head 4 shape: (71922,)\n",
      "Edge index shape: (2, 71922)\n",
      "Saved attention weights dataset to figures/attention_weights_tx-tx.csv\n"
     ]
    }
   ],
   "source": [
    "edge_type = \"tx-tx\"\n",
    "\n",
    "# Extract attention weights into a structured dataset\n",
    "attention_df = extract_attention_df(attention_weights, gene_names)\n",
    "\n",
    "# Save the attention weights dataset\n",
    "output_path = Path(f'figures/attention_weights_{edge_type}.csv')\n",
    "attention_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved attention weights dataset to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_df['source'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: edge_index type = <class 'dict'>, edge keys: ['tx', 'bd'], alpha type = <class 'dict'>, alpha keys: ['tx', 'bd']\n",
      "Alpha tensor shape: torch.Size([8211, 4])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# visualize attention weights\n",
    "num_nodes = batch.x_dict['tx'].shape[0]\n",
    "    \n",
    "for layer_idx, (edge_index, alpha) in enumerate(attention_weights):\n",
    "    print(f\"Layer {layer_idx + 1}: edge_index type = {type(edge_index)}, edge keys: {list(edge_index.keys())}, alpha type = {type(alpha)}, alpha keys: {list(alpha.keys())}\")\n",
    "    \n",
    "    if edge_type == 'tx-tx':\n",
    "        alpha_tensor = alpha['tx']\n",
    "        edge_index_tensor = edge_index['tx']\n",
    "    elif edge_type == 'tx-bd':\n",
    "        alpha_tensor = alpha['bd']\n",
    "        edge_index_tensor = edge_index['bd']\n",
    "    else:\n",
    "        raise ValueError(f\"Edge type must be 'tx-tx' or 'tx-bd', got {edge_type}\")\n",
    "    \n",
    "    print(f\"Alpha tensor shape: {alpha_tensor.shape}\")\n",
    "        \n",
    "    for head_idx in range(alpha_tensor.shape[1]):\n",
    "        visualize_attention_df(\n",
    "            attention_df=attention_df,\n",
    "            layer_idx=layer_idx,\n",
    "            head_idx=head_idx,\n",
    "            edge_type=edge_type,\n",
    "            gene_names=gene_names\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_df(\n",
    "    attention_df=attention_df,\n",
    "    layer_idx=0,\n",
    "    head_idx=0,\n",
    "    edge_type=edge_type,\n",
    "    gene_names=gene_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
