{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8232f15",
   "metadata": {},
   "source": [
    "# Preparing Inputs for *segger* from Different iST Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83eb19d",
   "metadata": {},
   "source": [
    "Because data formats differ across platforms, here we provide guides to convert raw spatial transcriptomics data into a format compatible with ***segger***. \n",
    "\n",
    "**NOTE**: Currently, we provide instructions for preprocessing standard outputs for the platforms listed below:\n",
    "- 10x Genomics Xenium\n",
    "- NanoString CosMX\n",
    "- Vizgen MERSCOPE\n",
    "\n",
    "However, as long as your data adheres to the input format described below, you can run Segger on custom or unsupported platforms as well.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Formatting for *segger* Inputs\n",
    "\n",
    "At minimum, ***segger*** expects the following two inputs:\n",
    "\n",
    "1. **Transcripts** (GeoParquet with point geometry). Each row must include:\n",
    "    - The gene or feature label for the transcript.\n",
    "    - A segmentation ID matching the index of one element in the boundary polygon file.\n",
    "    - An X and Y location.\n",
    "2. **Boundaries** (GeoParquet with polygon geometry). Each row must include:\n",
    "    - A GeoArrow polygon geometry column (coordinates must be in the same space as the transcript coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09d44016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "from segger.io.cosmx import get_cosmx_polygons\n",
    "from segger.io._enum import Filter_Substrings\n",
    "from segger.io import _utils as utils\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('moorman')\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "from pandas.errors import DtypeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DtypeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4ee26",
   "metadata": {},
   "source": [
    "## Nanostring CosMX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692d105",
   "metadata": {},
   "source": [
    "Segger uses nuclear boundaries as input, but CosMX datasets often only\n",
    "provide labeled cell and compartment masks. This section first extracts nuclear\n",
    "boundaries from CosMX output.\n",
    "\n",
    "The script expects the following structure in the CosMX output directory:\n",
    "\n",
    "```\n",
    "data_dir/\n",
    "├── *_fov_positions_file.csv\n",
    "├── CellLabels/\n",
    "│   ├── CellLabels_F001.tif\n",
    "│   ├── CellLabels_F002.tif\n",
    "│   └── ...\n",
    "└── CompartmentLabels/\n",
    "    ├── CompartmentLabels_F001.tif\n",
    "    ├── CompartmentLabels_F002.tif\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "These files are produced by NanoString’s CosMX pipeline. Nuclear masks are\n",
    "extracted as the regions where the compartment label equals `1` (nucleus),\n",
    "intersected with cell labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1959bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample base directory\n",
    "data_dir = Path('/data1/peerd/moormana/data/segger/segmentation_benchmarking'\n",
    "                '/cosmx_human_pancreas_healthy/data/raw')\n",
    "\n",
    "# Input file directories and paths\n",
    "cell_lbl_dir = data_dir / 'CellLabels'\n",
    "comp_lbl_dir = data_dir / 'CompartmentLabels'\n",
    "fov_path = data_dir / 'Pancreas_fov_positions_file.csv'\n",
    "tx_path = data_dir / 'Pancreas_tx_file.csv'\n",
    "\n",
    "# Boundary type ('cell' or 'nucleus')\n",
    "bd_type = 'cell'\n",
    "\n",
    "# Output file paths\n",
    "tx_path_out = data_dir / 'transcripts_geo.parquet'\n",
    "bd_path_out = data_dir / f'{bd_type}_boundaries_geo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3df6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CosMX nucleus boundaries and convert to GeoParquet\n",
    "bd = get_cosmx_polygons(cell_lbl_dir, comp_lbl_dir, fov_path, bd_type)\n",
    "bd.to_parquet(bd_path_out, write_covering_bbox=True,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# 18.3s for 47499 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fdae8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If transcripts already exists, just append boundary assignments\n",
    "if tx_path_out.exists():\n",
    "    tx = gpd.read_parquet(tx_path_out)\n",
    "\n",
    "# Else, read in CosMX transcript data and convert to GeoParquet\n",
    "else:\n",
    "    tx = pd.read_csv(tx_path) \n",
    "    pts = gpd.points_from_xy(tx.x_global_px, tx.y_global_px)\n",
    "    tx = gpd.GeoDataFrame(tx, geometry=pts)\n",
    "\n",
    "# Assign transcripts to boundaries using spatial join\n",
    "tx = gpd.sjoin(tx, bd, how='left', predicate='intersects')\n",
    "tx.rename({'index_right': f'{bd_type}_boundaries_id'}, inplace=True, axis=1)\n",
    "\n",
    "# Write to file\n",
    "tx.to_parquet(tx_path_out, write_covering_bbox=False,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# Performance: 3m 8.8s for 65794659 transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b410f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write counts matrix from transcript assignments (nuclei only)\n",
    "if bd_type == 'nucleus':\n",
    "\n",
    "    ad = utils.transcripts_to_anndata(\n",
    "        tx, cell_label=f'{bd_type}_boundaries_id', gene_label='target',\n",
    "        coordinate_labels=['x_global_px', 'y_global_px'])\n",
    "\n",
    "    ad.write_h5ad(data_dir / f'{bd_type}_boundaries.h5ad')\n",
    "\n",
    "# Performance: 14.5s for 47487 x 21731 AnnData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193737e7",
   "metadata": {},
   "source": [
    "## 10X Xenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037de394",
   "metadata": {},
   "source": [
    "This section expects the files listed below in a Xenium output directory:\n",
    "```\n",
    "data_dir/\n",
    "├── transcripts.parquet\n",
    "└── nucleus_boundaries.parquet\n",
    "```\n",
    "By default, these files are produced by 10x Genomics’ Xenium pipeline in the correct directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2beb710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample base directory\n",
    "data_dir = Path('/data1/peerd/moormana/data/segger/preprint'\n",
    "                '/xenium_human_colon_healthy')\n",
    "\n",
    "# Boundary type ('cell' or 'nucleus')\n",
    "bd_type = 'nucleus'\n",
    "\n",
    "# Input file paths\n",
    "tx_path = data_dir / 'transcripts.parquet'\n",
    "bd_path = data_dir / f'{bd_type}_boundaries.parquet'\n",
    "\n",
    "# Output file paths\n",
    "tx_path_out = data_dir / 'transcripts_geo.parquet'\n",
    "bd_path_out = data_dir / f'{bd_type}_boundaries_geo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbc35b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 10X Xenium boundaries and convert to GeoParquet\n",
    "bd = pd.read_parquet(bd_path)\n",
    "bd = utils.contours_to_polygons(bd['vertex_x'], bd['vertex_y'], bd['cell_id'])\n",
    "bd.to_parquet(bd_path_out, write_covering_bbox=True,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# Performance: 3.6s for 275822 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ade89d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If transcript file exists, just append boundary assignments\n",
    "if tx_path_out.exists():\n",
    "    tx = gpd.read_parquet(tx_path_out)\n",
    "\n",
    "else:\n",
    "    # Read in 10X Xenium transcript data and filter\n",
    "    tx = pd.read_parquet(tx_path)\n",
    "    mask = tx.qv.ge(20)\n",
    "    pat = '|'.join(Filter_Substrings.tenx_xenium.value)\n",
    "    mask &= ~tx.feature_name.str.contains(pat)\n",
    "    tx = tx[mask]\n",
    "\n",
    "    # Convert to GeoParquet\n",
    "    pts = gpd.points_from_xy(tx.x_location, tx.y_location)\n",
    "    tx = gpd.GeoDataFrame(tx, geometry=pts)\n",
    "\n",
    "# Assign transcripts to boundaries using spatial join\n",
    "tx = gpd.sjoin(tx, bd, how='left', predicate='intersects')\n",
    "tx.rename({'index_right': f'{bd_type}_boundaries_id'}, inplace=True, axis=1)\n",
    "\n",
    "# Write to file\n",
    "tx.to_parquet(tx_path_out, write_covering_bbox=False,\n",
    "              geometry_encoding='geoarrow')\n",
    "\n",
    "# Performance: 1m 29.4s for 39678353 transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1929d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write counts matrix from transcript assignments (nuclei only)\n",
    "if bd_type == 'nucleus':\n",
    "\n",
    "    ad = utils.transcripts_to_anndata(\n",
    "        tx, cell_label='nucleus_boundaries_id', gene_label='feature_name',\n",
    "        coordinate_labels=['x_location', 'y_location'])\n",
    "\n",
    "    ad.write_h5ad(data_dir / 'nucleus_boundaries.h5ad')\n",
    "\n",
    "# Performance: 7.7s for 274828 x 541 AnnData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
